<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Aws | Ian Blenke - DevOps]]></title>
  <link href="http://ian.blenke.com/blog/categories/aws/atom.xml" rel="self"/>
  <link href="http://ian.blenke.com/"/>
  <updated>2015-06-27T16:08:06-04:00</updated>
  <id>http://ian.blenke.com/</id>
  <author>
    <name><![CDATA[Ian Blenke]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[AWS Docker Walkthrough With ElasticBeanstalk: Part 1]]></title>
    <link href="http://ian.blenke.com/blog/2015/06/27/aws-docker-walkthrough-with-elasticbeanstalk-part-1/"/>
    <updated>2015-06-27T13:14:08-04:00</updated>
    <id>http://ian.blenke.com/blog/2015/06/27/aws-docker-walkthrough-with-elasticbeanstalk-part-1</id>
    <content type="html"><![CDATA[<p>While deploying docker containers for immutable infrastructure on AWS ElasticBeanstalk,
I’ve learned a number of useful tricks that go beyond the official Amazon documentation.</p>

<p>This series of posts are an attempt to summarize some of the useful bits that may benefit
others facing the same challenges.</p>

<hr />

<h1 id="part-1--preparing-a-vpc-for-your-elasticbeanstalk-environments">Part 1 : Preparing a VPC for your ElasticBeanstalk environments</h1>

<hr />

<h3 id="step-1--prepare-your-aws-development-environment">Step 1 : Prepare your AWS development environment.</h3>

<p>On OS/X, I install <a href="http://brew.sh">homebrew</a>, and then:</p>

<p><code>bash
brew install awscli
</code></p>

<p>On Windows, I install <a href="https://chocolatey.org/">chocolatey</a> and then:</p>

<p><code>bash
choco install awscli
</code></p>

<p>Because <code>awscli</code> is a python tool, on either of these, or on the various Linux distribution flavors, we can also avoid native package management and alternatively use python <code>easyinstall</code> or <code>pip</code> directly:</p>

<p><code>bash
pip install awscli
</code></p>

<p>You may (or may not) need to prefix that pip install with <code>sudo</code>, depending. ie:</p>

<p><code>bash
sudo pip install awscli awsebcli
</code></p>

<p>These tools will detect if they are out of date when you run them. You may eventually get a message like:</p>

<p><code>
Alert: An update to this CLI is available.
</code></p>

<p>When this happens, you will likely want to either upgrade via homebrew:</p>

<p><code>bash
brew update &amp; brew upgrade awscli
</code></p>

<p>or, more likely, upgrade using pip directly:</p>

<p><code>bash
pip install --upgrade awscli
</code></p>

<p>Again, you may (or may not) need to prefix that pip install with <code>sudo</code>, depending. ie:</p>

<p><code>bash
sudo pip install --upgrade awscli
</code></p>

<p>For the hardcore Docker fans out there, this is pretty trivial to run as a container as well. See <a href="https://github.com/CenturyLinkLabs/docker-aws-cli">CenturyLinkLabs/docker-aws-cli</a> for a good example of that. Managing an aws config file requires volume mapping, or passing <code>-e AWS_ACCESS_KEY_ID={redacted} -e AWS_SECRET_ACCESS_KEY={redacted}</code>. There are various guides to doing this out there. This will not be one of them ;)</p>

<h3 id="step-2-prepare-your-aws-environment-variables">Step 2: Prepare your AWS environment variables</h3>

<p>If you haven’t already, <a href="http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html#config-settings-and-precedence">prepare for AWS cli access</a>.</p>

<p>You can now configure your <code>~/.aws/config</code> by running:</p>

<pre><code>aws configure
</code></pre>

<p>This will create a default configuration.</p>

<p>I’ve yet to work with any company with only one AWS account though. You will likely find that you need to support managing multiple AWS configuration profiles.</p>

<p>Here’s an example <code>~/.aws/config</code> file with multiple profiles:</p>

<p>&#8220;`
[default]
output = json
region = us-east-1</p>

<p>[profile aws-dev]
AWS_ACCESS_KEY_ID={REDACTED}
AWS_SECRET_ACCESS_KEY={REDACTED}</p>

<p>[profile aws-prod]
AWS_ACCESS_KEY_ID={REDACTED}
AWS_SECRET_ACCESS_KEY={REDACTED}
&#8220;`</p>

<p>You can create this by running:</p>

<p><code>bash
$ aws configure --profile aws-dev
AWS Access Key ID [REDACTED]: YOURACCESSKEY
AWS Secret Access Key [REDACTED]: YOURSECRETKEY
Default region name [None]: us-east-1
Default output format [None]: json
</code></p>

<p>Getting in the habit of specifying <code>--profile aws-dev</code> is a bit of a reassurance that you’re provisioning resources into the correct AWS account, and not sullying AWS cloud resources between VPC environments.</p>

<h3 id="step-3-preparing-a-vpc">Step 3: Preparing a VPC</h3>

<p>Deploying anything to AWS EC2 Classic instances these days is to continue down the path of legacy maintenance.</p>

<p>For new ElasticBeanstalk deployments, a VPC should be used.</p>

<p>The easiest/best way to deploy a VPC is to use a <a href="http://aws.amazon.com/cloudformation/aws-cloudformation-templates/">CloudFormation template</a>. </p>

<p>Below is a public gist of a VPC CloudFormation that I use for deployment:</p>

<p><div><script src='https://gist.github.com/0a6a6f26d1ecaa0d81eb.js'></script>
<noscript><pre><code>{
  &quot;AWSTemplateFormatVersion&quot;: &quot;2010-09-09&quot;,
  &quot;Description&quot;: &quot;MyApp VPC&quot;,
  &quot;Parameters&quot; : {
    &quot;Project&quot; : {
      &quot;Description&quot; : &quot;Project name to tag resources with&quot;,
      &quot;Type&quot; : &quot;String&quot;,
      &quot;MinLength&quot;: &quot;1&quot;,
      &quot;MaxLength&quot;: &quot;16&quot;,
      &quot;AllowedPattern&quot; : &quot;[a-z]*&quot;,
      &quot;ConstraintDescription&quot; : &quot;any alphabetic string (1-16) characters in length&quot;
    },
    &quot;Environment&quot; : {
      &quot;Description&quot; : &quot;Environment name to tag resources with&quot;,
      &quot;Type&quot; : &quot;String&quot;,
      &quot;AllowedValues&quot; : [ &quot;dev&quot;, &quot;qa&quot;, &quot;prod&quot; ],
      &quot;ConstraintDescription&quot; : &quot;must be one of dev, qa, or prod&quot;
    },
    &quot;SSHFrom&quot;: {
      &quot;Description&quot; : &quot;Lockdown SSH access (default: can be accessed from anywhere)&quot;,
      &quot;Type&quot; : &quot;String&quot;,
      &quot;MinLength&quot;: &quot;9&quot;,
      &quot;MaxLength&quot;: &quot;18&quot;,
      &quot;Default&quot; : &quot;0.0.0.0/0&quot;,
      &quot;AllowedPattern&quot; : &quot;(\\d{1,3})\\.(\\d{1,3})\\.(\\d{1,3})\\.(\\d{1,3})/(\\d{1,2})&quot;,
      &quot;ConstraintDescription&quot; : &quot;must be a valid CIDR range of the form x.x.x.x/x.&quot;
    },
    &quot;VPCNetworkCIDR&quot; : {
      &quot;Description&quot;: &quot;The CIDR block for the entire VPC network&quot;,
      &quot;Type&quot;: &quot;String&quot;,
      &quot;AllowedPattern&quot; : &quot;(\\d{1,3})\\.(\\d{1,3})\\.(\\d{1,3})\\.(\\d{1,3})/(\\d{1,2})&quot;,
      &quot;Default&quot;: &quot;10.114.0.0/16&quot;,
      &quot;ConstraintDescription&quot; : &quot;must be an IPv4 dotted quad plus slash plus network bit length in CIDR format&quot;
    },
    &quot;VPCSubnet0CIDR&quot; : {
      &quot;Description&quot;: &quot;The CIDR block for VPC subnet0 segment&quot;,
      &quot;Type&quot;: &quot;String&quot;,
      &quot;AllowedPattern&quot; : &quot;(\\d{1,3})\\.(\\d{1,3})\\.(\\d{1,3})\\.(\\d{1,3})/(\\d{1,2})&quot;,
      &quot;Default&quot;: &quot;10.114.0.0/24&quot;,
      &quot;ConstraintDescription&quot; : &quot;must be an IPv4 dotted quad plus slash plus network bit length in CIDR format&quot;
    },
    &quot;VPCSubnet1CIDR&quot; : {
      &quot;Description&quot;: &quot;The CIDR block for VPC subnet1 segment&quot;,
      &quot;Type&quot;: &quot;String&quot;,
      &quot;AllowedPattern&quot; : &quot;(\\d{1,3})\\.(\\d{1,3})\\.(\\d{1,3})\\.(\\d{1,3})/(\\d{1,2})&quot;,
      &quot;Default&quot;: &quot;10.114.1.0/24&quot;,
      &quot;ConstraintDescription&quot; : &quot;must be an IPv4 dotted quad plus slash plus network bit length in CIDR format&quot;
    },
    &quot;VPCSubnet2CIDR&quot; : {
      &quot;Description&quot;: &quot;The CIDR block for VPC subnet2 segment&quot;,
      &quot;Type&quot;: &quot;String&quot;,
      &quot;AllowedPattern&quot; : &quot;(\\d{1,3})\\.(\\d{1,3})\\.(\\d{1,3})\\.(\\d{1,3})/(\\d{1,2})&quot;,
      &quot;Default&quot;: &quot;10.114.2.0/24&quot;,
      &quot;ConstraintDescription&quot; : &quot;must be an IPv4 dotted quad plus slash plus network bit length in CIDR format&quot;
    }
  },
  &quot;Resources&quot; : {
    &quot;VPC&quot; : {
      &quot;Type&quot; : &quot;AWS::EC2::VPC&quot;,
      &quot;Properties&quot; : {
        &quot;EnableDnsSupport&quot; : &quot;true&quot;,
        &quot;EnableDnsHostnames&quot; : &quot;true&quot;,
        &quot;CidrBlock&quot; : { &quot;Ref&quot;: &quot;VPCNetworkCIDR&quot; },
        &quot;Tags&quot; : [
          { &quot;Key&quot; : &quot;Name&quot;, &quot;Value&quot; : { &quot;Fn::Join&quot;: [ &quot;-&quot;, [ &quot;vpc&quot;, { &quot;Ref&quot;: &quot;Project&quot; }, { &quot;Ref&quot; : &quot;Environment&quot; } ] ] } },
          { &quot;Key&quot; : &quot;Project&quot;, &quot;Value&quot; : { &quot;Ref&quot;: &quot;Project&quot; } },
          { &quot;Key&quot; : &quot;Environment&quot;, &quot;Value&quot; : { &quot;Ref&quot;: &quot;Environment&quot; } }
        ]
      }
    },
    &quot;VPCSubnet0&quot; : {
      &quot;Type&quot; : &quot;AWS::EC2::Subnet&quot;,
      &quot;Properties&quot; : {
        &quot;VpcId&quot; : { &quot;Ref&quot; : &quot;VPC&quot; },
        &quot;AvailabilityZone&quot;: { &quot;Fn::Select&quot; : [ 0, { &quot;Fn::GetAZs&quot; : &quot;&quot; } ] },
        &quot;CidrBlock&quot; : { &quot;Ref&quot;: &quot;VPCSubnet0CIDR&quot; },
        &quot;Tags&quot; : [
          { &quot;Key&quot; : &quot;Name&quot;, &quot;Value&quot; : { &quot;Fn::Join&quot;: [ &quot;-&quot;, [ &quot;subnet&quot;, { &quot;Ref&quot;: &quot;Project&quot; }, { &quot;Ref&quot;: &quot;Environment&quot; } ] ] } },
          { &quot;Key&quot; : &quot;AZ&quot;, &quot;Value&quot; : { &quot;Fn::Select&quot; : [ 0, { &quot;Fn::GetAZs&quot; : &quot;&quot; } ] } },
          { &quot;Key&quot; : &quot;Project&quot;, &quot;Value&quot; : { &quot;Ref&quot;: &quot;Project&quot; } },
          { &quot;Key&quot; : &quot;Environment&quot;, &quot;Value&quot; : { &quot;Ref&quot;: &quot;Environment&quot; } }
        ]
      }
    },
    &quot;VPCSubnet1&quot; : {
      &quot;Type&quot; : &quot;AWS::EC2::Subnet&quot;,
      &quot;Properties&quot; : {
        &quot;VpcId&quot; : { &quot;Ref&quot; : &quot;VPC&quot; },
        &quot;AvailabilityZone&quot;: { &quot;Fn::Select&quot; : [ 1, { &quot;Fn::GetAZs&quot; : &quot;&quot; } ] },
        &quot;CidrBlock&quot; : { &quot;Ref&quot;: &quot;VPCSubnet1CIDR&quot; },
        &quot;Tags&quot; : [
          { &quot;Key&quot; : &quot;Name&quot;, &quot;Value&quot; : { &quot;Fn::Join&quot;: [ &quot;-&quot;, [ &quot;subnet&quot;, { &quot;Ref&quot;: &quot;Project&quot; }, { &quot;Ref&quot;: &quot;Environment&quot; } ] ] } },
          { &quot;Key&quot; : &quot;AZ&quot;, &quot;Value&quot; : { &quot;Fn::Select&quot; : [ 1, { &quot;Fn::GetAZs&quot; : &quot;&quot; } ] } },
          { &quot;Key&quot; : &quot;Project&quot;, &quot;Value&quot; : { &quot;Ref&quot;: &quot;Project&quot; } },
          { &quot;Key&quot; : &quot;Environment&quot;, &quot;Value&quot; : { &quot;Ref&quot;: &quot;Environment&quot; } }
        ]
      }
    },
    &quot;VPCSubnet2&quot; : {
      &quot;Type&quot; : &quot;AWS::EC2::Subnet&quot;,
      &quot;Properties&quot; : {
        &quot;VpcId&quot; : { &quot;Ref&quot; : &quot;VPC&quot; },
        &quot;AvailabilityZone&quot;: { &quot;Fn::Select&quot; : [ 2, { &quot;Fn::GetAZs&quot; : &quot;&quot; } ] },
        &quot;CidrBlock&quot; : { &quot;Ref&quot;: &quot;VPCSubnet2CIDR&quot; },
        &quot;Tags&quot; : [
          { &quot;Key&quot; : &quot;Name&quot;, &quot;Value&quot; : { &quot;Fn::Join&quot;: [ &quot;-&quot;, [ &quot;subnet&quot;, { &quot;Ref&quot;: &quot;Project&quot; }, { &quot;Ref&quot;: &quot;Environment&quot; } ] ] } },
          { &quot;Key&quot; : &quot;AZ&quot;, &quot;Value&quot; : { &quot;Fn::Select&quot; : [ 2, { &quot;Fn::GetAZs&quot; : &quot;&quot; } ] } },
          { &quot;Key&quot; : &quot;Project&quot;, &quot;Value&quot; : { &quot;Ref&quot;: &quot;Project&quot; } },
          { &quot;Key&quot; : &quot;Environment&quot;, &quot;Value&quot; : { &quot;Ref&quot;: &quot;Environment&quot; } }
        ]
      }
    },
    &quot;InternetGateway&quot; : {
      &quot;Type&quot; : &quot;AWS::EC2::InternetGateway&quot;,
      &quot;Properties&quot; : {
        &quot;Tags&quot; : [
          { &quot;Key&quot; : &quot;Name&quot;, &quot;Value&quot; : { &quot;Fn::Join&quot;: [ &quot;-&quot;, [ &quot;igw&quot;, { &quot;Ref&quot;: &quot;Project&quot; }, { &quot;Ref&quot;: &quot;Environment&quot; } ] ] } },
          { &quot;Key&quot; : &quot;Project&quot;, &quot;Value&quot; : { &quot;Ref&quot;: &quot;Project&quot; } },
          { &quot;Key&quot; : &quot;Environment&quot;, &quot;Value&quot; : { &quot;Ref&quot;: &quot;Environment&quot; } }
        ]
      }
    },
    &quot;GatewayToInternet&quot; : {
       &quot;Type&quot; : &quot;AWS::EC2::VPCGatewayAttachment&quot;,
       &quot;Properties&quot; : {
         &quot;VpcId&quot; : { &quot;Ref&quot; : &quot;VPC&quot; },
         &quot;InternetGatewayId&quot; : { &quot;Ref&quot; : &quot;InternetGateway&quot; }
       }
    },
    &quot;PublicRouteTable&quot; : {
      &quot;Type&quot; : &quot;AWS::EC2::RouteTable&quot;,
      &quot;DependsOn&quot; : &quot;GatewayToInternet&quot;,
      &quot;Properties&quot; : {
        &quot;VpcId&quot; : { &quot;Ref&quot; : &quot;VPC&quot; },
        &quot;Tags&quot; : [
          { &quot;Key&quot; : &quot;Name&quot;, &quot;Value&quot; : { &quot;Fn::Join&quot;: [ &quot;-&quot;, [ &quot;route&quot;, { &quot;Ref&quot;: &quot;Project&quot; }, { &quot;Ref&quot; : &quot;Environment&quot; } ] ] } },
          { &quot;Key&quot; : &quot;Project&quot;, &quot;Value&quot; : { &quot;Ref&quot;: &quot;Project&quot; } },
          { &quot;Key&quot; : &quot;Environment&quot;, &quot;Value&quot; : { &quot;Ref&quot;: &quot;Environment&quot; } }
        ]
      }
    },
    &quot;PublicRoute&quot; : {
      &quot;Type&quot; : &quot;AWS::EC2::Route&quot;,
      &quot;DependsOn&quot; : &quot;GatewayToInternet&quot;,
      &quot;Properties&quot; : {
        &quot;RouteTableId&quot; : { &quot;Ref&quot; : &quot;PublicRouteTable&quot; },
        &quot;DestinationCidrBlock&quot; : &quot;0.0.0.0/0&quot;,
        &quot;GatewayId&quot; : { &quot;Ref&quot; : &quot;InternetGateway&quot; }
      }
    },
    &quot;VPCSubnet0RouteTableAssociation&quot; : {
      &quot;Type&quot; : &quot;AWS::EC2::SubnetRouteTableAssociation&quot;,
      &quot;Properties&quot; : {
        &quot;SubnetId&quot; : { &quot;Ref&quot; : &quot;VPCSubnet0&quot; },
        &quot;RouteTableId&quot; : { &quot;Ref&quot; : &quot;PublicRouteTable&quot; }
      }
    },
    &quot;VPCSubnet1RouteTableAssociation&quot; : {
      &quot;Type&quot; : &quot;AWS::EC2::SubnetRouteTableAssociation&quot;,
      &quot;Properties&quot; : {
        &quot;SubnetId&quot; : { &quot;Ref&quot; : &quot;VPCSubnet1&quot; },
        &quot;RouteTableId&quot; : { &quot;Ref&quot; : &quot;PublicRouteTable&quot; }
      }
    },
    &quot;VPCSubnet2RouteTableAssociation&quot; : {
      &quot;Type&quot; : &quot;AWS::EC2::SubnetRouteTableAssociation&quot;,
      &quot;Properties&quot; : {
        &quot;SubnetId&quot; : { &quot;Ref&quot; : &quot;VPCSubnet2&quot; },
        &quot;RouteTableId&quot; : { &quot;Ref&quot; : &quot;PublicRouteTable&quot; }
      }
    },
    &quot;InstanceRole&quot;: {
      &quot;Type&quot;: &quot;AWS::IAM::Role&quot;,
      &quot;Properties&quot;: {
        &quot;AssumeRolePolicyDocument&quot;: {
          &quot;Version&quot;: &quot;2012-10-17&quot;,
          &quot;Statement&quot;: [
            {
              &quot;Effect&quot;: &quot;Allow&quot;,
              &quot;Principal&quot;: {
                &quot;Service&quot;: [ &quot;ec2.amazonaws.com&quot; ]
              },
              &quot;Action&quot;: [ &quot;sts:AssumeRole&quot; ]
            }
          ]
        },
        &quot;Path&quot;: &quot;/&quot;,
        &quot;Policies&quot;: [
          {
            &quot;PolicyName&quot;: &quot;ApplicationPolicy&quot;,
            &quot;PolicyDocument&quot;: {
              &quot;Version&quot;: &quot;2012-10-17&quot;,
              &quot;Statement&quot;: [
                {
                  &quot;Effect&quot;: &quot;Allow&quot;,
                  &quot;Action&quot;: [
                    &quot;elasticbeanstalk:*&quot;,
                    &quot;elastiCache:*&quot;,
                    &quot;ec2:*&quot;,
                    &quot;elasticloadbalancing:*&quot;,
                    &quot;autoscaling:*&quot;,
                    &quot;cloudwatch:*&quot;,
                    &quot;dynamodb:*&quot;,
                    &quot;s3:*&quot;,
                    &quot;sns:*&quot;,
                    &quot;sqs:*&quot;,
                    &quot;cloudformation:*&quot;,
                    &quot;rds:*&quot;,
                    &quot;iam:AddRoleToInstanceProfile&quot;,
                    &quot;iam:CreateInstanceProfile&quot;,
                    &quot;iam:CreateRole&quot;,
                    &quot;iam:PassRole&quot;,
                    &quot;iam:ListInstanceProfiles&quot;
                  ],
                  &quot;Resource&quot;: &quot;*&quot;
                }
              ]
            }
          }
        ]
      }
    },
    &quot;InstanceProfile&quot;: {
       &quot;Type&quot;: &quot;AWS::IAM::InstanceProfile&quot;,
       &quot;Properties&quot;: {
          &quot;Path&quot;: &quot;/&quot;,
          &quot;Roles&quot;: [ { &quot;Ref&quot;: &quot;InstanceRole&quot; } ]
       }
    },
    &quot;VPCSecurityGroup&quot; : {
      &quot;Type&quot; : &quot;AWS::EC2::SecurityGroup&quot;,
      &quot;Properties&quot; : {
        &quot;GroupDescription&quot; : { &quot;Fn::Join&quot;: [ &quot;&quot;, [ &quot;VPC Security Group for &quot;, { &quot;Fn::Join&quot;: [ &quot;-&quot;, [ { &quot;Ref&quot;: &quot;Project&quot; }, { &quot;Ref&quot;: &quot;Environment&quot; } ] ] } ] ] },
        &quot;SecurityGroupIngress&quot; : [
          {&quot;IpProtocol&quot;: &quot;tcp&quot;, &quot;FromPort&quot; : &quot;22&quot;,  &quot;ToPort&quot; : &quot;22&quot;,  &quot;CidrIp&quot; : { &quot;Ref&quot; : &quot;SSHFrom&quot; }},
          {&quot;IpProtocol&quot;: &quot;tcp&quot;, &quot;FromPort&quot;: &quot;80&quot;, &quot;ToPort&quot;: &quot;80&quot;, &quot;CidrIp&quot;: &quot;0.0.0.0/0&quot; },
          {&quot;IpProtocol&quot;: &quot;tcp&quot;, &quot;FromPort&quot;: &quot;443&quot;, &quot;ToPort&quot;: &quot;443&quot;, &quot;CidrIp&quot;: &quot;0.0.0.0/0&quot; }
        ],
        &quot;VpcId&quot; : { &quot;Ref&quot; : &quot;VPC&quot; },
        &quot;Tags&quot; : [
          { &quot;Key&quot; : &quot;Name&quot;, &quot;Value&quot; : { &quot;Fn::Join&quot;: [ &quot;-&quot;, [ &quot;sg&quot;, { &quot;Ref&quot;: &quot;Project&quot; }, { &quot;Ref&quot; : &quot;Environment&quot; } ] ] } },
          { &quot;Key&quot; : &quot;Project&quot;, &quot;Value&quot; : { &quot;Ref&quot;: &quot;Project&quot; } },
          { &quot;Key&quot; : &quot;Environment&quot;, &quot;Value&quot; : { &quot;Ref&quot;: &quot;Environment&quot; } }
        ]
      }
    },
    &quot;VPCSGIngress&quot;: {
      &quot;Type&quot;: &quot;AWS::EC2::SecurityGroupIngress&quot;,
      &quot;Properties&quot;: {
        &quot;GroupId&quot;: { &quot;Ref&quot; : &quot;VPCSecurityGroup&quot; },
        &quot;IpProtocol&quot;: &quot;-1&quot;,
        &quot;FromPort&quot;: &quot;0&quot;,
        &quot;ToPort&quot;: &quot;65535&quot;,
        &quot;SourceSecurityGroupId&quot;: { &quot;Ref&quot;: &quot;VPCSecurityGroup&quot; }
      }
    }
  },
  &quot;Outputs&quot; : {
    &quot;VpcId&quot; : {
      &quot;Description&quot; : &quot;VPC Id&quot;,
      &quot;Value&quot; :  { &quot;Ref&quot; : &quot;VPC&quot; }
    },
    &quot;VPCDefaultNetworkAcl&quot; : {
      &quot;Description&quot; : &quot;VPC&quot;,
      &quot;Value&quot; :  { &quot;Fn::GetAtt&quot; : [&quot;VPC&quot;, &quot;DefaultNetworkAcl&quot;] }
    },
    &quot;VPCDefaultSecurityGroup&quot; : {
      &quot;Description&quot; : &quot;VPC Default Security Group that we blissfully ignore thanks to self-referencing bugs&quot;,
      &quot;Value&quot; :  { &quot;Fn::GetAtt&quot; : [&quot;VPC&quot;, &quot;DefaultSecurityGroup&quot;] }
    },
    &quot;VPCSecurityGroup&quot; : {
      &quot;Description&quot; : &quot;VPC Security Group created by this stack&quot;,
      &quot;Value&quot; :  { &quot;Ref&quot;: &quot;VPCSecurityGroup&quot; }
    },
    &quot;VPCSubnet0&quot;: {
      &quot;Description&quot;: &quot;The subnet id for VPCSubnet0&quot;,
      &quot;Value&quot;: {
        &quot;Ref&quot;: &quot;VPCSubnet0&quot;
      }
    },
    &quot;VPCSubnet1&quot;: {
      &quot;Description&quot;: &quot;The subnet id for VPCSubnet1&quot;,
      &quot;Value&quot;: {
        &quot;Ref&quot;: &quot;VPCSubnet1&quot;
      }
    },
    &quot;VPCSubnet2&quot;: {
      &quot;Description&quot;: &quot;The subnet id for VPCSubnet2&quot;,
      &quot;Value&quot;: {
        &quot;Ref&quot;: &quot;VPCSubnet2&quot;
      }
    }
  }
}</code></pre></noscript></div>
</p>

<p>Here is an example CloudFormation parameters file for this template:</p>

<p><div><script src='https://gist.github.com/9f4b8dd2b39c7d1c31ef.js'></script>
<noscript><pre><code>[
  { &quot;ParameterKey&quot;: &quot;Project&quot;, &quot;ParameterValue&quot;: &quot;myapp&quot; },
  { &quot;ParameterKey&quot;: &quot;Environment&quot;, &quot;ParameterValue&quot;: &quot;dev&quot; },
  { &quot;ParameterKey&quot;: &quot;VPCNetworkCIDR&quot;, &quot;ParameterValue&quot;: &quot;10.0.0.0/16&quot; },
  { &quot;ParameterKey&quot;: &quot;VPCSubnet0CIDR&quot;, &quot;ParameterValue&quot;: &quot;10.0.0.0/24&quot; },
  { &quot;ParameterKey&quot;: &quot;VPCSubnet1CIDR&quot;, &quot;ParameterValue&quot;: &quot;10.0.1.0/24&quot; },
  { &quot;ParameterKey&quot;: &quot;VPCSubnet2CIDR&quot;, &quot;ParameterValue&quot;: &quot;10.0.2.0/24&quot; }
]</code></pre></noscript></div>
</p>

<p>To script the creation, updating, watching, and deleting of the CloudFormation VPC, I have this Makefile as well:</p>

<p><div><script src='https://gist.github.com/55b740ff19825d621ef4.js'></script>
<noscript><pre><code>STACK:=myapp-dev
TEMPLATE:=cloudformation-template_vpc-iam.json
PARAMETERS:=cloudformation-parameters_myapp-dev.json
AWS_REGION:=us-east-1
AWS_PROFILE:=aws-dev

all:
	@which aws || pip install awscli
	aws cloudformation create-stack --stack-name $(STACK) --template-body file://`pwd`/$(TEMPLATE) --parameters file://`pwd`/$(PARAMETERS) --capabilities CAPABILITY_IAM --profile $(AWS_PROFILE) --region $(AWS_REGION)

update:
	aws cloudformation update-stack --stack-name $(STACK) --template-body file://`pwd`/$(TEMPLATE) --parameters file://`pwd`/$(PARAMETERS) --capabilities CAPABILITY_IAM --profile $(AWS_PROFILE) --region $(AWS_REGION)

events:
	aws cloudformation describe-stack-events --stack-name $(STACK) --profile $(AWS_PROFILE) --region $(AWS_REGION)

watch:
	watch --interval 10 &quot;bash -c &#39;make events | head -25&#39;&quot;
	
outputs:
	@which jq || ( which brew &amp;&amp; brew install jq || which apt-get &amp;&amp; apt-get install jq || which yum &amp;&amp; yum install jq || which choco &amp;&amp; choco install jq)
	aws cloudformation describe-stacks --stack-name $(STACK) --profile $(AWS_PROFILE) --region $(AWS_REGION) | jq -r &#39;.Stacks[].Outputs&#39;

delete:
	aws cloudformation delete-stack --stack-name $(STACK) --profile $(AWS_PROFILE) --region $(AWS_REGION)
</code></pre></noscript></div>
</p>

<p>You can get these same files by cloning my github project, and ssuming you have a profile named <code>aws-dev</code> as mentioned above, you can even run <code>make</code> and have it create the <code>myapp-dev</code> VPC via CloudFormation:</p>

<pre><code>git clone https://github.com/ianblenke/aws-docker-walkthrough
cd aws-docker-walkthrough
make
</code></pre>

<p>You can run <code>make watch</code> to watch the CloudFormation events and wait for a <code>CREATE_COMPLETE</code> state.</p>

<p>When this is complete, you can see the CloudFormation outputs by running:</p>

<pre><code>make output
</code></pre>

<p>The output will look something like this:</p>

<p><div><script src='https://gist.github.com/59715079304a6db7182c.js'></script>
<noscript><pre><code>aws cloudformation describe-stacks --stack-name myapp-dev --profile aws-dev --region us-east-1 | jq -r &#39;.Stacks[].Outputs&#39;
[
  {
    &quot;Description&quot;: &quot;VPC Id&quot;,
    &quot;OutputKey&quot;: &quot;VpcId&quot;,
    &quot;OutputValue&quot;: &quot;vpc-b7d1d8d2&quot;
  },
  {
    &quot;Description&quot;: &quot;VPC&quot;,
    &quot;OutputKey&quot;: &quot;VPCDefaultNetworkAcl&quot;,
    &quot;OutputValue&quot;: &quot;acl-b3cfc7d6&quot;
  },
  {
    &quot;Description&quot;: &quot;VPC Default Security Group that we blissfully ignore thanks to self-referencing bugs&quot;,
    &quot;OutputKey&quot;: &quot;VPCDefaultSecurityGroup&quot;,
    &quot;OutputValue&quot;: &quot;sg-3e50a559&quot;
  },
  {
    &quot;Description&quot;: &quot;VPC Security Group created by this stack&quot;,
    &quot;OutputKey&quot;: &quot;VPCSecurityGroup&quot;,
    &quot;OutputValue&quot;: &quot;sg-0c50a56b&quot;
  },
  {
    &quot;Description&quot;: &quot;The subnet id for VPCSubnet0&quot;,
    &quot;OutputKey&quot;: &quot;VPCSubnet0&quot;,
    &quot;OutputValue&quot;: &quot;subnet-995236b2&quot;
  },
  {
    &quot;Description&quot;: &quot;The subnet id for VPCSubnet1&quot;,
    &quot;OutputKey&quot;: &quot;VPCSubnet1&quot;,
    &quot;OutputValue&quot;: &quot;subnet-6aa4fd1d&quot;
  },
  {
    &quot;Description&quot;: &quot;The subnet id for VPCSubnet2&quot;,
    &quot;OutputKey&quot;: &quot;VPCSubnet2&quot;,
    &quot;OutputValue&quot;: &quot;subnet-ad3644f4&quot;
  }
]</code></pre></noscript></div>
</p>

<p>These CloudFormation Outputs list parameters that we will need to pass to the ElasticBeanstalk Environment creation during the next part of this walkthrough. </p>

<p>Stay tuned…</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Deploying Amazon ECS on CoreOS]]></title>
    <link href="http://ian.blenke.com/blog/2015/03/10/deploying-amazon-ecs-on-coreos/"/>
    <updated>2015-03-10T16:38:33-04:00</updated>
    <id>http://ian.blenke.com/blog/2015/03/10/deploying-amazon-ecs-on-coreos</id>
    <content type="html"><![CDATA[<p>Today, I stumbled on the official <a href="https://coreos.com/docs/running-coreos/cloud-providers/ecs/">CoreOS page on ECS</a>.</p>

<p>I’ve been putting off ECS for a while, it was time to give it a try.</p>

<p>To create the ECS cluster, we will need the aws commandline tool:</p>

<pre><code>which aws || pip install awscli
</code></pre>

<p>Make sure you have your <code>AWS_ACCESS_KEY_ID</code> and <code>AWS_SECRET_ACCESS_KEY</code> defined in your shell environment.</p>

<p>Create the ECS cluster:</p>

<pre><code>aws ecs create-cluster --cluster-name Cosmos-Dev
{
    "cluster": {
        "clusterName": "Cosmos-Dev",
        "status": "ACTIVE",
        "clusterArn": "arn:aws:ecs:us-east-1:123456789012:cluster/My-ECS-Cluster"
    }
}
</code></pre>

<p>Install the global fleet unit for amazon-ecs-agent.service:</p>

<pre><code>cat &lt;&lt;EOF &gt; amazon-ecs-agent.service
[Unit]
Description=Amazon ECS Agent
After=docker.service
Requires=docker.service
[Service]
Environment=ECS_CLUSTER=My-ECS-Cluster
Environment=ECS_LOGLEVEL=warn
Environment=AWS_REGION=us-east-1
ExecStartPre=-/usr/bin/docker kill ecs-agent
ExecStartPre=-/usr/bin/docker rm ecs-agent
ExecStartPre=/usr/bin/docker pull amazon/amazon-ecs-agent
ExecStart=/usr/bin/docker run --name ecs-agent \
    --env=ECS_CLUSTER=${ECS_CLUSTER}\
    --env=ECS_LOGLEVEL=${ECS_LOGLEVEL} \
    --publish=127.0.0.1:51678:51678 \
    --volume=/var/run/docker.sock:/var/run/docker.sock \
    amazon/amazon-ecs-agent
ExecStop=/usr/bin/docker stop ecs-agent
[X-Fleet]
Global=true
EOF
fleetctl start amazon-ecs-agent.service
</code></pre>

<p>This registers a ContainerInstance to the <code>My-ECS-Cluster</code> in region <code>us-east-1</code>.</p>

<p>Note: this is using the EC2 instance’s instance profile IAM credentials. You will want to make sure you’ve assigned an instance profile with a Role that has “ecs:*” access.
For this, you may want to take a look at the <a href="https://s3.amazonaws.com/amazon-ecs-cloudformation/Amazon_ECS_Quickstart.template">Amazon ECS Quickstart CloudFormation template</a>.</p>

<p>Now from a CoreOS host, we can query locally to enumerate the running ContainerInstances in our fleet:</p>

<pre><code>fleetctl list-machines -fields=ip -no-legend | while read ip ; do \
    echo $ip $(ssh -n $ip curl -s http://169.254.169.254/latest/meta-data/instance-id) \
    $(ssh -n $ip curl -s http://localhost:51678/v1/metadata | \
      docker run -i realguess/jq jq .ContainerInstanceArn) ; \
  done
</code></pre>

<p>Which returns something like:</p>

<pre><code>10.113.0.23 i-12345678 "arn:aws:ecs:us-east-1:123456789012:container-instance/674140ae-1234-4321-1234-4abf7878caba"
10.113.1.42 i-23456789 "arn:aws:ecs:us-east-1:123456789012:container-instance/c3506771-1234-4321-1234-1f1b1783c924"
10.113.2.66 i-34567891 "arn:aws:ecs:us-east-1:123456789012:container-instance/75d30c64-1234-4321-1234-8be8edeec9c6"
</code></pre>

<p>And we can query ECS and get the same:</p>

<pre><code>$ aws ecs list-container-instances --cluster My-ECS-Cluster | grep arn | cut -d'"' -f2 | \
  xargs -L1 -I% aws ecs describe-container-instances --cluster My-ECS-Cluster --container-instance % | \
  jq '.containerInstances[] | .ec2InstanceId + " " + .containerInstanceArn'
"i-12345678 arn:aws:ecs:us-east-1:123456789012:container-instance/674140ae-1234-4321-1234-4abf7878caba"
"i-23456789 arn:aws:ecs:us-east-1:123456789012:container-instance/c3506771-1234-4321-1234-1f1b1783c924"
"i-34567891 arn:aws:ecs:us-east-1:123456789012:container-instance/75d30c64-1234-4321-1234-8be8edeec9c6"
</code></pre>

<p>This ECS cluster is ready to use.</p>

<p>Unfortunately, there is no scheduler here. ECS is a harness for orchestrating docker containers in a cluster as <em>tasks</em>. </p>

<p>Where these tasks are allocated is left up to the AWS customer.</p>

<p>What we really need is a <em>scheduler</em>.</p>

<p>CoreOS has a form of a scheduler in fleet, but that is for fleet units of systemd services, and is not limited to docker containers as ECS is.
Fleet’s scheduler is also currently a bit weak in that it schedules new units to the fleet machine with the fewest number of units.</p>

<p>Kubernetes has a random scheduler, which is better in a couple ways, but does not fairly allocate the system resources.</p>

<p>The <em>best</em> scheduler at present is Mesos, which takes into account resource sizing estimates and current utilization.</p>

<p>Normally, Mesos uses Mesos Slaves to run work. Mesos can also use ECS as a backend instead.</p>

<p>My next steps: Deploy Mesos using the <a href="https://github.com/awslabs/ecs-mesos-scheduler-driver">ecs-mesos-scheduler-driver</a>, as <a href="http://jpetazzo.github.io/2015/01/14/amazon-docker-ecs-ec2-container-service/">summarized by jpetazzo</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Self-standing Ceph/deis-store Docker Containers]]></title>
    <link href="http://ian.blenke.com/blog/2014/11/05/self-standing-ceph-slash-deis-store-docker-containers/"/>
    <updated>2014-11-05T14:40:59-05:00</updated>
    <id>http://ian.blenke.com/blog/2014/11/05/self-standing-ceph-slash-deis-store-docker-containers</id>
    <content type="html"><![CDATA[<p>A common challenge for cloud orchestration is simulating or providing an S3 service layer, particularly for development environments.</p>

<p>As Docker is meant for immutable infrastructure, this poses somewhat of a challenge for production deployments. Rather than tackle that subject here, we’ll revisit persistence on immutable infrastructure in a production capacity in a future blog post.</p>

<p>The first challenge is identifying an S3 implementation to throw into a container.</p>

<p>There are a few feature sparse/dummy solutions that might suit development needs:</p>

<ul>
  <li><a href="http://s3ninja.net/">s3-ninja</a> (github <a href="https://github.com/scireum/s3ninja">scireum/s3ninja</a>)</li>
  <li><a href="https://github.com/jubos/fake-s3">fake-s3</a></li>
  <li><a href="http://sourceforge.net/projects/s3mockup/">S3Mockup</a>
(and a number of others which I’d rather not even consider)</li>
</ul>

<p>There are a number of good functional options for actual S3 implementations:</p>

<ul>
  <li><a href="http://ceph.com">ceph</a> (github <a href="https://github.com/ceph/ceph">ceph/ceph</a>), specifically the <a href="http://ceph.com/docs/master/radosgw/">radosgw</a></li>
  <li><a href="https://github.com/eucalyptus/eucalyptus/wiki/Walrus-S3-API">walrus</a> from Eucalyptus</li>
  <li><a href="http://basho.com/riak-cloud-storage/">riak cs</a></li>
  <li><a href="http://www.skylable.com/download/#LibreS3">libres3</a>, backended by the opensource <a href="http://www.skylable.com/download/#SX">Skylable Sx</a></li>
  <li><a href="https://github.com/nimbusproject/nimbus/tree/master/cumulus">cumulus</a> is an S3 implementation for <a href="http://www.nimbusproject.org/docs/current/faq.html#cumulus">Nimbus</a></li>
  <li><a href="http://www.cloudian.com/community-edition.php">cloudian</a> which is a non-opensource commercial product</li>
  <li><a href="https://github.com/stackforge/swift3">swift3</a> as an S3 compatibility layer with swift on the backend</li>
  <li><a href="https://github.com/cloudfoundry-attic/vblob">vblob</a> a node.js based attic’ed project at CloudFoundry</li>
  <li><a href="https://github.com/mattjamieson/parkplace">parkplace</a> backended by bittorrent</li>
  <li><a href="https://github.com/razerbeans/boardwalk">boardwalk</a> backended by ruby, sinatra, and mongodb</li>
</ul>

<p>Of the above, one stands out as the underlying persistence engine used by a larger docker backended project: <a href="http://deis.io">Deis</a></p>

<p>Rather than re-invent the wheel, it is possible to use deis-store directly.</p>

<p>As Deis deploys on CoreOS, there is an understandable inherent dependency on <a href="http://github.com/coreos/etcd/">etcd</a> for service discovery.</p>

<p>If you happen to be targeting CoreOS, you can simply point your etcd –peers option or <code>ETCD_HOST</code> environment variable at <code>$COREOS_PRIVATE_IPV4</code> and skip this next step.</p>

<p>First, make sure your environment includes the <code>DOCKER_HOST</code> and related variables for the boot2docker environment:</p>

<p><div><script src='https://gist.github.com/cab2661e67f5d79ae9bd.js'></script>
<noscript><pre><code>eval $(boot2docker shellinit)</code></pre></noscript></div>
</p>

<p>Now, discover the IP of the boot2docker guest VM, as that is what we will bind the etcd to:</p>

<p><div><script src='https://gist.github.com/00d61147bbf81ca26d2d.js'></script>
<noscript><pre><code>IP=&quot;$(boot2docker ip 2&gt;/dev/null)&quot;</code></pre></noscript></div>
</p>

<p>Next, we can spawn etcd and publish the ports for the other containers to use:</p>

<p><div><script src='https://gist.github.com/3a47603ef0561e54ecb6.js'></script>
<noscript><pre><code>docker run --name etcd \
           --publish 4001:4001 \
           --publish 7001:7001 \
           --detach \
           coreos/etcd:latest \
           /go/bin/app -listen-client-urls http://0.0.0.0:4001 \
                       -advertise-client-urls http://$IP:4001 \
                       -listen-peer-urls http://0.0.0.0:7001 \
                       -initial-advertise-peer-urls http://$IP:7001 \
                       -data-dir=/tmp/etcd</code></pre></noscript></div>
</p>

<p>Normally, we wouldn’t put the etcd persistence in a tmpfs for consistency reasons after a reboot, but for a development container: we love speed!</p>

<p>Now that we have an etcd container running, we can spawn the deis-store daemon container that runs the ceph object-store daemon (OSD) module.</p>

<p><div><script src='https://gist.github.com/c05525539a9f38e51e4b.js'></script>
<noscript><pre><code>docker run --name deis-store-daemon \
           --volumes-from=deis-store-daemon-data \
           --env HOST=$IP \
           --publish 6800 \
           --net host \
           --detach \
           deis/store-daemon:latest</code></pre></noscript></div>
</p>

<p>It is probably a good idea to mount the /var/lib/deis/store volume for persistence, but this is a developer container, so we’ll forego that step.</p>

<p>The ceph-osd will wait in a loop when starting until it can talk to ceph-mon, which is the next component provided by the deis-store monitor container.</p>

<p>In order to prepare the etcd config tree for deis-store monitor, we must first set a key for this new deis-store-daemon component.</p>

<p>While we could do that with a wget/curl PUT to the etcd client port (4001), using etcdctl makes things a bit easier.</p>

<p>It is generally a good idea to match the version of the etcdctl client with the version of etcd you are using.</p>

<p>As the CoreOS team doesn’t put out an etcdctl container as of yet, one way to do this is to build/install etcdctl inside a coreos/etcd container:</p>

<p><div><script src='https://gist.github.com/4fcf5bcca7077a85e7ce.js'></script>
<noscript><pre><code>docker run --rm \
           coreos/etcd \
           /bin/sh -c &quot;cd /go/src/github.com/coreos/etcd/etcdctl; \
                       go install ; \
                       /go/bin/etcdctl --peers $IP:4001 \
                                       set /deis/store/hosts/$IP $IP&quot;</code></pre></noscript></div>
</p>

<p>This isn’t ideal, of course, as there is a slight delay as etcdctl is built and installed before we use it, but it serves the purpose.</p>

<p>There are also <a href="http://docs.deis.io/en/latest/managing_deis/store_daemon_settings/">deis/store-daemon settings</a> of etcd keys that customize the behavior of ceph-osd a bit.</p>

<p>Now we can start deis-store-monitor, which will use that key to spin up a ceph-mon that monitors this (and any other) ceph-osd instances likewise registered in the etcd configuration tree.</p>

<p><div><script src='https://gist.github.com/543a13ba9410f6cf2f8e.js'></script>
<noscript><pre><code>docker run --name deis-store-monitor \
           --env HOST=$IP \
           --publish 6789 \
           --net host \
           --detach \
           deis/store-monitor:latest</code></pre></noscript></div>
</p>

<p>As before, there are volumes that probably should be mounted for /etc/ceph and /var/lib/ceph/mon, but this is a development image, so we’ll skip that.</p>

<p>There are also <a href="http://docs.deis.io/en/latest/managing_deis/store_monitor_settings/">deis/store-monitor settings</a> of etcd keys that customize the behavior of ceph-mon a bit.</p>

<p>Now that ceph-mon is running, ceph-osd will continue starting up. We now have a single-node self-standing ceph storage platform, but no S3.</p>

<p>The S3 functionality is provided by the ceph-radosgw component, which is provided by the deis-store-gateway container.</p>

<p><div><script src='https://gist.github.com/5634583a93347c415b3d.js'></script>
<noscript><pre><code>docker run --name deis-store-gateway \
           --hostname deis-store-gateway \
           --env HOST=$IP \
           --env EXTERNAL_PORT=8888 \
           --publish 8888:8888 \
           --detach \
           deis/store-gateway:latest</code></pre></noscript></div>
</p>

<p>There is no persistence in ceph-radosgw that warrant a volume mapping, so we can ignore that entirely regardless of environment.</p>

<p>There are also <a href="http://docs.deis.io/en/latest/managing_deis/store_gateway_settings/">deis/store-gateway settings</a> of etcd keys that customize the behavior of ceph-radosgw a bit.</p>

<p>We now have a functional self-standing S3 gateway, but we don’t know the credentials to use it. For that, we can run etcdctl again:</p>

<p><div><script src='https://gist.github.com/9c43ccd03c6c082073b7.js'></script>
<noscript><pre><code>AWS_ACCESS_KEY_ID=$(docker run --rm coreos/etcd /bin/sh -c &quot;cd /go/src/github.com/coreos/etcd/etcdctl; go install ; /go/bin/etcdctl --peers $IP:4001 get /deis/store/gateway/accessKey&quot;)
AWS_SECRET_ACCESS_KEY=$(docker run --rm coreos/etcd /bin/sh -c &quot;cd /go/src/github.com/coreos/etcd/etcdctl; go install ; /go/bin/etcdctl --peers $IP:4001 get /deis/store/gateway/secretKey&quot;)
AWS_S3_HOST=$(docker run --rm coreos/etcd /bin/sh -c &quot;cd /go/src/github.com/coreos/etcd/etcdctl; go install ; /go/bin/etcdctl --peers $IP:4001 get /deis/store/gateway/host&quot;)
AWS_S3_PORT=$(docker run --rm coreos/etcd /bin/sh -c &quot;cd /go/src/github.com/coreos/etcd/etcdctl; go install ; /go/bin/etcdctl --peers $IP:4001 get /deis/store/gateway/port&quot;)</code></pre></noscript></div>
</p>

<p>Note that the host here isn’t the normal AWS gateway address, so you will need to specify things for your S3 client to access it correctly.</p>

<p>Likewise, you may need to specify an URL scheme of “http”, as the above does not expose an HTTPS encrypted port.</p>

<p>There are also S3 client changes that <a href="https://github.com/deis/deis/issues/2326">may be necessary</a> depending on the “calling format” of the client libraries. You may need to <a href="http://stackoverflow.com/questions/24312350/using-paperclip-fog-and-ceph">changes things like paperclip</a> to <a href="https://github.com/thoughtbot/paperclip/issues/1577">work with fog</a>. There are numerous tools that work happily with ceph, like <a href="https://github.com/stiller/s3_to_ceph/blob/master/s3_to_ceph.rb">s3_to_ceph</a> and even gems like <a href="https://github.com/fog/fog-radosgw">fog-radosgw</a> that try and help make this painless for your apps.</p>

<p>I will update this blog post shortly with an example of a containerized s3 client to show how to prove your ceph radosgw is working.</p>

<p>Have fun!</p>

]]></content>
  </entry>
  
</feed>
