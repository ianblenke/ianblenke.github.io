<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Aws | Ian Blenke - DevOps]]></title>
  <link href="http://ian.blenke.com/blog/categories/aws/atom.xml" rel="self"/>
  <link href="http://ian.blenke.com/"/>
  <updated>2015-03-10T17:26:08-04:00</updated>
  <id>http://ian.blenke.com/</id>
  <author>
    <name><![CDATA[Ian Blenke]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Deploying Amazon ECS on CoreOS]]></title>
    <link href="http://ian.blenke.com/blog/2015/03/10/deploying-amazon-ecs-on-coreos/"/>
    <updated>2015-03-10T16:38:33-04:00</updated>
    <id>http://ian.blenke.com/blog/2015/03/10/deploying-amazon-ecs-on-coreos</id>
    <content type="html"><![CDATA[<p>First, read the CoreOS page on ECS:</p>

<p>https://coreos.com/docs/running-coreos/cloud-providers/ecs/</p>

<p>We will need the aws commandline tool:</p>

<pre><code>which aws || pip install awscli
</code></pre>

<p>Make sure you have your <code>AWS_ACCESS_KEY_ID</code> and <code>AWS_SECRET_ACCESS_KEY</code> defined in your shell environment.</p>

<p>Create the ECS cluster:</p>

<pre><code>aws ecs create-cluster --cluster-name Cosmos-Dev
{
    "cluster": {
        "clusterName": "Cosmos-Dev",
        "status": "ACTIVE",
        "clusterArn": "arn:aws:ecs:us-east-1:123456789012:cluster/My-ECS-Cluster"
    }
}
</code></pre>

<p>Install the global fleet unit for amazon-ecs-agent.service:</p>

<pre><code>cat &lt;&lt;EOF &gt; amazon-ecs-agent.service
[Unit]
Description=Amazon ECS Agent
After=docker.service
Requires=docker.service
[Service]
Environment=ECS_CLUSTER=My-ECS-Cluster
Environment=ECS_LOGLEVEL=warn
Environment=AWS_REGION=us-east-1
ExecStartPre=-/usr/bin/docker kill ecs-agent
ExecStartPre=-/usr/bin/docker rm ecs-agent
ExecStartPre=/usr/bin/docker pull amazon/amazon-ecs-agent
ExecStart=/usr/bin/docker run --name ecs-agent \
    --env=ECS_CLUSTER=${ECS_CLUSTER}\
    --env=ECS_LOGLEVEL=${ECS_LOGLEVEL} \
    --publish=127.0.0.1:51678:51678 \
    --volume=/var/run/docker.sock:/var/run/docker.sock \
    amazon/amazon-ecs-agent
ExecStop=/usr/bin/docker stop ecs-agent
[X-Fleet]
Global=true
EOF
fleetctl start amazon-ecs-agent.service
</code></pre>

<p>This registers a ContainerInstance to the <code>My-ECS-Cluster</code> in region <code>us-east-1</code>.</p>

<p>Note: this is using the EC2 instance’s instance profile IAM credentials. You will want to make sure you’ve assigned an instance profile with a Role that has “ecs:*” access.
For this, you may want to take a look at the <a href="https://s3.amazonaws.com/amazon-ecs-cloudformation/Amazon_ECS_Quickstart.template">Amazon ECS Quickstart CloudFormation template</a>.</p>

<p>Now from a CoreOS host, we can query locally to enumerate the running ContainerInstances in our fleet:</p>

<pre><code>fleetctl list-machines -fields=ip -no-legend | while read ip ; do \
    echo $ip $(ssh -n $ip curl -s http://169.254.169.254/latest/meta-data/instance-id) \
    $(ssh -n $ip curl -s http://localhost:51678/v1/metadata | \
      docker run -i realguess/jq jq .ContainerInstanceArn) ; \
  done
</code></pre>

<p>Which returns something like:</p>

<pre><code>10.113.0.23 i-12345678 "arn:aws:ecs:us-east-1:123456789012:container-instance/674140ae-1234-4321-1234-4abf7878caba"
10.113.1.42 i-23456789 "arn:aws:ecs:us-east-1:123456789012:container-instance/c3506771-1234-4321-1234-1f1b1783c924"
10.113.2.66 i-34567891 "arn:aws:ecs:us-east-1:123456789012:container-instance/75d30c64-1234-4321-1234-8be8edeec9c6"
</code></pre>

<p>And we can query ECS and get the same:</p>

<pre><code>$ aws ecs list-container-instances --cluster My-ECS-Cluster | grep arn | cut -d'"' -f2 | \
  xargs -L1 -I% aws ecs describe-container-instances --cluster My-ECS-Cluster --container-instance % | \
  jq '.containerInstances[] | .ec2InstanceId + " " + .containerInstanceArn'
"i-12345678 arn:aws:ecs:us-east-1:123456789012:container-instance/674140ae-1234-4321-1234-4abf7878caba"
"i-23456789 arn:aws:ecs:us-east-1:123456789012:container-instance/c3506771-1234-4321-1234-1f1b1783c924"
"i-34567891 arn:aws:ecs:us-east-1:123456789012:container-instance/75d30c64-1234-4321-1234-8be8edeec9c6"
</code></pre>

<p>Next steps: Deploy Mesos using the <a href="https://github.com/awslabs/ecs-mesos-scheduler-driver">ecs-mesos-scheduler-driver</a>, as <a href="http://jpetazzo.github.io/2015/01/14/amazon-docker-ecs-ec2-container-service/">summarized by jpetazzo</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Self-standing Ceph/deis-store Docker Containers]]></title>
    <link href="http://ian.blenke.com/blog/2014/11/05/self-standing-ceph-slash-deis-store-docker-containers/"/>
    <updated>2014-11-05T14:40:59-05:00</updated>
    <id>http://ian.blenke.com/blog/2014/11/05/self-standing-ceph-slash-deis-store-docker-containers</id>
    <content type="html"><![CDATA[<p>A common challenge for cloud orchestration is simulating or providing an S3 service layer, particularly for development environments.</p>

<p>As Docker is meant for immutable infrastructure, this poses somewhat of a challenge for production deployments. Rather than tackle that subject here, we’ll revisit persistence on immutable infrastructure in a production capacity in a future blog post.</p>

<p>The first challenge is identifying an S3 implementation to throw into a container.</p>

<p>There are a few feature sparse/dummy solutions that might suit development needs:</p>

<ul>
  <li><a href="http://s3ninja.net/">s3-ninja</a> (github <a href="https://github.com/scireum/s3ninja">scireum/s3ninja</a>)</li>
  <li><a href="https://github.com/jubos/fake-s3">fake-s3</a></li>
  <li><a href="http://sourceforge.net/projects/s3mockup/">S3Mockup</a>
(and a number of others which I’d rather not even consider)</li>
</ul>

<p>There are a number of good functional options for actual S3 implementations:</p>

<ul>
  <li><a href="http://ceph.com">ceph</a> (github <a href="https://github.com/ceph/ceph">ceph/ceph</a>), specifically the <a href="http://ceph.com/docs/master/radosgw/">radosgw</a></li>
  <li><a href="https://github.com/eucalyptus/eucalyptus/wiki/Walrus-S3-API">walrus</a> from Eucalyptus</li>
  <li><a href="http://basho.com/riak-cloud-storage/">riak cs</a></li>
  <li><a href="http://www.skylable.com/download/#LibreS3">libres3</a>, backended by the opensource <a href="http://www.skylable.com/download/#SX">Skylable Sx</a></li>
  <li><a href="https://github.com/nimbusproject/nimbus/tree/master/cumulus">cumulus</a> is an S3 implementation for <a href="http://www.nimbusproject.org/docs/current/faq.html#cumulus">Nimbus</a></li>
  <li><a href="http://www.cloudian.com/community-edition.php">cloudian</a> which is a non-opensource commercial product</li>
  <li><a href="https://github.com/stackforge/swift3">swift3</a> as an S3 compatibility layer with swift on the backend</li>
  <li><a href="https://github.com/cloudfoundry-attic/vblob">vblob</a> a node.js based attic’ed project at CloudFoundry</li>
  <li><a href="https://github.com/mattjamieson/parkplace">parkplace</a> backended by bittorrent</li>
  <li><a href="https://github.com/razerbeans/boardwalk">boardwalk</a> backended by ruby, sinatra, and mongodb</li>
</ul>

<p>Of the above, one stands out as the underlying persistence engine used by a larger docker backended project: <a href="http://deis.io">Deis</a></p>

<p>Rather than re-invent the wheel, it is possible to use deis-store directly.</p>

<p>As Deis deploys on CoreOS, there is an understandable inherent dependency on <a href="http://github.com/coreos/etcd/">etcd</a> for service discovery.</p>

<p>If you happen to be targeting CoreOS, you can simply point your etcd –peers option or <code>ETCD_HOST</code> environment variable at <code>$COREOS_PRIVATE_IPV4</code> and skip this next step.</p>

<p>First, make sure your environment includes the <code>DOCKER_HOST</code> and related variables for the boot2docker environment:</p>

<p><div><script src='https://gist.github.com/cab2661e67f5d79ae9bd.js'></script>
<noscript><pre><code>eval $(boot2docker shellinit)</code></pre></noscript></div>
</p>

<p>Now, discover the IP of the boot2docker guest VM, as that is what we will bind the etcd to:</p>

<p><div><script src='https://gist.github.com/00d61147bbf81ca26d2d.js'></script>
<noscript><pre><code>IP=&quot;$(boot2docker ip 2&gt;/dev/null)&quot;</code></pre></noscript></div>
</p>

<p>Next, we can spawn etcd and publish the ports for the other containers to use:</p>

<p><div><script src='https://gist.github.com/3a47603ef0561e54ecb6.js'></script>
<noscript><pre><code>docker run --name etcd \
           --publish 4001:4001 \
           --publish 7001:7001 \
           --detach \
           coreos/etcd:latest \
           /go/bin/app -listen-client-urls http://0.0.0.0:4001 \
                       -advertise-client-urls http://$IP:4001 \
                       -listen-peer-urls http://0.0.0.0:7001 \
                       -initial-advertise-peer-urls http://$IP:7001 \
                       -data-dir=/tmp/etcd</code></pre></noscript></div>
</p>

<p>Normally, we wouldn’t put the etcd persistence in a tmpfs for consistency reasons after a reboot, but for a development container: we love speed!</p>

<p>Now that we have an etcd container running, we can spawn the deis-store daemon container that runs the ceph object-store daemon (OSD) module.</p>

<p><div><script src='https://gist.github.com/c05525539a9f38e51e4b.js'></script>
<noscript><pre><code>docker run --name deis-store-daemon \
           --volumes-from=deis-store-daemon-data \
           --env HOST=$IP \
           --publish 6800 \
           --net host \
           --detach \
           deis/store-daemon:latest</code></pre></noscript></div>
</p>

<p>It is probably a good idea to mount the /var/lib/deis/store volume for persistence, but this is a developer container, so we’ll forego that step.</p>

<p>The ceph-osd will wait in a loop when starting until it can talk to ceph-mon, which is the next component provided by the deis-store monitor container.</p>

<p>In order to prepare the etcd config tree for deis-store monitor, we must first set a key for this new deis-store-daemon component.</p>

<p>While we could do that with a wget/curl PUT to the etcd client port (4001), using etcdctl makes things a bit easier.</p>

<p>It is generally a good idea to match the version of the etcdctl client with the version of etcd you are using.</p>

<p>As the CoreOS team doesn’t put out an etcdctl container as of yet, one way to do this is to build/install etcdctl inside a coreos/etcd container:</p>

<p><div><script src='https://gist.github.com/4fcf5bcca7077a85e7ce.js'></script>
<noscript><pre><code>docker run --rm \
           coreos/etcd \
           /bin/sh -c &quot;cd /go/src/github.com/coreos/etcd/etcdctl; \
                       go install ; \
                       /go/bin/etcdctl --peers $IP:4001 \
                                       set /deis/store/hosts/$IP $IP&quot;</code></pre></noscript></div>
</p>

<p>This isn’t ideal, of course, as there is a slight delay as etcdctl is built and installed before we use it, but it serves the purpose.</p>

<p>There are also <a href="http://docs.deis.io/en/latest/managing_deis/store_daemon_settings/">deis/store-daemon settings</a> of etcd keys that customize the behavior of ceph-osd a bit.</p>

<p>Now we can start deis-store-monitor, which will use that key to spin up a ceph-mon that monitors this (and any other) ceph-osd instances likewise registered in the etcd configuration tree.</p>

<p><div><script src='https://gist.github.com/543a13ba9410f6cf2f8e.js'></script>
<noscript><pre><code>docker run --name deis-store-monitor \
           --env HOST=$IP \
           --publish 6789 \
           --net host \
           --detach \
           deis/store-monitor:latest</code></pre></noscript></div>
</p>

<p>As before, there are volumes that probably should be mounted for /etc/ceph and /var/lib/ceph/mon, but this is a development image, so we’ll skip that.</p>

<p>There are also <a href="http://docs.deis.io/en/latest/managing_deis/store_monitor_settings/">deis/store-monitor settings</a> of etcd keys that customize the behavior of ceph-mon a bit.</p>

<p>Now that ceph-mon is running, ceph-osd will continue starting up. We now have a single-node self-standing ceph storage platform, but no S3.</p>

<p>The S3 functionality is provided by the ceph-radosgw component, which is provided by the deis-store-gateway container.</p>

<p><div><script src='https://gist.github.com/5634583a93347c415b3d.js'></script>
<noscript><pre><code>docker run --name deis-store-gateway \
           --hostname deis-store-gateway \
           --env HOST=$IP \
           --env EXTERNAL_PORT=8888 \
           --publish 8888:8888 \
           --detach \
           deis/store-gateway:latest</code></pre></noscript></div>
</p>

<p>There is no persistence in ceph-radosgw that warrant a volume mapping, so we can ignore that entirely regardless of environment.</p>

<p>There are also <a href="http://docs.deis.io/en/latest/managing_deis/store_gateway_settings/">deis/store-gateway settings</a> of etcd keys that customize the behavior of ceph-radosgw a bit.</p>

<p>We now have a functional self-standing S3 gateway, but we don’t know the credentials to use it. For that, we can run etcdctl again:</p>

<p><div><script src='https://gist.github.com/9c43ccd03c6c082073b7.js'></script>
<noscript><pre><code>AWS_ACCESS_KEY_ID=$(docker run --rm coreos/etcd /bin/sh -c &quot;cd /go/src/github.com/coreos/etcd/etcdctl; go install ; /go/bin/etcdctl --peers $IP:4001 get /deis/store/gateway/accessKey&quot;)
AWS_SECRET_ACCESS_KEY=$(docker run --rm coreos/etcd /bin/sh -c &quot;cd /go/src/github.com/coreos/etcd/etcdctl; go install ; /go/bin/etcdctl --peers $IP:4001 get /deis/store/gateway/secretKey&quot;)
AWS_S3_HOST=$(docker run --rm coreos/etcd /bin/sh -c &quot;cd /go/src/github.com/coreos/etcd/etcdctl; go install ; /go/bin/etcdctl --peers $IP:4001 get /deis/store/gateway/host&quot;)
AWS_S3_PORT=$(docker run --rm coreos/etcd /bin/sh -c &quot;cd /go/src/github.com/coreos/etcd/etcdctl; go install ; /go/bin/etcdctl --peers $IP:4001 get /deis/store/gateway/port&quot;)</code></pre></noscript></div>
</p>

<p>Note that the host here isn’t the normal AWS gateway address, so you will need to specify things for your S3 client to access it correctly.</p>

<p>Likewise, you may need to specify an URL scheme of “http”, as the above does not expose an HTTPS encrypted port.</p>

<p>There are also S3 client changes that <a href="https://github.com/deis/deis/issues/2326">may be necessary</a> depending on the “calling format” of the client libraries. You may need to <a href="http://stackoverflow.com/questions/24312350/using-paperclip-fog-and-ceph">changes things like paperclip</a> to <a href="https://github.com/thoughtbot/paperclip/issues/1577">work with fog</a>. There are numerous tools that work happily with ceph, like <a href="https://github.com/stiller/s3_to_ceph/blob/master/s3_to_ceph.rb">s3_to_ceph</a> and even gems like <a href="https://github.com/fog/fog-radosgw">fog-radosgw</a> that try and help make this painless for your apps.</p>

<p>I will update this blog post shortly with an example of a containerized s3 client to show how to prove your ceph radosgw is working.</p>

<p>Have fun!</p>

]]></content>
  </entry>
  
</feed>
