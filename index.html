
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Ian Blenke - DevOps</title>
  <meta name="author" content="Ian Blenke">

  
  <meta name="description" content="While deploying docker containers for immutable infrastructure on AWS ElasticBeanstalk,
I’ve learned a number of useful tricks that go beyond the &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://ian.blenke.com">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/atom.xml" rel="alternate" title="Ian Blenke - DevOps" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="./javascripts/libs/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href='http://fonts.googleapis.com/css?family=Open+Sans' rel='stylesheet' type='text/css'>
<link href='http://fonts.googleapis.com/css?family=Fjalla+One' rel='stylesheet' type='text/css'>

  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-53218787-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body   >
  <header role="banner"><hgroup>
  
      <img style="float: left; margin: 0px 15px 15px 0px;" src="http://www.gravatar.com/avatar/b0efd275b44b660f817003f2baca7ea2" alt="Gravatar of Ian Blenke " title="Gravatar of Ian Blenke" />
  
  <h1><a href="/">Ian Blenke - DevOps</a></h1>
  
    <h2>A 6502 hack living in a cloud orchestration world</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  
  
</ul>

<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
  <li><a href="https://github.com/ianblenke">GitHub</a></li>
  <li><a href="https://registry.hub.docker.com/repos/ianblenke/">DockerHub</a></li>
  <li><a href="https://twitter.com/ianblenke">Twitter</a></li>
  <li><a href="https://www.linkedin.com/in/ianblenke/">LinkedIn</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div class="blog-index">
  
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2015/06/27/aws-docker-walkthrough-with-elasticbeanstalk-part-1/">AWS Docker Walkthrough With ElasticBeanstalk: Part 1</a></h1>
    
    
      <p class="meta">
        








  


Jun 27, 2015
        
           | <a href="/blog/2015/06/27/aws-docker-walkthrough-with-elasticbeanstalk-part-1/#disqus_thread"
             data-disqus-identifier="http://ian.blenke.com/blog/2015/06/27/aws-docker-walkthrough-with-elasticbeanstalk-part-1/">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>While deploying docker containers for immutable infrastructure on AWS ElasticBeanstalk,
I’ve learned a number of useful tricks that go beyond the official Amazon documentation.</p>

<p>This series of posts are an attempt to summarize some of the useful bits that may benefit
others facing the same challenges.</p>

<hr />

<h1 id="part-1--preparing-a-vpc-for-your-elasticbeanstalk-environments">Part 1 : Preparing a VPC for your ElasticBeanstalk environments</h1>

<h2 id="step-1--prepare-your-aws-development-environment">Step 1 : Prepare your AWS development environment.</h2>

<p>On OS/X, I install <a href="http://brew.sh">homebrew</a>, and then:</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="bash"><span class="line">brew install awscli
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>On Windows, I install <a href="https://chocolatey.org/">chocolatey</a> and then:</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="bash"><span class="line">choco install awscli
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>As <code>awscli</code> is a python tool, we can alternatively use python <code>easyinstall</code> or <code>pip</code> directly:</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="bash"><span class="line">pip install awscli
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>You may (or may not) need to prefix that pip install with <code>sudo</code>, depending. ie:</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="bash"><span class="line">sudo pip install awscli awsebcli
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>These tools will detect if they are out of date when you run them. You may eventually get a message like:</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="bash"><span class="line">Alert: An update to this CLI is available.
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>When this happens, you will likely want to either upgrade via homebrew:</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="bash"><span class="line">brew update <span class="p">&amp;</span> brew upgrade awscli
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>or, more likely, upgrade using pip directly:</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="bash"><span class="line">pip install --upgrade awscli
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>Again, you may (or may not) need to prefix that pip install with <code>sudo</code>, depending. ie:</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="bash"><span class="line">sudo pip install --upgrade awscli
</span></code></pre></td></tr></table></div></figure></notextile></div>

<h1 id="prepare-your-aws-environment-variables">Prepare your AWS environment variables</h1>

<p>If you haven’t already, <a href="http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html#config-settings-and-precedence">prepare for AWS cli access</a>.</p>

<p>You can now configure your <code>~/.aws/config</code> by running:</p>

<pre><code>aws configure
</code></pre>

<p>This will create a default configuration.</p>

<p>I’ve yet to work with any company with only one AWS account though. You will likely find that you need to support managing multiple AWS configuration profiles.</p>

<p>Here’s an example <code>~/.aws/config</code> file with multiple profiles:</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
</pre></td><td class="code"><pre><code class="bash"><span class="line"><span class="o">[</span>default<span class="o">]</span>
</span><span class="line"><span class="nv">output</span> <span class="o">=</span> json
</span><span class="line"><span class="nv">region</span> <span class="o">=</span> us-east-1
</span><span class="line">
</span><span class="line"><span class="o">[</span>profile aws-dev<span class="o">]</span>
</span><span class="line"><span class="nv">AWS_ACCESS_KEY_ID</span><span class="o">={</span>REDACTED<span class="o">}</span>
</span><span class="line"><span class="nv">AWS_SECRET_ACCESS_KEY</span><span class="o">={</span>REDACTED<span class="o">}</span>
</span><span class="line">
</span><span class="line"><span class="o">[</span>profile aws-prod<span class="o">]</span>
</span><span class="line"><span class="nv">AWS_ACCESS_KEY_ID</span><span class="o">={</span>REDACTED<span class="o">}</span>
</span><span class="line"><span class="nv">AWS_SECRET_ACCESS_KEY</span><span class="o">={</span>REDACTED<span class="o">}</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>You can create this by running:</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
</pre></td><td class="code"><pre><code class="bash"><span class="line"><span class="nv">$ </span>aws configure --profile aws-dev
</span><span class="line">AWS Access Key ID <span class="o">[</span>REDACTED<span class="o">]</span>: YOURACCESSKEY
</span><span class="line">AWS Secret Access Key <span class="o">[</span>REDACTED<span class="o">]</span>: YOURSECRETKEY
</span><span class="line">Default region name <span class="o">[</span>None<span class="o">]</span>: us-east-1
</span><span class="line">Default output format <span class="o">[</span>None<span class="o">]</span>: json
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>Getting in the habit of specifying <code>--profile aws-dev</code> is a bit of a reassurance that you’re provisioning resources into the correct AWS account, and not sullying AWS cloud resources between VPC environments.</p>

<h2 id="step-2-preparing-a-vpc">Step 2: Preparing a VPC</h2>

<p>Deploying anything to AWS EC2 Classic instances these days is to continue down the path of legacy maintenance.</p>

<p>For new ElasticBeanstalk deployments, a VPC should be used.</p>

<p>The easiest/best way to deploy a VPC is to use a <a href="http://aws.amazon.com/cloudformation/aws-cloudformation-templates/">CloudFormation template</a>. </p>

<p>Below is a public gist of a VPC CloudFormation that I use for deployment:</p>

<div><script src="https://gist.github.com/0a6a6f26d1ecaa0d81eb.js"></script>
<noscript><pre><code>{
  &quot;AWSTemplateFormatVersion&quot;: &quot;2010-09-09&quot;,
  &quot;Description&quot;: &quot;MyApp VPC&quot;,
  &quot;Parameters&quot; : {
    &quot;Project&quot; : {
      &quot;Description&quot; : &quot;Project name to tag resources with&quot;,
      &quot;Type&quot; : &quot;String&quot;,
      &quot;MinLength&quot;: &quot;1&quot;,
      &quot;MaxLength&quot;: &quot;16&quot;,
      &quot;AllowedPattern&quot; : &quot;[a-z]*&quot;,
      &quot;ConstraintDescription&quot; : &quot;any alphabetic string (1-16) characters in length&quot;
    },
    &quot;Environment&quot; : {
      &quot;Description&quot; : &quot;Environment name to tag resources with&quot;,
      &quot;Type&quot; : &quot;String&quot;,
      &quot;AllowedValues&quot; : [ &quot;dev&quot;, &quot;qa&quot;, &quot;prod&quot; ],
      &quot;ConstraintDescription&quot; : &quot;must be one of dev, qa, or prod&quot;
    },
    &quot;SSHFrom&quot;: {
      &quot;Description&quot; : &quot;Lockdown SSH access (default: can be accessed from anywhere)&quot;,
      &quot;Type&quot; : &quot;String&quot;,
      &quot;MinLength&quot;: &quot;9&quot;,
      &quot;MaxLength&quot;: &quot;18&quot;,
      &quot;Default&quot; : &quot;0.0.0.0/0&quot;,
      &quot;AllowedPattern&quot; : &quot;(\\d{1,3})\\.(\\d{1,3})\\.(\\d{1,3})\\.(\\d{1,3})/(\\d{1,2})&quot;,
      &quot;ConstraintDescription&quot; : &quot;must be a valid CIDR range of the form x.x.x.x/x.&quot;
    },
    &quot;VPCNetworkCIDR&quot; : {
      &quot;Description&quot;: &quot;The CIDR block for the entire VPC network&quot;,
      &quot;Type&quot;: &quot;String&quot;,
      &quot;AllowedPattern&quot; : &quot;(\\d{1,3})\\.(\\d{1,3})\\.(\\d{1,3})\\.(\\d{1,3})/(\\d{1,2})&quot;,
      &quot;Default&quot;: &quot;10.114.0.0/16&quot;,
      &quot;ConstraintDescription&quot; : &quot;must be an IPv4 dotted quad plus slash plus network bit length in CIDR format&quot;
    },
    &quot;VPCSubnet0CIDR&quot; : {
      &quot;Description&quot;: &quot;The CIDR block for VPC subnet0 segment&quot;,
      &quot;Type&quot;: &quot;String&quot;,
      &quot;AllowedPattern&quot; : &quot;(\\d{1,3})\\.(\\d{1,3})\\.(\\d{1,3})\\.(\\d{1,3})/(\\d{1,2})&quot;,
      &quot;Default&quot;: &quot;10.114.0.0/24&quot;,
      &quot;ConstraintDescription&quot; : &quot;must be an IPv4 dotted quad plus slash plus network bit length in CIDR format&quot;
    },
    &quot;VPCSubnet1CIDR&quot; : {
      &quot;Description&quot;: &quot;The CIDR block for VPC subnet1 segment&quot;,
      &quot;Type&quot;: &quot;String&quot;,
      &quot;AllowedPattern&quot; : &quot;(\\d{1,3})\\.(\\d{1,3})\\.(\\d{1,3})\\.(\\d{1,3})/(\\d{1,2})&quot;,
      &quot;Default&quot;: &quot;10.114.1.0/24&quot;,
      &quot;ConstraintDescription&quot; : &quot;must be an IPv4 dotted quad plus slash plus network bit length in CIDR format&quot;
    },
    &quot;VPCSubnet2CIDR&quot; : {
      &quot;Description&quot;: &quot;The CIDR block for VPC subnet2 segment&quot;,
      &quot;Type&quot;: &quot;String&quot;,
      &quot;AllowedPattern&quot; : &quot;(\\d{1,3})\\.(\\d{1,3})\\.(\\d{1,3})\\.(\\d{1,3})/(\\d{1,2})&quot;,
      &quot;Default&quot;: &quot;10.114.2.0/24&quot;,
      &quot;ConstraintDescription&quot; : &quot;must be an IPv4 dotted quad plus slash plus network bit length in CIDR format&quot;
    }
  },
  &quot;Resources&quot; : {
    &quot;VPC&quot; : {
      &quot;Type&quot; : &quot;AWS::EC2::VPC&quot;,
      &quot;Properties&quot; : {
        &quot;EnableDnsSupport&quot; : &quot;true&quot;,
        &quot;EnableDnsHostnames&quot; : &quot;true&quot;,
        &quot;CidrBlock&quot; : { &quot;Ref&quot;: &quot;VPCNetworkCIDR&quot; },
        &quot;Tags&quot; : [
          { &quot;Key&quot; : &quot;Name&quot;, &quot;Value&quot; : { &quot;Fn::Join&quot;: [ &quot;-&quot;, [ &quot;vpc&quot;, { &quot;Ref&quot;: &quot;Project&quot; }, { &quot;Ref&quot; : &quot;Environment&quot; } ] ] } },
          { &quot;Key&quot; : &quot;Project&quot;, &quot;Value&quot; : { &quot;Ref&quot;: &quot;Project&quot; } },
          { &quot;Key&quot; : &quot;Environment&quot;, &quot;Value&quot; : { &quot;Ref&quot;: &quot;Environment&quot; } }
        ]
      }
    },
    &quot;VPCSubnet0&quot; : {
      &quot;Type&quot; : &quot;AWS::EC2::Subnet&quot;,
      &quot;Properties&quot; : {
        &quot;VpcId&quot; : { &quot;Ref&quot; : &quot;VPC&quot; },
        &quot;AvailabilityZone&quot;: { &quot;Fn::Select&quot; : [ 0, { &quot;Fn::GetAZs&quot; : &quot;&quot; } ] },
        &quot;CidrBlock&quot; : { &quot;Ref&quot;: &quot;VPCSubnet0CIDR&quot; },
        &quot;Tags&quot; : [
          { &quot;Key&quot; : &quot;Name&quot;, &quot;Value&quot; : { &quot;Fn::Join&quot;: [ &quot;-&quot;, [ &quot;subnet&quot;, { &quot;Ref&quot;: &quot;Project&quot; }, { &quot;Ref&quot;: &quot;Environment&quot; } ] ] } },
          { &quot;Key&quot; : &quot;AZ&quot;, &quot;Value&quot; : { &quot;Fn::Select&quot; : [ 0, { &quot;Fn::GetAZs&quot; : &quot;&quot; } ] } },
          { &quot;Key&quot; : &quot;Project&quot;, &quot;Value&quot; : { &quot;Ref&quot;: &quot;Project&quot; } },
          { &quot;Key&quot; : &quot;Environment&quot;, &quot;Value&quot; : { &quot;Ref&quot;: &quot;Environment&quot; } }
        ]
      }
    },
    &quot;VPCSubnet1&quot; : {
      &quot;Type&quot; : &quot;AWS::EC2::Subnet&quot;,
      &quot;Properties&quot; : {
        &quot;VpcId&quot; : { &quot;Ref&quot; : &quot;VPC&quot; },
        &quot;AvailabilityZone&quot;: { &quot;Fn::Select&quot; : [ 1, { &quot;Fn::GetAZs&quot; : &quot;&quot; } ] },
        &quot;CidrBlock&quot; : { &quot;Ref&quot;: &quot;VPCSubnet1CIDR&quot; },
        &quot;Tags&quot; : [
          { &quot;Key&quot; : &quot;Name&quot;, &quot;Value&quot; : { &quot;Fn::Join&quot;: [ &quot;-&quot;, [ &quot;subnet&quot;, { &quot;Ref&quot;: &quot;Project&quot; }, { &quot;Ref&quot;: &quot;Environment&quot; } ] ] } },
          { &quot;Key&quot; : &quot;AZ&quot;, &quot;Value&quot; : { &quot;Fn::Select&quot; : [ 1, { &quot;Fn::GetAZs&quot; : &quot;&quot; } ] } },
          { &quot;Key&quot; : &quot;Project&quot;, &quot;Value&quot; : { &quot;Ref&quot;: &quot;Project&quot; } },
          { &quot;Key&quot; : &quot;Environment&quot;, &quot;Value&quot; : { &quot;Ref&quot;: &quot;Environment&quot; } }
        ]
      }
    },
    &quot;VPCSubnet2&quot; : {
      &quot;Type&quot; : &quot;AWS::EC2::Subnet&quot;,
      &quot;Properties&quot; : {
        &quot;VpcId&quot; : { &quot;Ref&quot; : &quot;VPC&quot; },
        &quot;AvailabilityZone&quot;: { &quot;Fn::Select&quot; : [ 2, { &quot;Fn::GetAZs&quot; : &quot;&quot; } ] },
        &quot;CidrBlock&quot; : { &quot;Ref&quot;: &quot;VPCSubnet2CIDR&quot; },
        &quot;Tags&quot; : [
          { &quot;Key&quot; : &quot;Name&quot;, &quot;Value&quot; : { &quot;Fn::Join&quot;: [ &quot;-&quot;, [ &quot;subnet&quot;, { &quot;Ref&quot;: &quot;Project&quot; }, { &quot;Ref&quot;: &quot;Environment&quot; } ] ] } },
          { &quot;Key&quot; : &quot;AZ&quot;, &quot;Value&quot; : { &quot;Fn::Select&quot; : [ 2, { &quot;Fn::GetAZs&quot; : &quot;&quot; } ] } },
          { &quot;Key&quot; : &quot;Project&quot;, &quot;Value&quot; : { &quot;Ref&quot;: &quot;Project&quot; } },
          { &quot;Key&quot; : &quot;Environment&quot;, &quot;Value&quot; : { &quot;Ref&quot;: &quot;Environment&quot; } }
        ]
      }
    },
    &quot;InternetGateway&quot; : {
      &quot;Type&quot; : &quot;AWS::EC2::InternetGateway&quot;,
      &quot;Properties&quot; : {
        &quot;Tags&quot; : [
          { &quot;Key&quot; : &quot;Name&quot;, &quot;Value&quot; : { &quot;Fn::Join&quot;: [ &quot;-&quot;, [ &quot;igw&quot;, { &quot;Ref&quot;: &quot;Project&quot; }, { &quot;Ref&quot;: &quot;Environment&quot; } ] ] } },
          { &quot;Key&quot; : &quot;Project&quot;, &quot;Value&quot; : { &quot;Ref&quot;: &quot;Project&quot; } },
          { &quot;Key&quot; : &quot;Environment&quot;, &quot;Value&quot; : { &quot;Ref&quot;: &quot;Environment&quot; } }
        ]
      }
    },
    &quot;GatewayToInternet&quot; : {
       &quot;Type&quot; : &quot;AWS::EC2::VPCGatewayAttachment&quot;,
       &quot;Properties&quot; : {
         &quot;VpcId&quot; : { &quot;Ref&quot; : &quot;VPC&quot; },
         &quot;InternetGatewayId&quot; : { &quot;Ref&quot; : &quot;InternetGateway&quot; }
       }
    },
    &quot;PublicRouteTable&quot; : {
      &quot;Type&quot; : &quot;AWS::EC2::RouteTable&quot;,
      &quot;DependsOn&quot; : &quot;GatewayToInternet&quot;,
      &quot;Properties&quot; : {
        &quot;VpcId&quot; : { &quot;Ref&quot; : &quot;VPC&quot; },
        &quot;Tags&quot; : [
          { &quot;Key&quot; : &quot;Name&quot;, &quot;Value&quot; : { &quot;Fn::Join&quot;: [ &quot;-&quot;, [ &quot;route&quot;, { &quot;Ref&quot;: &quot;Project&quot; }, { &quot;Ref&quot; : &quot;Environment&quot; } ] ] } },
          { &quot;Key&quot; : &quot;Project&quot;, &quot;Value&quot; : { &quot;Ref&quot;: &quot;Project&quot; } },
          { &quot;Key&quot; : &quot;Environment&quot;, &quot;Value&quot; : { &quot;Ref&quot;: &quot;Environment&quot; } }
        ]
      }
    },
    &quot;PublicRoute&quot; : {
      &quot;Type&quot; : &quot;AWS::EC2::Route&quot;,
      &quot;DependsOn&quot; : &quot;GatewayToInternet&quot;,
      &quot;Properties&quot; : {
        &quot;RouteTableId&quot; : { &quot;Ref&quot; : &quot;PublicRouteTable&quot; },
        &quot;DestinationCidrBlock&quot; : &quot;0.0.0.0/0&quot;,
        &quot;GatewayId&quot; : { &quot;Ref&quot; : &quot;InternetGateway&quot; }
      }
    },
    &quot;VPCSubnet0RouteTableAssociation&quot; : {
      &quot;Type&quot; : &quot;AWS::EC2::SubnetRouteTableAssociation&quot;,
      &quot;Properties&quot; : {
        &quot;SubnetId&quot; : { &quot;Ref&quot; : &quot;VPCSubnet0&quot; },
        &quot;RouteTableId&quot; : { &quot;Ref&quot; : &quot;PublicRouteTable&quot; }
      }
    },
    &quot;VPCSubnet1RouteTableAssociation&quot; : {
      &quot;Type&quot; : &quot;AWS::EC2::SubnetRouteTableAssociation&quot;,
      &quot;Properties&quot; : {
        &quot;SubnetId&quot; : { &quot;Ref&quot; : &quot;VPCSubnet1&quot; },
        &quot;RouteTableId&quot; : { &quot;Ref&quot; : &quot;PublicRouteTable&quot; }
      }
    },
    &quot;VPCSubnet2RouteTableAssociation&quot; : {
      &quot;Type&quot; : &quot;AWS::EC2::SubnetRouteTableAssociation&quot;,
      &quot;Properties&quot; : {
        &quot;SubnetId&quot; : { &quot;Ref&quot; : &quot;VPCSubnet2&quot; },
        &quot;RouteTableId&quot; : { &quot;Ref&quot; : &quot;PublicRouteTable&quot; }
      }
    },
    &quot;InstanceRole&quot;: {
      &quot;Type&quot;: &quot;AWS::IAM::Role&quot;,
      &quot;Properties&quot;: {
        &quot;AssumeRolePolicyDocument&quot;: {
          &quot;Version&quot;: &quot;2012-10-17&quot;,
          &quot;Statement&quot;: [
            {
              &quot;Effect&quot;: &quot;Allow&quot;,
              &quot;Principal&quot;: {
                &quot;Service&quot;: [ &quot;ec2.amazonaws.com&quot; ]
              },
              &quot;Action&quot;: [ &quot;sts:AssumeRole&quot; ]
            }
          ]
        },
        &quot;Path&quot;: &quot;/&quot;,
        &quot;Policies&quot;: [
          {
            &quot;PolicyName&quot;: &quot;ApplicationPolicy&quot;,
            &quot;PolicyDocument&quot;: {
              &quot;Version&quot;: &quot;2012-10-17&quot;,
              &quot;Statement&quot;: [
                {
                  &quot;Effect&quot;: &quot;Allow&quot;,
                  &quot;Action&quot;: [
                    &quot;elasticbeanstalk:*&quot;,
                    &quot;elastiCache:*&quot;,
                    &quot;ec2:*&quot;,
                    &quot;elasticloadbalancing:*&quot;,
                    &quot;autoscaling:*&quot;,
                    &quot;cloudwatch:*&quot;,
                    &quot;dynamodb:*&quot;,
                    &quot;s3:*&quot;,
                    &quot;sns:*&quot;,
                    &quot;sqs:*&quot;,
                    &quot;cloudformation:*&quot;,
                    &quot;rds:*&quot;,
                    &quot;iam:AddRoleToInstanceProfile&quot;,
                    &quot;iam:CreateInstanceProfile&quot;,
                    &quot;iam:CreateRole&quot;,
                    &quot;iam:PassRole&quot;,
                    &quot;iam:ListInstanceProfiles&quot;
                  ],
                  &quot;Resource&quot;: &quot;*&quot;
                }
              ]
            }
          }
        ]
      }
    },
    &quot;InstanceProfile&quot;: {
       &quot;Type&quot;: &quot;AWS::IAM::InstanceProfile&quot;,
       &quot;Properties&quot;: {
          &quot;Path&quot;: &quot;/&quot;,
          &quot;Roles&quot;: [ { &quot;Ref&quot;: &quot;InstanceRole&quot; } ]
       }
    },
    &quot;VPCSecurityGroup&quot; : {
      &quot;Type&quot; : &quot;AWS::EC2::SecurityGroup&quot;,
      &quot;Properties&quot; : {
        &quot;GroupDescription&quot; : { &quot;Fn::Join&quot;: [ &quot;&quot;, [ &quot;VPC Security Group for &quot;, { &quot;Fn::Join&quot;: [ &quot;-&quot;, [ { &quot;Ref&quot;: &quot;Project&quot; }, { &quot;Ref&quot;: &quot;Environment&quot; } ] ] } ] ] },
        &quot;SecurityGroupIngress&quot; : [
          {&quot;IpProtocol&quot;: &quot;tcp&quot;, &quot;FromPort&quot; : &quot;22&quot;,  &quot;ToPort&quot; : &quot;22&quot;,  &quot;CidrIp&quot; : { &quot;Ref&quot; : &quot;SSHFrom&quot; }},
          {&quot;IpProtocol&quot;: &quot;tcp&quot;, &quot;FromPort&quot;: &quot;80&quot;, &quot;ToPort&quot;: &quot;80&quot;, &quot;CidrIp&quot;: &quot;0.0.0.0/0&quot; },
          {&quot;IpProtocol&quot;: &quot;tcp&quot;, &quot;FromPort&quot;: &quot;443&quot;, &quot;ToPort&quot;: &quot;443&quot;, &quot;CidrIp&quot;: &quot;0.0.0.0/0&quot; }
        ],
        &quot;VpcId&quot; : { &quot;Ref&quot; : &quot;VPC&quot; },
        &quot;Tags&quot; : [
          { &quot;Key&quot; : &quot;Name&quot;, &quot;Value&quot; : { &quot;Fn::Join&quot;: [ &quot;-&quot;, [ &quot;sg&quot;, { &quot;Ref&quot;: &quot;Project&quot; }, { &quot;Ref&quot; : &quot;Environment&quot; } ] ] } },
          { &quot;Key&quot; : &quot;Project&quot;, &quot;Value&quot; : { &quot;Ref&quot;: &quot;Project&quot; } },
          { &quot;Key&quot; : &quot;Environment&quot;, &quot;Value&quot; : { &quot;Ref&quot;: &quot;Environment&quot; } }
        ]
      }
    },
    &quot;VPCSGIngress&quot;: {
      &quot;Type&quot;: &quot;AWS::EC2::SecurityGroupIngress&quot;,
      &quot;Properties&quot;: {
        &quot;GroupId&quot;: { &quot;Ref&quot; : &quot;VPCSecurityGroup&quot; },
        &quot;IpProtocol&quot;: &quot;-1&quot;,
        &quot;FromPort&quot;: &quot;0&quot;,
        &quot;ToPort&quot;: &quot;65535&quot;,
        &quot;SourceSecurityGroupId&quot;: { &quot;Ref&quot;: &quot;VPCSecurityGroup&quot; }
      }
    }
  },
  &quot;Outputs&quot; : {
    &quot;VpcId&quot; : {
      &quot;Description&quot; : &quot;VPC Id&quot;,
      &quot;Value&quot; :  { &quot;Ref&quot; : &quot;VPC&quot; }
    },
    &quot;VPCDefaultNetworkAcl&quot; : {
      &quot;Description&quot; : &quot;VPC&quot;,
      &quot;Value&quot; :  { &quot;Fn::GetAtt&quot; : [&quot;VPC&quot;, &quot;DefaultNetworkAcl&quot;] }
    },
    &quot;VPCDefaultSecurityGroup&quot; : {
      &quot;Description&quot; : &quot;VPC Default Security Group that we blissfully ignore thanks to self-referencing bugs&quot;,
      &quot;Value&quot; :  { &quot;Fn::GetAtt&quot; : [&quot;VPC&quot;, &quot;DefaultSecurityGroup&quot;] }
    },
    &quot;VPCSecurityGroup&quot; : {
      &quot;Description&quot; : &quot;VPC Security Group created by this stack&quot;,
      &quot;Value&quot; :  { &quot;Ref&quot;: &quot;VPCSecurityGroup&quot; }
    },
    &quot;VPCSubnet0&quot;: {
      &quot;Description&quot;: &quot;The subnet id for VPCSubnet0&quot;,
      &quot;Value&quot;: {
        &quot;Ref&quot;: &quot;VPCSubnet0&quot;
      }
    },
    &quot;VPCSubnet1&quot;: {
      &quot;Description&quot;: &quot;The subnet id for VPCSubnet1&quot;,
      &quot;Value&quot;: {
        &quot;Ref&quot;: &quot;VPCSubnet1&quot;
      }
    },
    &quot;VPCSubnet2&quot;: {
      &quot;Description&quot;: &quot;The subnet id for VPCSubnet2&quot;,
      &quot;Value&quot;: {
        &quot;Ref&quot;: &quot;VPCSubnet2&quot;
      }
    }
  }
}</code></pre></noscript></div>

<p>Here is an example CloudFormation parameters file for this template:</p>

<div><script src="https://gist.github.com/9f4b8dd2b39c7d1c31ef.js"></script>
<noscript><pre><code>[
  { &quot;ParameterKey&quot;: &quot;Project&quot;, &quot;ParameterValue&quot;: &quot;myapp&quot; },
  { &quot;ParameterKey&quot;: &quot;Environment&quot;, &quot;ParameterValue&quot;: &quot;dev&quot; },
  { &quot;ParameterKey&quot;: &quot;VPCNetworkCIDR&quot;, &quot;ParameterValue&quot;: &quot;10.0.0.0/16&quot; },
  { &quot;ParameterKey&quot;: &quot;VPCSubnet0CIDR&quot;, &quot;ParameterValue&quot;: &quot;10.0.0.0/24&quot; },
  { &quot;ParameterKey&quot;: &quot;VPCSubnet1CIDR&quot;, &quot;ParameterValue&quot;: &quot;10.0.1.0/24&quot; },
  { &quot;ParameterKey&quot;: &quot;VPCSubnet2CIDR&quot;, &quot;ParameterValue&quot;: &quot;10.0.2.0/24&quot; }
]</code></pre></noscript></div>

<p>To script the creation, updating, watching, and deleting of the CloudFormation VPC, I have this Makefile as well:</p>

<div><script src="https://gist.github.com/55b740ff19825d621ef4.js"></script>
<noscript><pre><code>STACK:=myapp-dev
TEMPLATE:=cloudformation-template_vpc-iam.json
PARAMETERS:=cloudformation-parameters_myapp-dev.json
AWS_REGION:=us-east-1
AWS_PROFILE:=aws-dev

all:
	@which aws || pip install awscli
	aws cloudformation create-stack --stack-name $(STACK) --template-body file://`pwd`/$(TEMPLATE) --parameters file://`pwd`/$(PARAMETERS) --capabilities CAPABILITY_IAM --profile $(AWS_PROFILE) --region $(AWS_REGION)

update:
	aws cloudformation update-stack --stack-name $(STACK) --template-body file://`pwd`/$(TEMPLATE) --parameters file://`pwd`/$(PARAMETERS) --capabilities CAPABILITY_IAM --profile $(AWS_PROFILE) --region $(AWS_REGION)

events:
	aws cloudformation describe-stack-events --stack-name $(STACK) --profile $(AWS_PROFILE) --region $(AWS_REGION)

watch:
	watch --interval 10 &quot;bash -c &#39;make events | head -25&#39;&quot;
	
outputs:
	@which jq || ( which brew &amp;&amp; brew install jq || which apt-get &amp;&amp; apt-get install jq || which yum &amp;&amp; yum install jq || which choco &amp;&amp; choco install jq)
	aws cloudformation describe-stacks --stack-name $(STACK) --profile $(AWS_PROFILE) --region $(AWS_REGION) | jq -r &#39;.Stacks[].Outputs&#39;

delete:
	aws cloudformation delete-stack --stack-name $(STACK) --profile $(AWS_PROFILE) --region $(AWS_REGION)
</code></pre></noscript></div>

<p>You can get these same files by cloning my github project, and ssuming you have a profile named <code>aws-dev</code> as mentioned above, you can even run <code>make</code> and have it create the <code>myapp-dev</code> VPC via CloudFormation:</p>

<pre><code>git clone https://github.com/ianblenke/aws-docker-walkthrough
cd aws-docker-walkthrough
make
</code></pre>

<p>You can run <code>make watch</code> to watch the CloudFormation events and wait for a <code>CREATE_COMPLETE</code> state.</p>

<p>When this is complete, you can see the CloudFormation outputs by running:</p>

<pre><code>make output
</code></pre>

<p>The output will look something like this:</p>

<div><script src="https://gist.github.com/59715079304a6db7182c.js"></script>
<noscript><pre><code>aws cloudformation describe-stacks --stack-name myapp-dev --profile aws-dev --region us-east-1 | jq -r &#39;.Stacks[].Outputs&#39;
[
  {
    &quot;Description&quot;: &quot;VPC Id&quot;,
    &quot;OutputKey&quot;: &quot;VpcId&quot;,
    &quot;OutputValue&quot;: &quot;vpc-b7d1d8d2&quot;
  },
  {
    &quot;Description&quot;: &quot;VPC&quot;,
    &quot;OutputKey&quot;: &quot;VPCDefaultNetworkAcl&quot;,
    &quot;OutputValue&quot;: &quot;acl-b3cfc7d6&quot;
  },
  {
    &quot;Description&quot;: &quot;VPC Default Security Group that we blissfully ignore thanks to self-referencing bugs&quot;,
    &quot;OutputKey&quot;: &quot;VPCDefaultSecurityGroup&quot;,
    &quot;OutputValue&quot;: &quot;sg-3e50a559&quot;
  },
  {
    &quot;Description&quot;: &quot;VPC Security Group created by this stack&quot;,
    &quot;OutputKey&quot;: &quot;VPCSecurityGroup&quot;,
    &quot;OutputValue&quot;: &quot;sg-0c50a56b&quot;
  },
  {
    &quot;Description&quot;: &quot;The subnet id for VPCSubnet0&quot;,
    &quot;OutputKey&quot;: &quot;VPCSubnet0&quot;,
    &quot;OutputValue&quot;: &quot;subnet-995236b2&quot;
  },
  {
    &quot;Description&quot;: &quot;The subnet id for VPCSubnet1&quot;,
    &quot;OutputKey&quot;: &quot;VPCSubnet1&quot;,
    &quot;OutputValue&quot;: &quot;subnet-6aa4fd1d&quot;
  },
  {
    &quot;Description&quot;: &quot;The subnet id for VPCSubnet2&quot;,
    &quot;OutputKey&quot;: &quot;VPCSubnet2&quot;,
    &quot;OutputValue&quot;: &quot;subnet-ad3644f4&quot;
  }
]</code></pre></noscript></div>

<p>These CloudFormation Outputs list parameters that we will need to pass to the ElasticBeanstalk Environment creation during the next part of this walkthrough. </p>

<p>Stay tuned…</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2015/03/10/deploying-amazon-ecs-on-coreos/">Deploying Amazon ECS on CoreOS</a></h1>
    
    
      <p class="meta">
        








  


Mar 3, 2015
        
           | <a href="/blog/2015/03/10/deploying-amazon-ecs-on-coreos/#disqus_thread"
             data-disqus-identifier="http://ian.blenke.com/blog/2015/03/10/deploying-amazon-ecs-on-coreos/">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>Today, I stumbled on the official <a href="https://coreos.com/docs/running-coreos/cloud-providers/ecs/">CoreOS page on ECS</a>.</p>

<p>I’ve been putting off ECS for a while, it was time to give it a try.</p>

<p>To create the ECS cluster, we will need the aws commandline tool:</p>

<pre><code>which aws || pip install awscli
</code></pre>

<p>Make sure you have your <code>AWS_ACCESS_KEY_ID</code> and <code>AWS_SECRET_ACCESS_KEY</code> defined in your shell environment.</p>

<p>Create the ECS cluster:</p>

<pre><code>aws ecs create-cluster --cluster-name Cosmos-Dev
{
    "cluster": {
        "clusterName": "Cosmos-Dev",
        "status": "ACTIVE",
        "clusterArn": "arn:aws:ecs:us-east-1:123456789012:cluster/My-ECS-Cluster"
    }
}
</code></pre>

<p>Install the global fleet unit for amazon-ecs-agent.service:</p>

<pre><code>cat &lt;&lt;EOF &gt; amazon-ecs-agent.service
[Unit]
Description=Amazon ECS Agent
After=docker.service
Requires=docker.service
[Service]
Environment=ECS_CLUSTER=My-ECS-Cluster
Environment=ECS_LOGLEVEL=warn
Environment=AWS_REGION=us-east-1
ExecStartPre=-/usr/bin/docker kill ecs-agent
ExecStartPre=-/usr/bin/docker rm ecs-agent
ExecStartPre=/usr/bin/docker pull amazon/amazon-ecs-agent
ExecStart=/usr/bin/docker run --name ecs-agent \
    --env=ECS_CLUSTER=${ECS_CLUSTER}\
    --env=ECS_LOGLEVEL=${ECS_LOGLEVEL} \
    --publish=127.0.0.1:51678:51678 \
    --volume=/var/run/docker.sock:/var/run/docker.sock \
    amazon/amazon-ecs-agent
ExecStop=/usr/bin/docker stop ecs-agent
[X-Fleet]
Global=true
EOF
fleetctl start amazon-ecs-agent.service
</code></pre>

<p>This registers a ContainerInstance to the <code>My-ECS-Cluster</code> in region <code>us-east-1</code>.</p>

<p>Note: this is using the EC2 instance’s instance profile IAM credentials. You will want to make sure you’ve assigned an instance profile with a Role that has “ecs:*” access.
For this, you may want to take a look at the <a href="https://s3.amazonaws.com/amazon-ecs-cloudformation/Amazon_ECS_Quickstart.template">Amazon ECS Quickstart CloudFormation template</a>.</p>

<p>Now from a CoreOS host, we can query locally to enumerate the running ContainerInstances in our fleet:</p>

<pre><code>fleetctl list-machines -fields=ip -no-legend | while read ip ; do \
    echo $ip $(ssh -n $ip curl -s http://169.254.169.254/latest/meta-data/instance-id) \
    $(ssh -n $ip curl -s http://localhost:51678/v1/metadata | \
      docker run -i realguess/jq jq .ContainerInstanceArn) ; \
  done
</code></pre>

<p>Which returns something like:</p>

<pre><code>10.113.0.23 i-12345678 "arn:aws:ecs:us-east-1:123456789012:container-instance/674140ae-1234-4321-1234-4abf7878caba"
10.113.1.42 i-23456789 "arn:aws:ecs:us-east-1:123456789012:container-instance/c3506771-1234-4321-1234-1f1b1783c924"
10.113.2.66 i-34567891 "arn:aws:ecs:us-east-1:123456789012:container-instance/75d30c64-1234-4321-1234-8be8edeec9c6"
</code></pre>

<p>And we can query ECS and get the same:</p>

<pre><code>$ aws ecs list-container-instances --cluster My-ECS-Cluster | grep arn | cut -d'"' -f2 | \
  xargs -L1 -I% aws ecs describe-container-instances --cluster My-ECS-Cluster --container-instance % | \
  jq '.containerInstances[] | .ec2InstanceId + " " + .containerInstanceArn'
"i-12345678 arn:aws:ecs:us-east-1:123456789012:container-instance/674140ae-1234-4321-1234-4abf7878caba"
"i-23456789 arn:aws:ecs:us-east-1:123456789012:container-instance/c3506771-1234-4321-1234-1f1b1783c924"
"i-34567891 arn:aws:ecs:us-east-1:123456789012:container-instance/75d30c64-1234-4321-1234-8be8edeec9c6"
</code></pre>

<p>This ECS cluster is ready to use.</p>

<p>Unfortunately, there is no scheduler here. ECS is a harness for orchestrating docker containers in a cluster as <em>tasks</em>. </p>

<p>Where these tasks are allocated is left up to the AWS customer.</p>

<p>What we really need is a <em>scheduler</em>.</p>

<p>CoreOS has a form of a scheduler in fleet, but that is for fleet units of systemd services, and is not limited to docker containers as ECS is.
Fleet’s scheduler is also currently a bit weak in that it schedules new units to the fleet machine with the fewest number of units.</p>

<p>Kubernetes has a random scheduler, which is better in a couple ways, but does not fairly allocate the system resources.</p>

<p>The <em>best</em> scheduler at present is Mesos, which takes into account resource sizing estimates and current utilization.</p>

<p>Normally, Mesos uses Mesos Slaves to run work. Mesos can also use ECS as a backend instead.</p>

<p>My next steps: Deploy Mesos using the <a href="https://github.com/awslabs/ecs-mesos-scheduler-driver">ecs-mesos-scheduler-driver</a>, as <a href="http://jpetazzo.github.io/2015/01/14/amazon-docker-ecs-ec2-container-service/">summarized by jpetazzo</a></p>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/11/10/docker-rspec-tdd/">Docker Rspec TDD</a></h1>
    
    
      <p class="meta">
        








  


Nov 10, 2014
        
           | <a href="/blog/2014/11/10/docker-rspec-tdd/#disqus_thread"
             data-disqus-identifier="http://ian.blenke.com/blog/2014/11/10/docker-rspec-tdd/">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>A Dockerfile both describes a Docker image as well as layers for the working directory, environment variables, ports, entrypoint commands, and other important interfaces.</p>

<p>Test-Driven Design should drive a developer toward implementation details, not the other way around.</p>

<p>A devops without tests is a sad devops indeed.</p>

<p>Working toward a docker based development environment, my first thoughts were toward <a href="http://serverspec.org/">Serverspec</a> by <a href="https://github.com/mizzy">Gosuke Miayshita</a>, as it is entirely framework agnostic. Gosuke gave an excellent presentation at ChefConf this year re-inforcing that Serverspec is <em>not</em> a chef centric tool, and works quite well in conjunction with other configuration management tools.</p>

<p>Researching Serverspec and docker a bit more, <a href="https://github.com/tcnksm">Taichi Nakashima</a> based his <a href="https://github.com/tcnksm-sample/docker-rspec">TDD of Dockerfile by RSpec on OS/X</a> using ssh.</p>

<p>With Docker 1.3 and later, there is a “docker exec” interactive docker API for allowing live sessions on processes spawned in the same process namespace as a running container, effectively allowing external access into a running docker container using only the docker API.</p>

<p><a href="http://blog.wercker.com/2013/12/23/Test-driven-development-for-docker.html">PIETER JOOST VAN DE SANDE</a> posted about using the docker-api to accomplish the goal of testing a Dockerfile. His work is based on the <a href="https://rubygems.org/gems/docker-api">docker-api</a> gem (github <a href="https://github.com/swipely/docker-api">swipely/docker-api</a>).</p>

<p>Looking into the docker-api source, there is no support yet for docker 1.3’s exec API interface to run Serverspec tests against the contents of a running docker container.</p>

<p>Attempting even the most basic docker API calls with docker-api, <a href="https://github.com/swipely/docker-api/issues/202">issue 202</a> made it apparent that TLS support for boot2docker would need to be addressed first.</p>

<p>Here is my functional <code>spec_helper.rb</code> with the fixes necessary to use docker-api without modifications:</p>

<div><script src="https://gist.github.com/5335483e4021954d815f.js"></script>
<noscript><pre><code>require &quot;docker&quot;

docker_host = ENV[&#39;DOCKER_HOST&#39;].dup

if(ENV[&#39;DOCKER_TLS_VERIFY&#39;])
  cert_path = File.expand_path ENV[&#39;DOCKER_CERT_PATH&#39;]
  Docker.options = {
    client_cert: File.join(cert_path, &#39;cert.pem&#39;),
    client_key: File.join(cert_path, &#39;key.pem&#39;)
  }
  Excon.defaults[:ssl_ca_file] = File.join(cert_path, &#39;ca.pem&#39;)
  docker_host.gsub!(/^tcp/,&#39;https&#39;)
end

Docker.url = docker_host</code></pre></noscript></div>

<p>Following this, I can drive the generation of a Dockerfile with a spec:</p>

<div><script src="https://gist.github.com/261e6fe930c922202151.js"></script>
<noscript><pre><code>require &quot;spec_helper&quot;

describe &quot;dockerfile built my_app image&quot; do
  before(:all) do
    @image = Docker::Image.all(:all =&gt; true).find { |image|
      Docker::Util.parse_repo_tag( image.info[&#39;RepoTags&#39;].first ).first == &#39;my_app&#39;
    }
    p @image.json[&quot;Env&quot;]
  end

  it &quot;should exist&quot; do
    expect(@image).not_to be_nil
  end

  it &quot;should have CMD&quot; do
    expect(@image.json[&quot;Config&quot;][&quot;Cmd&quot;]).to include(&quot;/run.sh&quot;)
  end

  it &quot;should expose the default port&quot; do
    expect(@image.json[&quot;Config&quot;][&quot;ExposedPorts&quot;].has_key?(&quot;3000/tcp&quot;)).to be_truthy
  end

  it &quot;should have environmental variable&quot; do
    expect(@image.json[&quot;Config&quot;][&quot;Env&quot;]).to include(&quot;HOME=/usr/src/app&quot;)
  end
end</code></pre></noscript></div>

<p>This drives me iteratively to write a Dockerfile that looks like:</p>

<div><script src="https://gist.github.com/ef7150420cbd328d5e41.js"></script>
<noscript><pre><code>FROM rails:onbuild
ENV HOME /usr/src/app
ADD docker/run.sh /run.sh
RUN chmod 755 /run.sh
EXPOSE 3000
CMD /run.sh</code></pre></noscript></div>

<p>Next step: extend docker-api to support exec for serverspec based testing of actual docker image contents.</p>

<p>Sláinte!</p>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/11/09/openstack-on-a-chromebox/">OpenStack on a Chromebox</a></h1>
    
    
      <p class="meta">
        








  


Nov 9th, 2014
        
           | <a href="/blog/2014/11/09/openstack-on-a-chromebox/#disqus_thread"
             data-disqus-identifier="http://ian.blenke.com/blog/2014/11/09/openstack-on-a-chromebox/">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>Saturday’s project was installing OpenStack on a ChromeBox.</p>

<h2 id="step-0-identify-your-hardware-add-ram">Step 0: Identify your hardware, add RAM</h2>

<p>Before you begin, make sure you <a href="http://www.chromium.org/chromium-os/developer-information-for-chrome-os-devices">know which ChromeOS device you have</a>.</p>

<p>In my case, it was a <a href="http://www.chromium.org/chromium-os/developer-information-for-chrome-os-devices/samsung-sandy-bridge">Samsung Series 3 Chromebox</a>.</p>

<p>Thankfully, the memory was very easy to upgrade to 16G, as the bottom snaps right off.</p>

<p><img src="/images/chromebox/stumpy-top-mid.jpg" alt="Samsung Series 3 Chromebox with bottom exposed" /></p>

<h2 id="step-1-make-a-chromeos-recovery-usb">Step 1: Make a ChromeOS recovery USB</h2>

<p>Plug in a 4G or larger USB stick, then open this URL on your ChromeOS device:</p>

<p><a href="chrome://imageburner">chrome://imageburner</a></p>

<p>Follow the instructions.</p>

<p>We shouldn’t need this, but you never know. (And, yes, I did end up needing it during one of my iterations while writing this post).</p>

<h2 id="step-2-enable-developer-mode">Step 2: Enable developer mode</h2>

<p>The switch in the image is how I put my ChromeBox <a href="http://www.chromium.org/chromium-os/poking-around-your-chrome-os-device#TOC-Putting-your-Chrome-OS-Device-into-Developer-Mode">into development mode</a>.</p>

<p><img src="/images/chromebox/stumpy-dev-switch.jpg" alt="Dev Switch on a Samsung Chromebox" /></p>

<p>After flipping the switch, reboot.</p>

<p>On this first reboot, the existing on-board storage will be wiped entirely, erasing any account credentials and cached content.</p>

<h2 id="step-3-login-to-the-console-as-chronos">Step 3: Login to the console as “chronos”</h2>

<p>Using cntl-alt-F2, enter the username “chronos” at the login: prompt. Hit return at the password: prompt (the default chronos password is an empty string).</p>

<p>Note: You did not actually need to login to google via the UI interface.</p>

<h2 id="step-4-enable-usb-and-seabios-booting">Step 4: Enable USB and SeaBIOS booting</h2>

<p>Now that you have a shell as chronos, you can enable USB booting.</p>

<pre><code>sudo crossystem dev_boot_usb=1
</code></pre>

<p>After enabling and rebooting, you can now boot from USB drives with <code>cntl-u</code></p>

<p>In order to install Ubuntu (or your distro of choice), we need to legacy boot. This requires a BIOS.</p>

<p>Newer ChromeBox hardware includes SeaBIOS natively.</p>

<pre><code>sudo crossystem dev_boot_legacy=1
</code></pre>

<p>After enabling and rebooting, you can now boot to legacy images with <code>cntl-l</code></p>

<p>If you have an older ChromeBox (like the Samsung Series 3) that doesn’t have a SeaBIOS boot, you will need to flash one.</p>

<p>Flashing a new bootstrap requires a jumper or other physical change to hardware to allow the <a href="http://flashrom.org/Flashrom">flashrom</a> tool to write to flash.</p>

<p>NOTE: <b>ASSUME THAT THIS WILL LIKELY BRICK YOUR CHROMEBOX. YOU HAVE BEEN WARNED</b></p>

<p>On the Samsung Series 3 ChromeBox, the jumper looks like this:</p>

<p><img src="/images/chromebox/spi-flash-chromebox.jpg" alt="Samsung Series 3 flash jumper" /></p>

<p>A bit of folded tin-foil made for a quick jumper.</p>

<p><a href="http://johnlewis.ie/">John Lewis</a> is <a href="https://johnlewis.ie/custom-chromebook-firmware/rom-download/">maintaining a marvelous script</a> that auto-detects and flashes an updated SeaBIOS for most ChromeBook/ChromeBox hardware:</p>

<div><script src="https://gist.github.com/22e429b4424b74e51869.js"></script>
<noscript><pre><code>wget https://johnlewis.ie/getnflash_johnlewis_rom.sh
chmod u+x getnflash_johnlewis_rom.sh
./getnflash_johnlewis_rom.sh</code></pre></noscript></div>

<p>The script makes you type “If this bricks my Chromebook/box, on my head be it!” to make sure you understand that you are most likely going to brick your chromebox/chromebook by proceeding. This is no joke.</p>

<p>Being ok with potentially bricking my ChromeBox, I went ahead.</p>

<p>The script ran to completion without errors, and was thoroughly successful.</p>

<p>After rebooting, I now get a SeaBIOS splash identification (rather than the eventual sick computer).</p>

<p>The downside to doing this is that I now <em>must</em> boot off of an external USB device, as the SeaBIOS doesn’t seem to support booting from the built-in MMC SSD anymore.</p>

<h2 id="step-5-install-your-linux-distribution">Step 5: Install your Linux distribution</h2>

<p>I went ahead and pulled an Ubuntu Trust 14.04 ISO and DD’ed it to a USB stick on my Mac.</p>

<pre><code>wget -c http://releases.ubuntu.com/14.04/ubuntu-14.04-desktop-amd64.iso
diskutil list
hdiutil unmount /Volumes/USBSTICK
sudo dd if=ubuntu-14.04-desktop-amd64.iso of=/dev/disk5 bs=1m
</code></pre>

<p>After it finished flashing, I removed the USB stick from my Mac and plugged it into the front of the ChromeBox.</p>

<p>The USB installation media for Ubuntu was detected by SeaBIOS as the second bootable USB device.</p>

<p>I also attached 2 external 1TB USB disks to the back as the media that will be installed to.
These appeared as the third and fourth bootable devices to SeaBIOS.</p>

<p>With my new SeaBIOS bootstrap, I now must hit “Esc” and “2” to boot off of the first USB stick for the Ubuntu installation.</p>

<p>This presented me with the Ubunu boot menu.</p>

<p>Beyond this point, I installed Ubuntu to the two external 1TB USB disks, with a primary boot partition (type “83”) on each for /boot as normal linux ext4, and a primary RAID partition (type “fd”) on each for the RAID1 mirror upon which I layered LVM with a volume group named “vg” and a “rootfs” and a “swapfs” logical volume. At the end, I installed the grub boot sector to /dev/sdb and /dev/sdc (the two external 1TB USB drives).</p>

<p>After removing the USB stick for the Ubuntu installation media, the SeaBIOS entries shifted by 1.</p>

<p>With my new SeaBIOS bootstrap, I now must hit “Esc” and “2” to boot off of the first USB 1TB drive, or “3” for the second USB 1TB drive.</p>

<p>When I figure out how to get around the SeaBIOS hang on boot if I don’t do this, I will update this blog post.</p>

<h2 id="step-4-devstack-installation-of-openstack">Step 4: Devstack installation of OpenStack</h2>

<p>From this point forward, I followed the <a href="http://devstack.org">DevStack</a> <a href="http://docs.openstack.org/developer/devstack/guides/single-machine.html">All-in-one single-machine install guide</a>.</p>

<p>My local.conf for the all-in-one install is a collection of bits and pieces collected while digging around:</p>

<div><script src="https://gist.github.com/7084bf5a815d4bdc474c.js"></script>
<noscript><pre><code>[[local|localrc]]

SERVICE_TOKEN=a682f596-76f3-11e3-b3b2-e716f9080d50
ADMIN_PASSWORD=nomoresecrets
MYSQL_PASSWORD=iheartdatabases
RABBIT_PASSWORD=flopsymopsy
SERVICE_PASSWORD=$ADMIN_PASSWORD
#HOST_IP=10.0.0.106
DEST=/opt/stack
LOGFILE=$DEST/logs/stack.sh.log
LOGDAYS=7
#LOG_COLOR=False
# Uncomment these to grab the milestone-proposed branches from the repos:
#CINDER_BRANCH=milestone-proposed
#GLANCE_BRANCH=milestone-proposed
#HORIZON_BRANCH=milestone-proposed
#KEYSTONE_BRANCH=milestone-proposed
#KEYSTONECLIENT_BRANCH=milestone-proposed
#NOVA_BRANCH=milestone-proposed
#NOVACLIENT_BRANCH=milestone-proposed
#NEUTRON_BRANCH=milestone-proposed
#SWIFT_BRANCH=milestone-proposed
SWIFT_HASH=66a3d6b56c1f479c8b4e70ab5c2000f5
SWIFT_REPLICAS=1
SWIFT_DATA_DIR=$DEST/data/swift
FIXED_RANGE=10.1.0.0/24
FLOATING_RANGE=10.2.0.0/24
FIXED_NETWORK_SIZE=256
PUBLIC_INTERFACE=eth0
NET_MAN=FlatDHCPManager
FLAT_NETWORK_BRIDGE=br100
SCREEN_LOGDIR=$DEST/logs/screen
VOLUME_GROUP=&quot;vg&quot;
VOLUME_NAME_PREFIX=&quot;cinder-&quot;
VOLUME_BACKING_FILE_SIZE=10250M
API_RATE_LIMIT=False
VIRT_DRIVER=libvirt
LIBVIRT_TYPE=kvm
SCHEDULER=nova.scheduler.simple.SimpleScheduler</code></pre></noscript></div>

<p>As the stack user, running <code>./stack.sh</code> kicked off the install, and it completed successfully.</p>

<p>At the end, it tells you the URLs to use to access your new OpenStack server:</p>

<pre><code>Horizon is now available at http://10.0.0.106/
Keystone is serving at http://10.0.0.106:5000/v2.0/
Examples on using novaclient command line is in exercise.sh
The default users are: admin and demo
The password: nomoresecrets
This is your host ip: 10.0.0.106
</code></pre>

<p>I also ended up creating this <code>~/.fog</code> file locally on my Mac, based on <a href="http://docs.cloudfoundry.org/deploying/openstack/validate_openstack.html">CloudFoundry’s guide to validating your OpenStack</a>.</p>

<div><script src="https://gist.github.com/c87328c5635cdcadb8e0.js"></script>
<noscript><pre><code>:openstack:
  :openstack_auth_url:  http://10.0.0.106:5000/v2.0/tokens
  :openstack_api_key:   nomoresecrets
  :openstack_username:  admin
  :openstack_tenant:    admin
  :openstack_region:    RegionOne # Optional
  </code></pre></noscript></div>

<p>With it, I can now use the <a href="http://fog.io">fog</a> command-line tool locally on my development Mac to manipulate the ChromeBox based OpenStack server.</p>

<p>Cheers!</p>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/11/07/fig-docker/">Fig-docker</a></h1>
    
    
      <p class="meta">
        








  


Nov 7th, 2014
        
           | <a href="/blog/2014/11/07/fig-docker/#disqus_thread"
             data-disqus-identifier="http://ian.blenke.com/blog/2014/11/07/fig-docker/">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>A common devops problem when developing <a href="http://docker.io">Docker</a> containers is managing the orchestration of multiple containers in a development environment.</p>

<p>There are a number of orchestration harnesses for Docker available:</p>

<ul>
  <li>Docker’s <a href="http://fig.sh">Fig</a></li>
  <li><a href="https://github.com/dcm-oss/blockade">blockade</a></li>
  <li><a href="https://docs.vagrantup.com/v2/provisioning/docker.html">Vagrant</a></li>
  <li><a href="https://github.com/GoogleCloudPlatform/kubernetes">kubernetes</a></li>
  <li><a href="https://github.com/signalfuse/maestro-ng">maestro-ng</a></li>
  <li><a href="https://github.com/michaelsauter/crane">crane</a></li>
  <li>Centurylink’s <a href="http://panamax.io/">Panamax</a></li>
  <li><a href="http://shipyard-project.com/">Shipyard</a></li>
  <li><a href="http://decking.io/">Decking</a></li>
  <li>NewRelic’s <a href="https://github.com/newrelic/centurion">Centurion</a></li>
  <li>Spotify’s <a href="https://github.com/spotify/helios">Helios</a></li>
  <li><a href="https://github.com/cattleio/stampede">Stampede</a></li>
  <li><a href="https://www.getchef.com/solutions/docker/">Chef</a></li>
  <li><a href="http://www.ansible.com/docker">Ansible</a></li>
  <li><a href="https://flynn.io/">Flynn</a></li>
  <li><a href="https://github.com/mailgun/shipper">Shipper</a></li>
  <li><a href="http://octohost.io">Octohost</a></li>
  <li><a href="http://tsuru.io/">Tsuru</a> with <a href="https://github.com/tsuru/docker-cluster">docker-cluster</a></li>
  <li><a href="https://clusterhq.com/">Flocker</a></li>
  <li><a href="https://github.com/CloudCredo/cloudfocker">CloudFocker</a></li>
  <li><a href="http://www.cloudsoftcorp.com/blog/2014/06/clocker-creating-a-docker-cloud-with-apache-brooklyn/">Clocker</a> and <a href="http://brooklyn.incubator.apache.org">Apache Brooklyn</a></li>
  <li><a href="http://cloudfoundry.org">CloudFoundry</a>’s <a href="https://github.com/cf-platform-eng/docker-boshrelease">docker-boshrelease</a>/<a href="https://github.com/cloudfoundry-incubator/diego-release">diego</a></li>
  <li>Mesosphere <a href="https://github.com/mesosphere/deimos">Deimos</a></li>
  <li><a href="http://deis.io">Deis</a> (a PaaS that can git push deploy containers using <a href="http://heroku.com">Heroku</a> buildpacks <em>or</em> a Dockerfile)</li>
</ul>

<p>There are a number of hosted service offerings now as well:</p>

<ul>
  <li><a href="http://aws.amazon.com/ecs">Amazon ECS</a></li>
  <li><a href="http://www.qualisystems.com/cloudshell-6-0-sneak-peek/">CloudShell</a></li>
  <li><a href="https://elasticbox.com/how-it-works/">ElasticBox</a></li>
  <li>Aw, heck, just check the <a href="http://www.mindmeister.com/389671722/docker-ecosystem">docker ecosystem mindmap</a></li>
</ul>

<p>There are also RAFT/GOSSIP clustering solutions like:</p>

<ul>
  <li><a href="https://coreos.com/">CoreOS</a>/<a href="https://github.com/coreos/fleet">Fleet</a></li>
  <li><a href="https://www.openshift.com/products/origin">OpenShift Origin</a> uses <a href="http://www.projectatomic.io/">ProjectAtomic</a>/<a href="https://openshift.github.io/geard/">Geard</a></li>
</ul>

<p>My <a href="https://github.com/ianblenke/coreos-vagrant-kitchen-sink">coreos-vagrant-kitchen-sink</a> github project submits <a href="https://github.com/ianblenke/coreos-vagrant-kitchen-sink/tree/master/cloud-init">cloud-init units</a> via a YAML file when booting member nodes. It’s a good model for production, but it’s a bit heavy for development.</p>

<p>Docker is currently working on <a href="https://www.youtube.com/watch?v=vtnSL79rZ6o">Docker Clustering</a>, but it is presently just a proof-of-concept and is now under a total re-write.</p>

<p>They are also <a href="https://www.youtube.com/watch?v=YuSq6bXHnOI">implementing docker composition</a> which provides Fig like functionality using upcoming docker “groups”.</p>

<p>That influence of Fig makes sense, as <a href="http://venturebeat.com/2014/07/22/docker-buys-orchard-a-2-man-startup-with-a-cloud-service-for-running-docker-friendly-apps/">Docker bought Orchard</a>.</p>

<p>Internally, Docker developers use <a href="http://fig.sh">Fig</a>.</p>

<p>Docker’s website also directs everyone to <a href="http://boot2docker.io">Boot2Docker</a>, as that is the tool Docker developers use as their docker baseline environment. </p>

<p>Boot2Docker spawns a <a href="https://www.virtualbox.org/">VirtualBox</a> based VM as well as a native docker client runtime on the developer’s host machine, and provides the <code>DOCKER_HOST</code> and related enviroments necessary for the client to talk to the VM.</p>

<p>This allows a developer’s Windows or OS/X machine to have a docker command that behaves as if the docker containers are running natively on their host machine.</p>

<p>While Fig is easy to install under OS/X as it has native Python support (“pip install fig”), installing Fig on a Windows developer workstation would normally require Python support be installed separately.</p>

<p>Rather than do that, I’ve built a new <a href="https://registry.hub.docker.com/u/ianblenke/fig-docker/">ianblenke/fig-docker</a> docker Hub image, which is auto-built from <a href="https://github.com/ianblenke/docker-fig-docker">ianblenke/docker-fig-docker</a> on github.</p>

<p>This allows running fig inside a docker container using:</p>

<div><script src="https://gist.github.com/6cd8f8a4065bcac99443.js"></script>
<noscript><pre><code>docker run -v $(pwd):/app -v $DOCKER_CERT_PATH:/certs -e DOCKER_CERT_PATH=/certs -e DOCKER_HOST=$DOCKER_HOST -e DOCKER_TLS_VERIFY=$DOCKER_TLS_VERIFY -ti --rm ianblenke/fig-docker fig --help</code></pre></noscript></div>

<p>Alternatively, a developer can alias it:</p>

<div><script src="https://gist.github.com/f48bc5cf8b2567bd8006.js"></script>
<noscript><pre><code>alias fig=&quot;docker run -v $(pwd):/app -v $DOCKER_CERT_PATH:/certs -e DOCKER_CERT_PATH=/certs -e DOCKER_HOST=$DOCKER_HOST -e DOCKER_TLS_VERIFY=$DOCKER_TLS_VERIFY -ti --rm ianblenke/fig-docker fig&quot;</code></pre></noscript></div>

<p>Now the developer can run <code>fig</code> as if it is running on their development host, continuing the boot2docker illusion.</p>

<p>In the above examples, the current directory <code>$(pwd)</code> is being mounted as /app inside the docker container.</p>

<p>On a boot2docker install, the boot2docker VM is the actual source of that volume path.</p>

<p>That means you would actually have to have the current path inside the boot2docker VM as well.</p>

<p>To do that, on a Mac, do this:</p>

<div><script src="https://gist.github.com/444c3f6552744ef25f59.js"></script>
<noscript><pre><code>boot2docker down
VBoxManage sharedfolder add boot2docker-vm -name home -hostpath /Users
boot2docker up</code></pre></noscript></div>

<p>From this point forward, until the next <code>boot2docker init</code>, your boot2docker VM should have your home directory mounted as /Users and the path should be the same.</p>

<p>A similar trick happens for Windows hosts, providing the same path inside the boot2docker VM as a developer would use.</p>

<p>This allows a normalized docker/fig interface for developers to begin their foray into docker orchestration.</p>

<p>Let’s setup a very quick <a href="http://rubyonrails.org/">Ruby on Rails</a> application from scratch, and then add a Dockerfile and fig.yml that spins up a mysql service for it to talk to.</p>

<p>Here’s a quick script that does just that. The only requirement is a functional docker command able to spin up containers.</p>

<div><script src="https://gist.github.com/92648de57cb7d38fd1e8.js"></script>
<noscript><pre><code>#!/bin/bash
set -ex

# Source the boot2docker environment variables
eval $(boot2docker shellinit 2&gt;/dev/null)

# Use a rails container to create a new rails project in the current directory called figgypudding
docker run -it --rm -v $(pwd):/app rails:latest bash -c &#39;rails new figgypudding; cp -a /figgypudding /app&#39;

cd figgypudding

# Create the Dockerfile used to build the figgypudding_web:latest image used by the figgypudding_web_1 container
cat &lt;&lt;EOD &gt; Dockerfile
FROM rails:onbuild
ENV HOME /usr/src/app
EOD

# This is the Fig orchestration configuration
cat &lt;&lt;EOF &gt; fig.yml
mysql:
  environment:
    MYSQL_ROOT_PASSWORD: supersecret
    MYSQL_DATABASE: figgydata
    MYSQL_USER: figgyuser
    MYSQL_PASSWORD: password
  ports:
    - &quot;3306:3306&quot;
  image: mysql:latest
figgypudding:
  environment:
    RAILS_ENV: development
    DATABASE_URL: mysql2://figgyuser:password@172.17.42.1:3306/figgydata
  links:
    - mysql
  ports:
    - &quot;3000:3000&quot;
  build: .
  command: bash -xc &#39;bundle exec rake db:migrate &amp;&amp; bundle exec rails server&#39;
EOF

# Rails defaults to sqlite, convert it to use mysql
sed -i -e &#39;s/sqlite3/mysql2/&#39; Gemfile

# Update the Gemfile.lock using the rails container we referenced earlier
docker run --rm -v $(pwd):/usr/src/app -w /usr/src/app rails:latest bundle update

# Use the fig command from my fig-docker container to fire up the Fig formation
docker run -v $(pwd):/app -v $DOCKER_CERT_PATH:/certs -e DOCKER_CERT_PATH=/certs -e DOCKER_HOST=$DOCKER_HOST -e DOCKER_TLS_VERIFY=$DOCKER_TLS_VERIFY -ti --rm ianblenke/fig-docker fig up</code></pre></noscript></div>

<p>After running that, there should now be a web server running on the boot2docker VM, which should generally be <a href="http://192.168.59.103:3000/">http://192.168.59.103:3000/</a> as that seems to be the common boot2docker default IP.</p>

<p>This is fig, distilled to its essence.</p>

<p>Beyond this point, a developer can “fig build ; fig up” and see the latest result of their work. This is something ideally added as a git post-commit hook or a iteration harness like <a href="https://github.com/guard/guard">Guard</a>.</p>

<p>While it may not appear <em>pretty</em> at first glance, realize that only <code>cat</code>, and <code>sed</code> were used on the host here (and very well could also themselves have also been avoided). No additional software was installed on the host, yet a rails app was created and deployed in docker containers, talking to a mysql server.</p>

<p>And therein lies the elegance of dockerizing application deployment: simple, clean, repeatable units of software. Orchestrated.</p>

<p>Have fun!</p>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/11/05/self-standing-ceph-slash-deis-store-docker-containers/">Self-standing Ceph/deis-store Docker Containers</a></h1>
    
    
      <p class="meta">
        








  


Nov 5th, 2014
        
           | <a href="/blog/2014/11/05/self-standing-ceph-slash-deis-store-docker-containers/#disqus_thread"
             data-disqus-identifier="http://ian.blenke.com/blog/2014/11/05/self-standing-ceph-slash-deis-store-docker-containers/">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>A common challenge for cloud orchestration is simulating or providing an S3 service layer, particularly for development environments.</p>

<p>As Docker is meant for immutable infrastructure, this poses somewhat of a challenge for production deployments. Rather than tackle that subject here, we’ll revisit persistence on immutable infrastructure in a production capacity in a future blog post.</p>

<p>The first challenge is identifying an S3 implementation to throw into a container.</p>

<p>There are a few feature sparse/dummy solutions that might suit development needs:</p>

<ul>
  <li><a href="http://s3ninja.net/">s3-ninja</a> (github <a href="https://github.com/scireum/s3ninja">scireum/s3ninja</a>)</li>
  <li><a href="https://github.com/jubos/fake-s3">fake-s3</a></li>
  <li><a href="http://sourceforge.net/projects/s3mockup/">S3Mockup</a>
(and a number of others which I’d rather not even consider)</li>
</ul>

<p>There are a number of good functional options for actual S3 implementations:</p>

<ul>
  <li><a href="http://ceph.com">ceph</a> (github <a href="https://github.com/ceph/ceph">ceph/ceph</a>), specifically the <a href="http://ceph.com/docs/master/radosgw/">radosgw</a></li>
  <li><a href="https://github.com/eucalyptus/eucalyptus/wiki/Walrus-S3-API">walrus</a> from Eucalyptus</li>
  <li><a href="http://basho.com/riak-cloud-storage/">riak cs</a></li>
  <li><a href="http://www.skylable.com/download/#LibreS3">libres3</a>, backended by the opensource <a href="http://www.skylable.com/download/#SX">Skylable Sx</a></li>
  <li><a href="https://github.com/nimbusproject/nimbus/tree/master/cumulus">cumulus</a> is an S3 implementation for <a href="http://www.nimbusproject.org/docs/current/faq.html#cumulus">Nimbus</a></li>
  <li><a href="http://www.cloudian.com/community-edition.php">cloudian</a> which is a non-opensource commercial product</li>
  <li><a href="https://github.com/stackforge/swift3">swift3</a> as an S3 compatibility layer with swift on the backend</li>
  <li><a href="https://github.com/cloudfoundry-attic/vblob">vblob</a> a node.js based attic’ed project at CloudFoundry</li>
  <li><a href="https://github.com/mattjamieson/parkplace">parkplace</a> backended by bittorrent</li>
  <li><a href="https://github.com/razerbeans/boardwalk">boardwalk</a> backended by ruby, sinatra, and mongodb</li>
</ul>

<p>Of the above, one stands out as the underlying persistence engine used by a larger docker backended project: <a href="http://deis.io">Deis</a></p>

<p>Rather than re-invent the wheel, it is possible to use deis-store directly.</p>

<p>As Deis deploys on CoreOS, there is an understandable inherent dependency on <a href="http://github.com/coreos/etcd/">etcd</a> for service discovery.</p>

<p>If you happen to be targeting CoreOS, you can simply point your etcd –peers option or <code>ETCD_HOST</code> environment variable at <code>$COREOS_PRIVATE_IPV4</code> and skip this next step.</p>

<p>First, make sure your environment includes the <code>DOCKER_HOST</code> and related variables for the boot2docker environment:</p>

<div><script src="https://gist.github.com/cab2661e67f5d79ae9bd.js"></script>
<noscript><pre><code>eval $(boot2docker shellinit)</code></pre></noscript></div>

<p>Now, discover the IP of the boot2docker guest VM, as that is what we will bind the etcd to:</p>

<div><script src="https://gist.github.com/00d61147bbf81ca26d2d.js"></script>
<noscript><pre><code>IP=&quot;$(boot2docker ip 2&gt;/dev/null)&quot;</code></pre></noscript></div>

<p>Next, we can spawn etcd and publish the ports for the other containers to use:</p>

<div><script src="https://gist.github.com/3a47603ef0561e54ecb6.js"></script>
<noscript><pre><code>docker run --name etcd \
           --publish 4001:4001 \
           --publish 7001:7001 \
           --detach \
           coreos/etcd:latest \
           /go/bin/app -listen-client-urls http://0.0.0.0:4001 \
                       -advertise-client-urls http://$IP:4001 \
                       -listen-peer-urls http://0.0.0.0:7001 \
                       -initial-advertise-peer-urls http://$IP:7001 \
                       -data-dir=/tmp/etcd</code></pre></noscript></div>

<p>Normally, we wouldn’t put the etcd persistence in a tmpfs for consistency reasons after a reboot, but for a development container: we love speed!</p>

<p>Now that we have an etcd container running, we can spawn the deis-store daemon container that runs the ceph object-store daemon (OSD) module.</p>

<div><script src="https://gist.github.com/c05525539a9f38e51e4b.js"></script>
<noscript><pre><code>docker run --name deis-store-daemon \
           --volumes-from=deis-store-daemon-data \
           --env HOST=$IP \
           --publish 6800 \
           --net host \
           --detach \
           deis/store-daemon:latest</code></pre></noscript></div>

<p>It is probably a good idea to mount the /var/lib/deis/store volume for persistence, but this is a developer container, so we’ll forego that step.</p>

<p>The ceph-osd will wait in a loop when starting until it can talk to ceph-mon, which is the next component provided by the deis-store monitor container.</p>

<p>In order to prepare the etcd config tree for deis-store monitor, we must first set a key for this new deis-store-daemon component.</p>

<p>While we could do that with a wget/curl PUT to the etcd client port (4001), using etcdctl makes things a bit easier.</p>

<p>It is generally a good idea to match the version of the etcdctl client with the version of etcd you are using.</p>

<p>As the CoreOS team doesn’t put out an etcdctl container as of yet, one way to do this is to build/install etcdctl inside a coreos/etcd container:</p>

<div><script src="https://gist.github.com/4fcf5bcca7077a85e7ce.js"></script>
<noscript><pre><code>docker run --rm \
           coreos/etcd \
           /bin/sh -c &quot;cd /go/src/github.com/coreos/etcd/etcdctl; \
                       go install ; \
                       /go/bin/etcdctl --peers $IP:4001 \
                                       set /deis/store/hosts/$IP $IP&quot;</code></pre></noscript></div>

<p>This isn’t ideal, of course, as there is a slight delay as etcdctl is built and installed before we use it, but it serves the purpose.</p>

<p>There are also <a href="http://docs.deis.io/en/latest/managing_deis/store_daemon_settings/">deis/store-daemon settings</a> of etcd keys that customize the behavior of ceph-osd a bit.</p>

<p>Now we can start deis-store-monitor, which will use that key to spin up a ceph-mon that monitors this (and any other) ceph-osd instances likewise registered in the etcd configuration tree.</p>

<div><script src="https://gist.github.com/543a13ba9410f6cf2f8e.js"></script>
<noscript><pre><code>docker run --name deis-store-monitor \
           --env HOST=$IP \
           --publish 6789 \
           --net host \
           --detach \
           deis/store-monitor:latest</code></pre></noscript></div>

<p>As before, there are volumes that probably should be mounted for /etc/ceph and /var/lib/ceph/mon, but this is a development image, so we’ll skip that.</p>

<p>There are also <a href="http://docs.deis.io/en/latest/managing_deis/store_monitor_settings/">deis/store-monitor settings</a> of etcd keys that customize the behavior of ceph-mon a bit.</p>

<p>Now that ceph-mon is running, ceph-osd will continue starting up. We now have a single-node self-standing ceph storage platform, but no S3.</p>

<p>The S3 functionality is provided by the ceph-radosgw component, which is provided by the deis-store-gateway container.</p>

<div><script src="https://gist.github.com/5634583a93347c415b3d.js"></script>
<noscript><pre><code>docker run --name deis-store-gateway \
           --hostname deis-store-gateway \
           --env HOST=$IP \
           --env EXTERNAL_PORT=8888 \
           --publish 8888:8888 \
           --detach \
           deis/store-gateway:latest</code></pre></noscript></div>

<p>There is no persistence in ceph-radosgw that warrant a volume mapping, so we can ignore that entirely regardless of environment.</p>

<p>There are also <a href="http://docs.deis.io/en/latest/managing_deis/store_gateway_settings/">deis/store-gateway settings</a> of etcd keys that customize the behavior of ceph-radosgw a bit.</p>

<p>We now have a functional self-standing S3 gateway, but we don’t know the credentials to use it. For that, we can run etcdctl again:</p>

<div><script src="https://gist.github.com/9c43ccd03c6c082073b7.js"></script>
<noscript><pre><code>AWS_ACCESS_KEY_ID=$(docker run --rm coreos/etcd /bin/sh -c &quot;cd /go/src/github.com/coreos/etcd/etcdctl; go install ; /go/bin/etcdctl --peers $IP:4001 get /deis/store/gateway/accessKey&quot;)
AWS_SECRET_ACCESS_KEY=$(docker run --rm coreos/etcd /bin/sh -c &quot;cd /go/src/github.com/coreos/etcd/etcdctl; go install ; /go/bin/etcdctl --peers $IP:4001 get /deis/store/gateway/secretKey&quot;)
AWS_S3_HOST=$(docker run --rm coreos/etcd /bin/sh -c &quot;cd /go/src/github.com/coreos/etcd/etcdctl; go install ; /go/bin/etcdctl --peers $IP:4001 get /deis/store/gateway/host&quot;)
AWS_S3_PORT=$(docker run --rm coreos/etcd /bin/sh -c &quot;cd /go/src/github.com/coreos/etcd/etcdctl; go install ; /go/bin/etcdctl --peers $IP:4001 get /deis/store/gateway/port&quot;)</code></pre></noscript></div>

<p>Note that the host here isn’t the normal AWS gateway address, so you will need to specify things for your S3 client to access it correctly.</p>

<p>Likewise, you may need to specify an URL scheme of “http”, as the above does not expose an HTTPS encrypted port.</p>

<p>There are also S3 client changes that <a href="https://github.com/deis/deis/issues/2326">may be necessary</a> depending on the “calling format” of the client libraries. You may need to <a href="http://stackoverflow.com/questions/24312350/using-paperclip-fog-and-ceph">changes things like paperclip</a> to <a href="https://github.com/thoughtbot/paperclip/issues/1577">work with fog</a>. There are numerous tools that work happily with ceph, like <a href="https://github.com/stiller/s3_to_ceph/blob/master/s3_to_ceph.rb">s3_to_ceph</a> and even gems like <a href="https://github.com/fog/fog-radosgw">fog-radosgw</a> that try and help make this painless for your apps.</p>

<p>I will update this blog post shortly with an example of a containerized s3 client to show how to prove your ceph radosgw is working.</p>

<p>Have fun!</p>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/11/04/using-boot2docker/">Using Boot2Docker</a></h1>
    
    
      <p class="meta">
        








  


Nov 4th, 2014
        
           | <a href="/blog/2014/11/04/using-boot2docker/#disqus_thread"
             data-disqus-identifier="http://ian.blenke.com/blog/2014/11/04/using-boot2docker/">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><h2 id="boot2docker-command-line">Boot2Docker command-line</h2>

<p>Preface: the <a href="https://github.com/boot2docker/boot2docker">boot2docker README</a> is a great place to discover the below commands in more detail.</p>

<p>Now that we have Boot2Docker installed, we need to initialize a VM instance</p>

<pre><code>boot2docker init
</code></pre>

<p>This merely defines the default boot2docker VM, it does not start it. To do that, we need to bring it “up”</p>

<pre><code>boot2docker up
</code></pre>

<p>When run, it looks something like this:</p>

<pre><code>icbcfmbp:~ icblenke$ boot2docker up
Waiting for VM and Docker daemon to start...
..........ooo
Started.
Writing /Users/icblenke/.boot2docker/certs/boot2docker-vm/ca.pem
Writing /Users/icblenke/.boot2docker/certs/boot2docker-vm/cert.pem
Writing /Users/icblenke/.boot2docker/certs/boot2docker-vm/key.pem

To connect the Docker client to the Docker daemon, please set:
    export DOCKER_CERT_PATH=/Users/icblenke/.boot2docker/certs/boot2docker-vm
    export DOCKER_TLS_VERIFY=1
    export DOCKER_HOST=tcp://192.168.59.103:2376

icbcfmbp:~ icblenke$
</code></pre>

<p>This is all fine and dandy, but that shell didn’t actually source those variables. To do that we use boot2docker shellinit:</p>

<pre><code>eval $(boot2docker shellinit)
</code></pre>

<p>Now the shell has those variables exported for the running boot2docker VM.</p>

<p>The persistence of the boot2docker VM lasts only until we run a boot2docker destroy</p>

<pre><code>boot2docker destroy
</code></pre>

<p>After doing this, there is no longer a VM defined. We would need to go back to the boot2docker init step above and repeat.</p>

<h2 id="docker-command-line">Docker command-line</h2>

<p>From this point forward, we use the docker command to interact with the boot2docker VM as if we are on a linux docker host.</p>

<p>The docker command is just a compiled go application that makes RESTful calls to the docker daemon inside the linux VM.</p>

<pre><code>bash-3.2$ docker info
Containers: 0
Images: 0
Storage Driver: aufs
 Root Dir: /mnt/sda1/var/lib/docker/aufs
  Dirs: 0
  Execution Driver: native-0.2
  Kernel Version: 3.16.4-tinycore64
  Operating System: Boot2Docker 1.3.1 (TCL 5.4); master : 9a31a68 - Fri Oct 31 03:14:34 UTC 2014
  Debug mode (server): true
  Debug mode (client): false
  Fds: 10
  Goroutines: 11
  EventsListeners: 0
  Init Path: /usr/local/bin/docker
</code></pre>

<p>This holds true for both OS/X and Windows. </p>

<p>The boot2docker facade is just a handy wrapper to prepare the guest linux host VM for the docker daemin and local docker command-line client for your development host OS environment.</p>

<p>And now you have a starting point for exploring <a href="http://docker.io">Docker</a>!</p>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/11/04/installing-boot2docker/">Installing Boot2Docker</a></h1>
    
    
      <p class="meta">
        








  


Nov 4th, 2014
        
           | <a href="/blog/2014/11/04/installing-boot2docker/#disqus_thread"
             data-disqus-identifier="http://ian.blenke.com/blog/2014/11/04/installing-boot2docker/">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>Starting with a new team of developers, it helps to document the bootstrapping steps to a development environment.</p>

<p>Rather than try and use a convergence tool like Chef, Puppet, Ansible, or SALT, this time the environment will embrace Docker.</p>

<p>We could use a tool like Vagrant, but we need to support both Windows and Mac development workstations, and Vagrant under Windows can be vexing.</p>

<p>For this, we will begin anew using <a href="http://boot2docker.io">Boot2Docker</a></p>

<p>Before we begin, be sure to install <a href="https://www.virtualbox.org/">VirtualBox</a> from Oracle’s <a href="https://www.virtualbox.org/">VirtualBox.org website</a></p>

<p>The easiest way to install VirtualBox is to use <a href="http://caskroom.io/">HomeBrew Cask</a> under <a href="http://brew.sh">HomeBrew</a></p>

<pre><code>brew install caskroom/cask/brew-cask
brew cask install virtualbox
</code></pre>

<p>The easiest way to install boot2docker is to use <a href="http://brew.sh">HomeBrew</a></p>

<pre><code>brew install boot2docker
</code></pre>

<p>Afterward, be sure to upgrade the homebrew bottle to the latest version of boot2docker:</p>

<pre><code>boot2docker upgrade
</code></pre>

<p>Alternatively, a sample commandline install of boot2docker might look like this:</p>

<pre><code>wget https://github.com/boot2docker/osx-installer/releases/download/v1.3.1/Boot2Docker-1.3.1.pkg
sudo installer -pkg ~/Downloads/Boot2Docker-1.3.1.pkg -target /
</code></pre>

<p>I’ll leave the commandline install of VirtualBox up to your imagination. With <a href="http://caskroom.io">HomeBrew Cask</a>, there’s really not much of a point.</p>

<p>If you’re still not comfortable, below is a pictoral screenshot guide to installing boot2docker the point-and-click way.</p>

<h2 id="step-0">Step 0</h2>

<p>Download <a href="https://github.com/boot2docker/osx-installer/releases">boot2docker for OS/X</a> or <a href="https://github.com/boot2docker/windows-installer/releases">boot2docker for Windows</a></p>

<h2 id="step-1">Step 1</h2>

<p>Run the downloaded Boot2Docker.pkg or docker-install.exe to kick off the installer.</p>

<p><img src="/images/screenshots/boot2docker/step1-downloads.png" width="1636" height="1022" title="Downloads" alt="Boot2docker.pkg in Downloads folder" /></p>

<h2 id="step-2">Step 2</h2>

<p>Click the Continue button to allow the installer to run a program to detect if boot2docker can be installed.&lt;/p&gt;</p>

<p><img src="/images/screenshots/boot2docker/step2-run-a-program.png" width="1318" height="968" title="Click Continue button" alt="Allow installer to run a program to detect if boot2docker can be installed" /></p>

<h2 id="step-3">Step 3</h2>

<p>Click the Continue button to proceed beyond the initial installation instructions dialog.</p>

<p><img src="/images/screenshots/boot2docker/step3-install-splash.png" width="1316" height="944" title="Click Continue button" alt="Instructions to install boot2docker" /></p>

<h2 id="step-4">Step 4</h2>

<p>The installer will now ask for an admin username/password to obtain admin rights to install boot2docker.</p>

<p><img src="/images/screenshots/boot2docker/step4-enter-password.png" width="1390" height="1104" title="Enter your password" alt="Installer asks for admin rights to install boot2docker" /></p>

<h2 id="step-5">Step 5</h2>

<p>Before installing, the installer will advise how much space the install will take. Click the Install button to start the actual install.</p>

<p><img src="/images/screenshots/boot2docker/step5-standard-install.png" width="1310" height="968" title="Click Install button" alt="Advice on how much space boot2docker will take when installed" /></p>

<h2 id="step-6">Step 6</h2>

<p>When the installation is successfully, click the Close button to exit the installer.</p>

<p><img src="/images/screenshots/boot2docker/step6-install-completed-successfully.png" width="1334" height="984" title="Click Close button" alt="Install completed successfully" /></p>

<h2 id="step-7">Step 7</h2>

<p>You now have a shiny icon for boot2docker in /Applications you can click on to start a boot2docker terminal window session.</p>

<p><img src="/images/screenshots/boot2docker/step7-installed-boot2docker-app.png" width="1352" height="1006" title="Run boot2docker app" alt="Boot2docker app is in Applications" /></p>

<p>Congrats. You now have boot2docker installed.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/10/18/immutable-infrastructure-persistence/">Immutable Infrastructure Persistence</a></h1>
    
    
      <p class="meta">
        








  


Oct 18th, 2014
        
           | <a href="/blog/2014/10/18/immutable-infrastructure-persistence/#disqus_thread"
             data-disqus-identifier="http://ian.blenke.com/blog/2014/10/18/immutable-infrastructure-persistence/">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>Here is a link to my <a href="http://barcamptampabay.org/">Tampa Bay Barcamp 2014 presentation slides</a> for <a href="http://ian.blenke.com/immutable-infrastructure-persistence/">Immutable Infrastructure Persistence</a></p>
</div>
  
  


    </article>
  
  <div class="pagination">
    
    <a href="/blog/archives">Blog Archives</a>
    
  </div>
</div>
<aside class="sidebar">
   
<form action="https://www.google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:ian.blenke.com" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
  
    
<section>
	<span>
		<img src="http://www.gravatar.com/avatar/b0efd275b44b660f817003f2baca7ea2" alt="Gravatar of Ian Blenke " title="Gravatar of Ian Blenke" />
	</span>
</section>
<section>
  <h1>About Me</h1>
  <p>Ian is a Senior DevOps Engineer at MalwareBytes</p>
</section>
<section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2015/06/27/aws-docker-walkthrough-with-elasticbeanstalk-part-1/">AWS Docker Walkthrough With ElasticBeanstalk: Part 1</a>
      </li>
    
      <li class="post">
        <a href="/blog/2015/03/10/deploying-amazon-ecs-on-coreos/">Deploying Amazon ECS on CoreOS</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/11/10/docker-rspec-tdd/">Docker Rspec TDD</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/11/09/openstack-on-a-chromebox/">OpenStack on a Chromebox</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/11/07/fig-docker/">Fig-docker</a>
      </li>
    
  </ul>
</section>

<section>
  <h1>GitHub Repos</h1>
  <ul id="gh_repos">
    <li class="loading">Status updating&#8230;</li>
  </ul>
  
  <a href="https://github.com/ianblenke">@ianblenke</a> on GitHub
  
  <script type="text/javascript">
    $(document).ready(function(){
        if (!window.jXHR){
            var jxhr = document.createElement('script');
            jxhr.type = 'text/javascript';
            jxhr.src = '/javascripts/libs/jXHR.js';
            var s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(jxhr, s);
        }

        github.showRepos({
            user: 'ianblenke',
            count: 0,
            skip_forks: true,
            target: '#gh_repos'
        });
    });
  </script>
  <script src="/javascripts/github.js" type="text/javascript"> </script>
</section>



<section class="googleplus">
  <h1>
    <a href="https://plus.google.com/+IanBlenke?rel=author">
      <img src="http://www.google.com/images/icons/ui/gprofile_button-32.png" width="32" height="32">
      Google+
    </a>
  </h1>
  <script src="/javascripts/google-tracking.js" type="text/javascript"> </script>
</section>



  
</aside>


    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2015 - Ian Blenke -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>.
  <span class="credit">Get the source for this blog at <a href="https://github.com/ianblenke/blog/tree/source">github.com/ianblenke/blog/tree/source<a/></span>
</p>

</footer>
  

<script type="text/javascript">
      var disqus_shortname = 'ianblenke';
      
        
        var disqus_script = 'count.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>





  <script type="text/javascript">
    (function() {
      var script = document.createElement('script'); script.type = 'text/javascript'; script.async = true;
      script.src = 'https://apis.google.com/js/plusone.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(script, s);
    })();
  </script>



  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = '//platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>



<script>
  $(document).ready(function() {  
  var stickyNavTop = $('nav').offset().top;  
    
  var stickyNav = function(){  
  var scrollTop = $(window).scrollTop();  
         
  if (scrollTop > stickyNavTop) {   
      $('nav').addClass('sticky');  
  } else {  
      $('nav').removeClass('sticky');   
  }  
  };  
    
  stickyNav();  
    
  $(window).scroll(function() {  
      stickyNav();  
  });  
  });  
</script>


</body>
</html>
