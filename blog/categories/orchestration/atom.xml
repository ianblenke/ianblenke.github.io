<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Orchestration | Ian Blenke - DevOps]]></title>
  <link href="http://ian.blenke.com/blog/categories/orchestration/atom.xml" rel="self"/>
  <link href="http://ian.blenke.com/"/>
  <updated>2015-06-28T04:16:50-04:00</updated>
  <id>http://ian.blenke.com/</id>
  <author>
    <name><![CDATA[Ian Blenke]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Fig-docker]]></title>
    <link href="http://ian.blenke.com/blog/2014/11/07/fig-docker/"/>
    <updated>2014-11-07T19:20:06-05:00</updated>
    <id>http://ian.blenke.com/blog/2014/11/07/fig-docker</id>
    <content type="html"><![CDATA[<p>A common devops problem when developing <a href="http://docker.io">Docker</a> containers is managing the orchestration of multiple containers in a development environment.</p>

<p>There are a number of orchestration harnesses for Docker available:</p>

<ul>
  <li>Docker’s <a href="http://fig.sh">Fig</a></li>
  <li><a href="https://github.com/dcm-oss/blockade">blockade</a></li>
  <li><a href="https://docs.vagrantup.com/v2/provisioning/docker.html">Vagrant</a></li>
  <li><a href="https://github.com/GoogleCloudPlatform/kubernetes">kubernetes</a></li>
  <li><a href="https://github.com/signalfuse/maestro-ng">maestro-ng</a></li>
  <li><a href="https://github.com/michaelsauter/crane">crane</a></li>
  <li>Centurylink’s <a href="http://panamax.io/">Panamax</a></li>
  <li><a href="http://shipyard-project.com/">Shipyard</a></li>
  <li><a href="http://decking.io/">Decking</a></li>
  <li>NewRelic’s <a href="https://github.com/newrelic/centurion">Centurion</a></li>
  <li>Spotify’s <a href="https://github.com/spotify/helios">Helios</a></li>
  <li><a href="https://github.com/cattleio/stampede">Stampede</a></li>
  <li><a href="https://www.getchef.com/solutions/docker/">Chef</a></li>
  <li><a href="http://www.ansible.com/docker">Ansible</a></li>
  <li><a href="https://flynn.io/">Flynn</a></li>
  <li><a href="https://github.com/mailgun/shipper">Shipper</a></li>
  <li><a href="http://octohost.io">Octohost</a></li>
  <li><a href="http://tsuru.io/">Tsuru</a> with <a href="https://github.com/tsuru/docker-cluster">docker-cluster</a></li>
  <li><a href="https://clusterhq.com/">Flocker</a></li>
  <li><a href="https://github.com/CloudCredo/cloudfocker">CloudFocker</a></li>
  <li><a href="http://www.cloudsoftcorp.com/blog/2014/06/clocker-creating-a-docker-cloud-with-apache-brooklyn/">Clocker</a> and <a href="http://brooklyn.incubator.apache.org">Apache Brooklyn</a></li>
  <li><a href="http://cloudfoundry.org">CloudFoundry</a>’s <a href="https://github.com/cf-platform-eng/docker-boshrelease">docker-boshrelease</a>/<a href="https://github.com/cloudfoundry-incubator/diego-release">diego</a></li>
  <li>Mesosphere <a href="https://github.com/mesosphere/deimos">Deimos</a></li>
  <li><a href="http://deis.io">Deis</a> (a PaaS that can git push deploy containers using <a href="http://heroku.com">Heroku</a> buildpacks <em>or</em> a Dockerfile)</li>
</ul>

<p>There are a number of hosted service offerings now as well:</p>

<ul>
  <li><a href="http://aws.amazon.com/ecs">Amazon ECS</a></li>
  <li><a href="http://www.qualisystems.com/cloudshell-6-0-sneak-peek/">CloudShell</a></li>
  <li><a href="https://elasticbox.com/how-it-works/">ElasticBox</a></li>
  <li>Aw, heck, just check the <a href="http://www.mindmeister.com/389671722/docker-ecosystem">docker ecosystem mindmap</a></li>
</ul>

<p>There are also RAFT/GOSSIP clustering solutions like:</p>

<ul>
  <li><a href="https://coreos.com/">CoreOS</a>/<a href="https://github.com/coreos/fleet">Fleet</a></li>
  <li><a href="https://www.openshift.com/products/origin">OpenShift Origin</a> uses <a href="http://www.projectatomic.io/">ProjectAtomic</a>/<a href="https://openshift.github.io/geard/">Geard</a></li>
</ul>

<p>My <a href="https://github.com/ianblenke/coreos-vagrant-kitchen-sink">coreos-vagrant-kitchen-sink</a> github project submits <a href="https://github.com/ianblenke/coreos-vagrant-kitchen-sink/tree/master/cloud-init">cloud-init units</a> via a YAML file when booting member nodes. It’s a good model for production, but it’s a bit heavy for development.</p>

<p>Docker is currently working on <a href="https://www.youtube.com/watch?v=vtnSL79rZ6o">Docker Clustering</a>, but it is presently just a proof-of-concept and is now under a total re-write.</p>

<p>They are also <a href="https://www.youtube.com/watch?v=YuSq6bXHnOI">implementing docker composition</a> which provides Fig like functionality using upcoming docker “groups”.</p>

<p>That influence of Fig makes sense, as <a href="http://venturebeat.com/2014/07/22/docker-buys-orchard-a-2-man-startup-with-a-cloud-service-for-running-docker-friendly-apps/">Docker bought Orchard</a>.</p>

<p>Internally, Docker developers use <a href="http://fig.sh">Fig</a>.</p>

<p>Docker’s website also directs everyone to <a href="http://boot2docker.io">Boot2Docker</a>, as that is the tool Docker developers use as their docker baseline environment. </p>

<p>Boot2Docker spawns a <a href="https://www.virtualbox.org/">VirtualBox</a> based VM as well as a native docker client runtime on the developer’s host machine, and provides the <code>DOCKER_HOST</code> and related enviroments necessary for the client to talk to the VM.</p>

<p>This allows a developer’s Windows or OS/X machine to have a docker command that behaves as if the docker containers are running natively on their host machine.</p>

<p>While Fig is easy to install under OS/X as it has native Python support (“pip install fig”), installing Fig on a Windows developer workstation would normally require Python support be installed separately.</p>

<p>Rather than do that, I’ve built a new <a href="https://registry.hub.docker.com/u/ianblenke/fig-docker/">ianblenke/fig-docker</a> docker Hub image, which is auto-built from <a href="https://github.com/ianblenke/docker-fig-docker">ianblenke/docker-fig-docker</a> on github.</p>

<p>This allows running fig inside a docker container using:</p>

<p><div><script src='https://gist.github.com/6cd8f8a4065bcac99443.js'></script>
<noscript><pre><code>docker run -v $(pwd):/app -v $DOCKER_CERT_PATH:/certs -e DOCKER_CERT_PATH=/certs -e DOCKER_HOST=$DOCKER_HOST -e DOCKER_TLS_VERIFY=$DOCKER_TLS_VERIFY -ti --rm ianblenke/fig-docker fig --help</code></pre></noscript></div>
</p>

<p>Alternatively, a developer can alias it:</p>

<p><div><script src='https://gist.github.com/f48bc5cf8b2567bd8006.js'></script>
<noscript><pre><code>alias fig=&quot;docker run -v $(pwd):/app -v $DOCKER_CERT_PATH:/certs -e DOCKER_CERT_PATH=/certs -e DOCKER_HOST=$DOCKER_HOST -e DOCKER_TLS_VERIFY=$DOCKER_TLS_VERIFY -ti --rm ianblenke/fig-docker fig&quot;</code></pre></noscript></div>
</p>

<p>Now the developer can run <code>fig</code> as if it is running on their development host, continuing the boot2docker illusion.</p>

<p>In the above examples, the current directory <code>$(pwd)</code> is being mounted as /app inside the docker container.</p>

<p>On a boot2docker install, the boot2docker VM is the actual source of that volume path.</p>

<p>That means you would actually have to have the current path inside the boot2docker VM as well.</p>

<p>To do that, on a Mac, do this:</p>

<p><div><script src='https://gist.github.com/444c3f6552744ef25f59.js'></script>
<noscript><pre><code>boot2docker down
VBoxManage sharedfolder add boot2docker-vm -name home -hostpath /Users
boot2docker up</code></pre></noscript></div>
</p>

<p>From this point forward, until the next <code>boot2docker init</code>, your boot2docker VM should have your home directory mounted as /Users and the path should be the same.</p>

<p>A similar trick happens for Windows hosts, providing the same path inside the boot2docker VM as a developer would use.</p>

<p>This allows a normalized docker/fig interface for developers to begin their foray into docker orchestration.</p>

<p>Let’s setup a very quick <a href="http://rubyonrails.org/">Ruby on Rails</a> application from scratch, and then add a Dockerfile and fig.yml that spins up a mysql service for it to talk to.</p>

<p>Here’s a quick script that does just that. The only requirement is a functional docker command able to spin up containers.</p>

<p><div><script src='https://gist.github.com/92648de57cb7d38fd1e8.js'></script>
<noscript><pre><code>#!/bin/bash
set -ex

# Source the boot2docker environment variables
eval $(boot2docker shellinit 2&gt;/dev/null)

# Use a rails container to create a new rails project in the current directory called figgypudding
docker run -it --rm -v $(pwd):/app rails:latest bash -c &#39;rails new figgypudding; cp -a /figgypudding /app&#39;

cd figgypudding

# Create the Dockerfile used to build the figgypudding_web:latest image used by the figgypudding_web_1 container
cat &lt;&lt;EOD &gt; Dockerfile
FROM rails:onbuild
ENV HOME /usr/src/app
EOD

# This is the Fig orchestration configuration
cat &lt;&lt;EOF &gt; fig.yml
mysql:
  environment:
    MYSQL_ROOT_PASSWORD: supersecret
    MYSQL_DATABASE: figgydata
    MYSQL_USER: figgyuser
    MYSQL_PASSWORD: password
  ports:
    - &quot;3306:3306&quot;
  image: mysql:latest
figgypudding:
  environment:
    RAILS_ENV: development
    DATABASE_URL: mysql2://figgyuser:password@172.17.42.1:3306/figgydata
  links:
    - mysql
  ports:
    - &quot;3000:3000&quot;
  build: .
  command: bash -xc &#39;bundle exec rake db:migrate &amp;&amp; bundle exec rails server&#39;
EOF

# Rails defaults to sqlite, convert it to use mysql
sed -i -e &#39;s/sqlite3/mysql2/&#39; Gemfile

# Update the Gemfile.lock using the rails container we referenced earlier
docker run --rm -v $(pwd):/usr/src/app -w /usr/src/app rails:latest bundle update

# Use the fig command from my fig-docker container to fire up the Fig formation
docker run -v $(pwd):/app -v $DOCKER_CERT_PATH:/certs -e DOCKER_CERT_PATH=/certs -e DOCKER_HOST=$DOCKER_HOST -e DOCKER_TLS_VERIFY=$DOCKER_TLS_VERIFY -ti --rm ianblenke/fig-docker fig up</code></pre></noscript></div>
</p>

<p>After running that, there should now be a web server running on the boot2docker VM, which should generally be <a href="http://192.168.59.103:3000/">http://192.168.59.103:3000/</a> as that seems to be the common boot2docker default IP.</p>

<p>This is fig, distilled to its essence.</p>

<p>Beyond this point, a developer can “fig build ; fig up” and see the latest result of their work. This is something ideally added as a git post-commit hook or a iteration harness like <a href="https://github.com/guard/guard">Guard</a>.</p>

<p>While it may not appear <em>pretty</em> at first glance, realize that only <code>cat</code>, and <code>sed</code> were used on the host here (and very well could also themselves have also been avoided). No additional software was installed on the host, yet a rails app was created and deployed in docker containers, talking to a mysql server.</p>

<p>And therein lies the elegance of dockerizing application deployment: simple, clean, repeatable units of software. Orchestrated.</p>

<p>Have fun!</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Self-standing Ceph/deis-store Docker Containers]]></title>
    <link href="http://ian.blenke.com/blog/2014/11/05/self-standing-ceph-slash-deis-store-docker-containers/"/>
    <updated>2014-11-05T14:40:59-05:00</updated>
    <id>http://ian.blenke.com/blog/2014/11/05/self-standing-ceph-slash-deis-store-docker-containers</id>
    <content type="html"><![CDATA[<p>A common challenge for cloud orchestration is simulating or providing an S3 service layer, particularly for development environments.</p>

<p>As Docker is meant for immutable infrastructure, this poses somewhat of a challenge for production deployments. Rather than tackle that subject here, we’ll revisit persistence on immutable infrastructure in a production capacity in a future blog post.</p>

<p>The first challenge is identifying an S3 implementation to throw into a container.</p>

<p>There are a few feature sparse/dummy solutions that might suit development needs:</p>

<ul>
  <li><a href="http://s3ninja.net/">s3-ninja</a> (github <a href="https://github.com/scireum/s3ninja">scireum/s3ninja</a>)</li>
  <li><a href="https://github.com/jubos/fake-s3">fake-s3</a></li>
  <li><a href="http://sourceforge.net/projects/s3mockup/">S3Mockup</a>
(and a number of others which I’d rather not even consider)</li>
</ul>

<p>There are a number of good functional options for actual S3 implementations:</p>

<ul>
  <li><a href="http://ceph.com">ceph</a> (github <a href="https://github.com/ceph/ceph">ceph/ceph</a>), specifically the <a href="http://ceph.com/docs/master/radosgw/">radosgw</a></li>
  <li><a href="https://github.com/eucalyptus/eucalyptus/wiki/Walrus-S3-API">walrus</a> from Eucalyptus</li>
  <li><a href="http://basho.com/riak-cloud-storage/">riak cs</a></li>
  <li><a href="http://www.skylable.com/download/#LibreS3">libres3</a>, backended by the opensource <a href="http://www.skylable.com/download/#SX">Skylable Sx</a></li>
  <li><a href="https://github.com/nimbusproject/nimbus/tree/master/cumulus">cumulus</a> is an S3 implementation for <a href="http://www.nimbusproject.org/docs/current/faq.html#cumulus">Nimbus</a></li>
  <li><a href="http://www.cloudian.com/community-edition.php">cloudian</a> which is a non-opensource commercial product</li>
  <li><a href="https://github.com/stackforge/swift3">swift3</a> as an S3 compatibility layer with swift on the backend</li>
  <li><a href="https://github.com/cloudfoundry-attic/vblob">vblob</a> a node.js based attic’ed project at CloudFoundry</li>
  <li><a href="https://github.com/mattjamieson/parkplace">parkplace</a> backended by bittorrent</li>
  <li><a href="https://github.com/razerbeans/boardwalk">boardwalk</a> backended by ruby, sinatra, and mongodb</li>
</ul>

<p>Of the above, one stands out as the underlying persistence engine used by a larger docker backended project: <a href="http://deis.io">Deis</a></p>

<p>Rather than re-invent the wheel, it is possible to use deis-store directly.</p>

<p>As Deis deploys on CoreOS, there is an understandable inherent dependency on <a href="http://github.com/coreos/etcd/">etcd</a> for service discovery.</p>

<p>If you happen to be targeting CoreOS, you can simply point your etcd –peers option or <code>ETCD_HOST</code> environment variable at <code>$COREOS_PRIVATE_IPV4</code> and skip this next step.</p>

<p>First, make sure your environment includes the <code>DOCKER_HOST</code> and related variables for the boot2docker environment:</p>

<p><div><script src='https://gist.github.com/cab2661e67f5d79ae9bd.js'></script>
<noscript><pre><code>eval $(boot2docker shellinit)</code></pre></noscript></div>
</p>

<p>Now, discover the IP of the boot2docker guest VM, as that is what we will bind the etcd to:</p>

<p><div><script src='https://gist.github.com/00d61147bbf81ca26d2d.js'></script>
<noscript><pre><code>IP=&quot;$(boot2docker ip 2&gt;/dev/null)&quot;</code></pre></noscript></div>
</p>

<p>Next, we can spawn etcd and publish the ports for the other containers to use:</p>

<p><div><script src='https://gist.github.com/3a47603ef0561e54ecb6.js'></script>
<noscript><pre><code>docker run --name etcd \
           --publish 4001:4001 \
           --publish 7001:7001 \
           --detach \
           coreos/etcd:latest \
           /go/bin/app -listen-client-urls http://0.0.0.0:4001 \
                       -advertise-client-urls http://$IP:4001 \
                       -listen-peer-urls http://0.0.0.0:7001 \
                       -initial-advertise-peer-urls http://$IP:7001 \
                       -data-dir=/tmp/etcd</code></pre></noscript></div>
</p>

<p>Normally, we wouldn’t put the etcd persistence in a tmpfs for consistency reasons after a reboot, but for a development container: we love speed!</p>

<p>Now that we have an etcd container running, we can spawn the deis-store daemon container that runs the ceph object-store daemon (OSD) module.</p>

<p><div><script src='https://gist.github.com/c05525539a9f38e51e4b.js'></script>
<noscript><pre><code>docker run --name deis-store-daemon \
           --volumes-from=deis-store-daemon-data \
           --env HOST=$IP \
           --publish 6800 \
           --net host \
           --detach \
           deis/store-daemon:latest</code></pre></noscript></div>
</p>

<p>It is probably a good idea to mount the /var/lib/deis/store volume for persistence, but this is a developer container, so we’ll forego that step.</p>

<p>The ceph-osd will wait in a loop when starting until it can talk to ceph-mon, which is the next component provided by the deis-store monitor container.</p>

<p>In order to prepare the etcd config tree for deis-store monitor, we must first set a key for this new deis-store-daemon component.</p>

<p>While we could do that with a wget/curl PUT to the etcd client port (4001), using etcdctl makes things a bit easier.</p>

<p>It is generally a good idea to match the version of the etcdctl client with the version of etcd you are using.</p>

<p>As the CoreOS team doesn’t put out an etcdctl container as of yet, one way to do this is to build/install etcdctl inside a coreos/etcd container:</p>

<p><div><script src='https://gist.github.com/4fcf5bcca7077a85e7ce.js'></script>
<noscript><pre><code>docker run --rm \
           coreos/etcd \
           /bin/sh -c &quot;cd /go/src/github.com/coreos/etcd/etcdctl; \
                       go install ; \
                       /go/bin/etcdctl --peers $IP:4001 \
                                       set /deis/store/hosts/$IP $IP&quot;</code></pre></noscript></div>
</p>

<p>This isn’t ideal, of course, as there is a slight delay as etcdctl is built and installed before we use it, but it serves the purpose.</p>

<p>There are also <a href="http://docs.deis.io/en/latest/managing_deis/store_daemon_settings/">deis/store-daemon settings</a> of etcd keys that customize the behavior of ceph-osd a bit.</p>

<p>Now we can start deis-store-monitor, which will use that key to spin up a ceph-mon that monitors this (and any other) ceph-osd instances likewise registered in the etcd configuration tree.</p>

<p><div><script src='https://gist.github.com/543a13ba9410f6cf2f8e.js'></script>
<noscript><pre><code>docker run --name deis-store-monitor \
           --env HOST=$IP \
           --publish 6789 \
           --net host \
           --detach \
           deis/store-monitor:latest</code></pre></noscript></div>
</p>

<p>As before, there are volumes that probably should be mounted for /etc/ceph and /var/lib/ceph/mon, but this is a development image, so we’ll skip that.</p>

<p>There are also <a href="http://docs.deis.io/en/latest/managing_deis/store_monitor_settings/">deis/store-monitor settings</a> of etcd keys that customize the behavior of ceph-mon a bit.</p>

<p>Now that ceph-mon is running, ceph-osd will continue starting up. We now have a single-node self-standing ceph storage platform, but no S3.</p>

<p>The S3 functionality is provided by the ceph-radosgw component, which is provided by the deis-store-gateway container.</p>

<p><div><script src='https://gist.github.com/5634583a93347c415b3d.js'></script>
<noscript><pre><code>docker run --name deis-store-gateway \
           --hostname deis-store-gateway \
           --env HOST=$IP \
           --env EXTERNAL_PORT=8888 \
           --publish 8888:8888 \
           --detach \
           deis/store-gateway:latest</code></pre></noscript></div>
</p>

<p>There is no persistence in ceph-radosgw that warrant a volume mapping, so we can ignore that entirely regardless of environment.</p>

<p>There are also <a href="http://docs.deis.io/en/latest/managing_deis/store_gateway_settings/">deis/store-gateway settings</a> of etcd keys that customize the behavior of ceph-radosgw a bit.</p>

<p>We now have a functional self-standing S3 gateway, but we don’t know the credentials to use it. For that, we can run etcdctl again:</p>

<p><div><script src='https://gist.github.com/9c43ccd03c6c082073b7.js'></script>
<noscript><pre><code>AWS_ACCESS_KEY_ID=$(docker run --rm coreos/etcd /bin/sh -c &quot;cd /go/src/github.com/coreos/etcd/etcdctl; go install ; /go/bin/etcdctl --peers $IP:4001 get /deis/store/gateway/accessKey&quot;)
AWS_SECRET_ACCESS_KEY=$(docker run --rm coreos/etcd /bin/sh -c &quot;cd /go/src/github.com/coreos/etcd/etcdctl; go install ; /go/bin/etcdctl --peers $IP:4001 get /deis/store/gateway/secretKey&quot;)
AWS_S3_HOST=$(docker run --rm coreos/etcd /bin/sh -c &quot;cd /go/src/github.com/coreos/etcd/etcdctl; go install ; /go/bin/etcdctl --peers $IP:4001 get /deis/store/gateway/host&quot;)
AWS_S3_PORT=$(docker run --rm coreos/etcd /bin/sh -c &quot;cd /go/src/github.com/coreos/etcd/etcdctl; go install ; /go/bin/etcdctl --peers $IP:4001 get /deis/store/gateway/port&quot;)</code></pre></noscript></div>
</p>

<p>Note that the host here isn’t the normal AWS gateway address, so you will need to specify things for your S3 client to access it correctly.</p>

<p>Likewise, you may need to specify an URL scheme of “http”, as the above does not expose an HTTPS encrypted port.</p>

<p>There are also S3 client changes that <a href="https://github.com/deis/deis/issues/2326">may be necessary</a> depending on the “calling format” of the client libraries. You may need to <a href="http://stackoverflow.com/questions/24312350/using-paperclip-fog-and-ceph">changes things like paperclip</a> to <a href="https://github.com/thoughtbot/paperclip/issues/1577">work with fog</a>. There are numerous tools that work happily with ceph, like <a href="https://github.com/stiller/s3_to_ceph/blob/master/s3_to_ceph.rb">s3_to_ceph</a> and even gems like <a href="https://github.com/fog/fog-radosgw">fog-radosgw</a> that try and help make this painless for your apps.</p>

<p>I will update this blog post shortly with an example of a containerized s3 client to show how to prove your ceph radosgw is working.</p>

<p>Have fun!</p>

]]></content>
  </entry>
  
</feed>
