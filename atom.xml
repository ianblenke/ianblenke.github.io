<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Ian Blenke - DevOps]]></title>
  <link href="http://ianblenke.github.io/atom.xml" rel="self"/>
  <link href="http://ianblenke.github.io/"/>
  <updated>2014-11-11T01:59:29-05:00</updated>
  <id>http://ianblenke.github.io/</id>
  <author>
    <name><![CDATA[Ian Blenke]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Docker Rspec TDD]]></title>
    <link href="http://ianblenke.github.io/blog/2014/11/10/docker-rspec-tdd/"/>
    <updated>2014-11-10T14:38:37-05:00</updated>
    <id>http://ianblenke.github.io/blog/2014/11/10/docker-rspec-tdd</id>
    <content type="html"><![CDATA[<p>A Dockerfile both describes a Docker image as well as layers for the working directory, environment variables, ports, entrypoint commands, and other important interfaces.</p>

<p>Test-Driven Design should drive a developer toward implementation details, not the other way around.</p>

<p>A devops without tests is a sad devops indeed.</p>

<p>Working toward a docker based development environment, my first thoughts were toward <a href="http://serverspec.org/">Serverspec</a> by <a href="https://github.com/mizzy">Gosuke Miayshita</a>, as it is entirely framework agnostic. Gosuke gave an excellent presentation at ChefConf this year re-inforcing that Serverspec is <em>not</em> a chef centric tool, and works quite well in conjunction with other configuration management tools.</p>

<p>Researching Serverspec and docker a bit more, <a href="https://github.com/tcnksm">Taichi Nakashima</a> based his <a href="https://github.com/tcnksm-sample/docker-rspec">TDD of Dockerfile by RSpec on OS/X</a> using ssh.</p>

<p>With Docker 1.3 and later, there is a “docker exec” interactive docker API for allowing live sessions on processes spawned in the same process namespace as a running container, effectively allowing external access into a running docker container using only the docker API.</p>

<p><a href="http://blog.wercker.com/2013/12/23/Test-driven-development-for-docker.html">PIETER JOOST VAN DE SANDE</a> posted about using the docker-api to accomplish the goal of testing a Dockerfile. His work is based on the <a href="https://rubygems.org/gems/docker-api">docker-api</a> gem (github <a href="https://github.com/swipely/docker-api">swipely/docker-api</a>).</p>

<p>Looking into the docker-api source, there is no support yet for docker 1.3’s exec API interface to run Serverspec tests against the contents of a running docker container.</p>

<p>Attempting even the most basic docker API calls with docker-api, <a href="https://github.com/swipely/docker-api/issues/202">issue 202</a> made it apparent that TLS support for boot2docker would need to be addressed first.</p>

<p>Here is my functional <code>spec_helper.rb</code> with the fixes necessary to use docker-api without modifications:</p>

<div><script src="https://gist.github.com/5335483e4021954d815f.js"></script>
<noscript><pre><code>require &quot;docker&quot;

docker_host = ENV[&#39;DOCKER_HOST&#39;].dup

if(ENV[&#39;DOCKER_TLS_VERIFY&#39;])
  cert_path = File.expand_path ENV[&#39;DOCKER_CERT_PATH&#39;]
  Docker.options = {
    client_cert: File.join(cert_path, &#39;cert.pem&#39;),
    client_key: File.join(cert_path, &#39;key.pem&#39;)
  }
  Excon.defaults[:ssl_ca_file] = File.join(cert_path, &#39;ca.pem&#39;)
  docker_host.gsub!(/^tcp/,&#39;https&#39;)
end

Docker.url = docker_host</code></pre></noscript></div>

<p>Following this, I can drive the generation of a Dockerfile with a spec:</p>

<div><script src="https://gist.github.com/261e6fe930c922202151.js"></script>
<noscript><pre><code>require &quot;spec_helper&quot;

describe &quot;dockerfile built my_app image&quot; do
  before(:all) do
    @image = Docker::Image.all(:all =&gt; true).find { |image|
      Docker::Util.parse_repo_tag( image.info[&#39;RepoTags&#39;].first ).first == &#39;my_app&#39;
    }
    p @image.json[&quot;Env&quot;]
  end

  it &quot;should exist&quot; do
    expect(@image).not_to be_nil
  end

  it &quot;should have CMD&quot; do
    expect(@image.json[&quot;Config&quot;][&quot;Cmd&quot;]).to include(&quot;/run.sh&quot;)
  end

  it &quot;should expose the default port&quot; do
    expect(@image.json[&quot;Config&quot;][&quot;ExposedPorts&quot;].has_key?(&quot;3000/tcp&quot;)).to be_truthy
  end

  it &quot;should have environmental variable&quot; do
    expect(@image.json[&quot;Config&quot;][&quot;Env&quot;]).to include(&quot;HOME=/usr/src/app&quot;)
  end
end</code></pre></noscript></div>

<p>This drives me iteratively to write a Dockerfile that looks like:</p>

<div><script src="https://gist.github.com/ef7150420cbd328d5e41.js"></script>
<noscript><pre><code>FROM rails:onbuild
ENV HOME /usr/src/app
ADD docker/run.sh /run.sh
RUN chmod 755 /run.sh
EXPOSE 3000
CMD /run.sh</code></pre></noscript></div>

<p>Next step: extend docker-api to support exec for serverspec based testing of actual docker image contents.</p>

<p>Sláinte!</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[OpenStack on a Chromebox]]></title>
    <link href="http://ianblenke.github.io/blog/2014/11/09/openstack-on-a-chromebox/"/>
    <updated>2014-11-09T19:02:40-05:00</updated>
    <id>http://ianblenke.github.io/blog/2014/11/09/openstack-on-a-chromebox</id>
    <content type="html"><![CDATA[<p>Saturday’s project was installing OpenStack on a ChromeBox.</p>

<h2 id="step-0-identify-your-hardware-add-ram">Step 0: Identify your hardware, add RAM</h2>

<p>Before you begin, make sure you <a href="http://www.chromium.org/chromium-os/developer-information-for-chrome-os-devices">know which ChromeOS device you have</a>.</p>

<p>In my case, it was a <a href="http://www.chromium.org/chromium-os/developer-information-for-chrome-os-devices/samsung-sandy-bridge">Samsung Series 3 Chromebox</a>.</p>

<p>Thankfully, the memory was very easy to upgrade to 16G, as the bottom snaps right off.</p>

<p><img src="http://ianblenke.github.io/images/chromebox/stumpy-top-mid.jpg" alt="Samsung Series 3 Chromebox with bottom exposed" /></p>

<h2 id="step-1-make-a-chromeos-recovery-usb">Step 1: Make a ChromeOS recovery USB</h2>

<p>Plug in a 4G or larger USB stick, then open this URL on your ChromeOS device:</p>

<p><a href="chrome://imageburner">chrome://imageburner</a></p>

<p>Follow the instructions.</p>

<p>We shouldn’t need this, but you never know. (And, yes, I did end up needing it during one of my iterations while writing this post).</p>

<h2 id="step-2-enable-developer-mode">Step 2: Enable developer mode</h2>

<p>The switch in the image is how I put my ChromeBox <a href="http://www.chromium.org/chromium-os/poking-around-your-chrome-os-device#TOC-Putting-your-Chrome-OS-Device-into-Developer-Mode">into development mode</a>.</p>

<p><img src="http://ianblenke.github.io/images/chromebox/stumpy-dev-switch.jpg" alt="Dev Switch on a Samsung Chromebox" /></p>

<p>After flipping the switch, reboot.</p>

<p>On this first reboot, the existing on-board storage will be wiped entirely, erasing any account credentials and cached content.</p>

<h2 id="step-3-login-to-the-console-as-chronos">Step 3: Login to the console as “chronos”</h2>

<p>Using cntl-alt-F2, enter the username “chronos” at the login: prompt. Hit return at the password: prompt (the default chronos password is an empty string).</p>

<p>Note: You did not actually need to login to google via the UI interface.</p>

<h2 id="step-4-enable-usb-and-seabios-booting">Step 4: Enable USB and SeaBIOS booting</h2>

<p>Now that you have a shell as chronos, you can enable USB booting.</p>

<pre><code>sudo crossystem dev_boot_usb=1
</code></pre>

<p>After enabling and rebooting, you can now boot from USB drives with <code>cntl-u</code></p>

<p>In order to install Ubuntu (or your distro of choice), we need to legacy boot. This requires a BIOS.</p>

<p>Newer ChromeBox hardware includes SeaBIOS natively.</p>

<pre><code>sudo crossystem dev_boot_legacy=1
</code></pre>

<p>After enabling and rebooting, you can now boot to legacy images with <code>cntl-l</code></p>

<p>If you have an older ChromeBox (like the Samsung Series 3) that doesn’t have a SeaBIOS boot, you will need to flash one.</p>

<p>Flashing a new bootstrap requires a jumper or other physical change to hardware to allow the <a href="http://flashrom.org/Flashrom">flashrom</a> tool to write to flash.</p>

<p>NOTE: <b>ASSUME THAT THIS WILL LIKELY BRICK YOUR CHROMEBOX. YOU HAVE BEEN WARNED</b></p>

<p>On the Samsung Series 3 ChromeBox, the jumper looks like this:</p>

<p><img src="http://ianblenke.github.io/images/chromebox/spi-flash-chromebox.jpg" alt="Samsung Series 3 flash jumper" /></p>

<p>A bit of folded tin-foil made for a quick jumper.</p>

<p><a href="http://johnlewis.ie/">John Lewis</a> is <a href="https://johnlewis.ie/custom-chromebook-firmware/rom-download/">maintaining a marvelous script</a> that auto-detects and flashes an updated SeaBIOS for most ChromeBook/ChromeBox hardware:</p>

<div><script src="https://gist.github.com/22e429b4424b74e51869.js"></script>
<noscript><pre><code>wget https://johnlewis.ie/getnflash_johnlewis_rom.sh
chmod u+x getnflash_johnlewis_rom.sh
./getnflash_johnlewis_rom.sh</code></pre></noscript></div>

<p>The script makes you type “If this bricks my Chromebook/box, on my head be it!” to make sure you understand that you are most likely going to brick your chromebox/chromebook by proceeding. This is no joke.</p>

<p>Being ok with potentially bricking my ChromeBox, I went ahead.</p>

<p>The script ran to completion without errors, and was thoroughly successful.</p>

<p>After rebooting, I now get a SeaBIOS splash identification (rather than the eventual sick computer).</p>

<p>The downside to doing this is that I now <em>must</em> boot off of an external USB device, as the SeaBIOS doesn’t seem to support booting from the built-in MMC SSD anymore.</p>

<h2 id="step-5-install-your-linux-distribution">Step 5: Install your Linux distribution</h2>

<p>I went ahead and pulled an Ubuntu Trust 14.04 ISO and DD’ed it to a USB stick on my Mac.</p>

<pre><code>wget -c http://releases.ubuntu.com/14.04/ubuntu-14.04-desktop-amd64.iso
diskutil list
hdiutil unmount /Volumes/USBSTICK
sudo dd if=ubuntu-14.04-desktop-amd64.iso of=/dev/disk5 bs=1m
</code></pre>

<p>After it finished flashing, I removed the USB stick from my Mac and plugged it into the front of the ChromeBox.</p>

<p>The USB installation media for Ubuntu was detected by SeaBIOS as the second bootable USB device.</p>

<p>I also attached 2 external 1TB USB disks to the back as the media that will be installed to.
These appeared as the third and fourth bootable devices to SeaBIOS.</p>

<p>With my new SeaBIOS bootstrap, I now must hit “Esc” and “2” to boot off of the first USB stick for the Ubuntu installation.</p>

<p>This presented me with the Ubunu boot menu.</p>

<p>Beyond this point, I installed Ubuntu to the two external 1TB USB disks, with a primary boot partition (type “83”) on each for /boot as normal linux ext4, and a primary RAID partition (type “fd”) on each for the RAID1 mirror upon which I layered LVM with a volume group named “vg” and a “rootfs” and a “swapfs” logical volume. At the end, I installed the grub boot sector to /dev/sdb and /dev/sdc (the two external 1TB USB drives).</p>

<p>After removing the USB stick for the Ubuntu installation media, the SeaBIOS entries shifted by 1.</p>

<p>With my new SeaBIOS bootstrap, I now must hit “Esc” and “2” to boot off of the first USB 1TB drive, or “3” for the second USB 1TB drive.</p>

<p>When I figure out how to get around the SeaBIOS hang on boot if I don’t do this, I will update this blog post.</p>

<h2 id="step-4-devstack-installation-of-openstack">Step 4: Devstack installation of OpenStack</h2>

<p>From this point forward, I followed the <a href="http://devstack.org">DevStack</a> <a href="http://docs.openstack.org/developer/devstack/guides/single-machine.html">All-in-one single-machine install guide</a>.</p>

<p>My local.conf for the all-in-one install is a collection of bits and pieces collected while digging around:</p>

<div><script src="https://gist.github.com/7084bf5a815d4bdc474c.js"></script>
<noscript><pre><code>[[local|localrc]]

SERVICE_TOKEN=a682f596-76f3-11e3-b3b2-e716f9080d50
ADMIN_PASSWORD=nomoresecrets
MYSQL_PASSWORD=iheartdatabases
RABBIT_PASSWORD=flopsymopsy
SERVICE_PASSWORD=$ADMIN_PASSWORD
#HOST_IP=10.0.0.106
DEST=/opt/stack
LOGFILE=$DEST/logs/stack.sh.log
LOGDAYS=7
#LOG_COLOR=False
# Uncomment these to grab the milestone-proposed branches from the repos:
#CINDER_BRANCH=milestone-proposed
#GLANCE_BRANCH=milestone-proposed
#HORIZON_BRANCH=milestone-proposed
#KEYSTONE_BRANCH=milestone-proposed
#KEYSTONECLIENT_BRANCH=milestone-proposed
#NOVA_BRANCH=milestone-proposed
#NOVACLIENT_BRANCH=milestone-proposed
#NEUTRON_BRANCH=milestone-proposed
#SWIFT_BRANCH=milestone-proposed
SWIFT_HASH=66a3d6b56c1f479c8b4e70ab5c2000f5
SWIFT_REPLICAS=1
SWIFT_DATA_DIR=$DEST/data/swift
FIXED_RANGE=10.1.0.0/24
FLOATING_RANGE=10.2.0.0/24
FIXED_NETWORK_SIZE=256
PUBLIC_INTERFACE=eth0
NET_MAN=FlatDHCPManager
FLAT_NETWORK_BRIDGE=br100
SCREEN_LOGDIR=$DEST/logs/screen
VOLUME_GROUP=&quot;vg&quot;
VOLUME_NAME_PREFIX=&quot;cinder-&quot;
VOLUME_BACKING_FILE_SIZE=10250M
API_RATE_LIMIT=False
VIRT_DRIVER=libvirt
LIBVIRT_TYPE=kvm
SCHEDULER=nova.scheduler.simple.SimpleScheduler</code></pre></noscript></div>

<p>As the stack user, running <code>./stack.sh</code> kicked off the install, and it completed successfully.</p>

<p>At the end, it tells you the URLs to use to access your new OpenStack server:</p>

<pre><code>Horizon is now available at http://10.0.0.106/
Keystone is serving at http://10.0.0.106:5000/v2.0/
Examples on using novaclient command line is in exercise.sh
The default users are: admin and demo
The password: nomoresecrets
This is your host ip: 10.0.0.106
</code></pre>

<p>I also ended up creating this <code>~/.fog</code> file locally on my Mac, based on <a href="http://docs.cloudfoundry.org/deploying/openstack/validate_openstack.html">CloudFoundry’s guide to validating your OpenStack</a>.</p>

<div><script src="https://gist.github.com/c87328c5635cdcadb8e0.js"></script>
<noscript><pre><code>:openstack:
  :openstack_auth_url:  http://10.0.0.106:5000/v2.0/tokens
  :openstack_api_key:   nomoresecrets
  :openstack_username:  admin
  :openstack_tenant:    admin
  :openstack_region:    RegionOne # Optional
  </code></pre></noscript></div>

<p>With it, I can now use the <a href="http://fog.io">fog</a> command-line tool locally on my development Mac to manipulate the ChromeBox based OpenStack server.</p>

<p>Cheers!</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Fig-docker]]></title>
    <link href="http://ianblenke.github.io/blog/2014/11/07/fig-docker/"/>
    <updated>2014-11-07T19:20:06-05:00</updated>
    <id>http://ianblenke.github.io/blog/2014/11/07/fig-docker</id>
    <content type="html"><![CDATA[<p>A common devops problem when developing <a href="http://docker.io">Docker</a> containers is managing the orchestration of multiple containers in a development environment.</p>

<p>There are a number of orchestration harnesses for Docker available:</p>

<ul>
  <li>Docker’s <a href="http://fig.sh">Fig</a></li>
  <li><a href="https://docs.vagrantup.com/v2/provisioning/docker.html">Vagrant</a></li>
  <li><a href="https://github.com/GoogleCloudPlatform/kubernetes">kubernetes</a></li>
  <li><a href="https://github.com/signalfuse/maestro-ng">maestro-ng</a></li>
  <li>Centurylink’s <a href="http://panamax.io/">Panamax</a></li>
  <li><a href="http://shipyard-project.com/">Shipyard</a></li>
  <li><a href="http://decking.io/">Decking</a></li>
  <li>NewRelic’s <a href="https://github.com/newrelic/centurion">Centurion</a></li>
  <li>Spotify’s <a href="https://github.com/spotify/helios">Helios</a></li>
  <li><a href="https://github.com/cattleio/stampede">Stampede</a></li>
  <li><a href="https://www.getchef.com/solutions/docker/">Chef</a></li>
  <li><a href="http://www.ansible.com/docker">Ansible</a></li>
  <li><a href="https://flynn.io/">Flynn</a></li>
  <li><a href="http://tsuru.io/">Tsuru</a></li>
  <li><a href="https://clusterhq.com/">Flocker</a></li>
  <li><a href="https://github.com/CloudCredo/cloudfocker">CloudFocker</a></li>
  <li><a href="http://cloudfoundry.org">CloudFoundry</a>’s <a href="https://github.com/cf-platform-eng/docker-boshrelease">docker-boshrelease</a>/<a href="https://github.com/cloudfoundry-incubator/diego-release">diego</a></li>
  <li><a href="http://deis.io">Deis</a> (a PaaS that can git push deploy containers using <a href="http://heroku.com">Heroku</a> buildpacks <em>or</em> a Dockerfile)</li>
</ul>

<p>There are also RAFT/GOSSIP clustering solutions like:</p>

<ul>
  <li><a href="https://coreos.com/">CoreOS</a>/<a href="https://github.com/coreos/fleet">Fleet</a></li>
  <li><a href="https://www.openshift.com/products/origin">OpenShift Origin</a> uses <a href="http://www.projectatomic.io/">ProjectAtomic</a>/<a href="https://openshift.github.io/geard/">Geard</a></li>
</ul>

<p>My <a href="https://github.com/ianblenke/coreos-vagrant-kitchen-sink">coreos-vagrant-kitchen-sink</a> github project submits <a href="https://github.com/ianblenke/coreos-vagrant-kitchen-sink/tree/master/cloud-init">cloud-init units</a> via a YAML file when booting member nodes. It’s a good model for production, but it’s a bit heavy for development.</p>

<p>Docker is currently working on <a href="https://www.youtube.com/watch?v=vtnSL79rZ6o">Docker Clustering</a>, but it is presently just a proof-of-concept and is now under a total re-write.</p>

<p>They are also <a href="https://www.youtube.com/watch?v=YuSq6bXHnOI">implementing docker composition</a> which provides Fig like functionality using upcoming docker “groups”.</p>

<p>That influence of Fig makes sense, as <a href="http://venturebeat.com/2014/07/22/docker-buys-orchard-a-2-man-startup-with-a-cloud-service-for-running-docker-friendly-apps/">Docker bought Orchard</a>.</p>

<p>Internally, Docker developers use <a href="http://fig.sh">Fig</a>.</p>

<p>Docker’s website also directs everyone to <a href="http://boot2docker.io">Boot2Docker</a>, as that is the tool Docker developers use as their docker baseline environment. </p>

<p>Boot2Docker spawns a <a href="https://www.virtualbox.org/">VirtualBox</a> based VM as well as a native docker client runtime on the developer’s host machine, and provides the <code>DOCKER_HOST</code> and related enviroments necessary for the client to talk to the VM.</p>

<p>This allows a developer’s Windows or OS/X machine to have a docker command that behaves as if the docker containers are running natively on their host machine.</p>

<p>While Fig is easy to install under OS/X as it has native Python support (“pip install fig”), installing Fig on a Windows developer workstation would normally require Python support be installed separately.</p>

<p>Rather than do that, I’ve built a new <a href="https://registry.hub.docker.com/u/ianblenke/fig-docker/">ianblenke/fig-docker</a> docker Hub image, which is auto-built from <a href="https://github.com/ianblenke/docker-fig-docker">ianblenke/docker-fig-docker</a> on github.</p>

<p>This allows running fig inside a docker container using:</p>

<div><script src="https://gist.github.com/6cd8f8a4065bcac99443.js"></script>
<noscript><pre><code>docker run -v $(pwd):/app -v $DOCKER_CERT_PATH:/certs -e DOCKER_CERT_PATH=/certs -e DOCKER_HOST=$DOCKER_HOST -e DOCKER_TLS_VERIFY=$DOCKER_TLS_VERIFY -ti --rm ianblenke/fig-docker fig --help</code></pre></noscript></div>

<p>Alternatively, a developer can alias it:</p>

<div><script src="https://gist.github.com/f48bc5cf8b2567bd8006.js"></script>
<noscript><pre><code>alias fig=&quot;docker run -v $(pwd):/app -v $DOCKER_CERT_PATH:/certs -e DOCKER_CERT_PATH=/certs -e DOCKER_HOST=$DOCKER_HOST -e DOCKER_TLS_VERIFY=$DOCKER_TLS_VERIFY -ti --rm ianblenke/fig-docker fig&quot;</code></pre></noscript></div>

<p>Now the developer can run <code>fig</code> as if it is running on their development host, continuing the boot2docker illusion.</p>

<p>In the above examples, the current directory <code>$(pwd)</code> is being mounted as /app inside the docker container.</p>

<p>On a boot2docker install, the boot2docker VM is the actual source of that volume path.</p>

<p>That means you would actually have to have the current path inside the boot2docker VM as well.</p>

<p>To do that, on a Mac, do this:</p>

<div><script src="https://gist.github.com/444c3f6552744ef25f59.js"></script>
<noscript><pre><code>boot2docker down
VBoxManage sharedfolder add boot2docker-vm -name home -hostpath /Users
boot2docker up</code></pre></noscript></div>

<p>From this point forward, until the next <code>boot2docker init</code>, your boot2docker VM should have your home directory mounted as /Users and the path should be the same.</p>

<p>A similar trick happens for Windows hosts, providing the same path inside the boot2docker VM as a developer would use.</p>

<p>This allows a normalized docker/fig interface for developers to begin their foray into docker orchestration.</p>

<p>Let’s setup a very quick <a href="http://rubyonrails.org/">Ruby on Rails</a> application from scratch, and then add a Dockerfile and fig.yml that spins up a mysql service for it to talk to.</p>

<p>Here’s a quick script that does just that. The only requirement is a functional docker command able to spin up containers.</p>

<div><script src="https://gist.github.com/92648de57cb7d38fd1e8.js"></script>
<noscript><pre><code>#!/bin/bash
set -ex

# Source the boot2docker environment variables
eval $(boot2docker shellinit 2&gt;/dev/null)

# Use a rails container to create a new rails project in the current directory called figgypudding
docker run -it --rm -v $(pwd):/app rails:latest bash -c &#39;rails new figgypudding; cp -a /figgypudding /app&#39;

cd figgypudding

# Create the Dockerfile used to build the figgypudding_web:latest image used by the figgypudding_web_1 container
cat &lt;&lt;EOD &gt; Dockerfile
FROM rails:onbuild
ENV HOME /usr/src/app
EOD

# This is the Fig orchestration configuration
cat &lt;&lt;EOF &gt; fig.yml
mysql:
  environment:
    MYSQL_ROOT_PASSWORD: supersecret
    MYSQL_DATABASE: figgydata
    MYSQL_USER: figgyuser
    MYSQL_PASSWORD: password
  ports:
    - &quot;3306:3306&quot;
  image: mysql:latest
figgypudding:
  environment:
    RAILS_ENV: development
    DATABASE_URL: mysql2://figgyuser:password@172.17.42.1:3306/figgydata
  links:
    - mysql
  ports:
    - &quot;3000:3000&quot;
  build: .
  command: bash -xc &#39;bundle exec rake db:migrate &amp;&amp; bundle exec rails server&#39;
EOF

# Rails defaults to sqlite, convert it to use mysql
sed -i -e &#39;s/sqlite3/mysql2/&#39; Gemfile

# Update the Gemfile.lock using the rails container we referenced earlier
docker run --rm -v $(pwd):/usr/src/app -w /usr/src/app rails:latest bundle update

# Use the fig command from my fig-docker container to fire up the Fig formation
docker run -v $(pwd):/app -v $DOCKER_CERT_PATH:/certs -e DOCKER_CERT_PATH=/certs -e DOCKER_HOST=$DOCKER_HOST -e DOCKER_TLS_VERIFY=$DOCKER_TLS_VERIFY -ti --rm ianblenke/fig-docker fig up</code></pre></noscript></div>

<p>After running that, there should now be a web server running on the boot2docker VM, which should generally be <a href="http://192.168.59.103:3000/">http://192.168.59.103:3000/</a> as that seems to be the common boot2docker default IP.</p>

<p>This is fig, distilled to its essence.</p>

<p>Beyond this point, a developer can “fig build ; fig up” and see the latest result of their work. This is something ideally added as a git post-commit hook or a iteration harness like <a href="https://github.com/guard/guard">Guard</a>.</p>

<p>While it may not appear <em>pretty</em> at first glance, realize that only <code>cat</code>, and <code>sed</code> were used on the host here (and very well could also themselves have also been avoided). No additional software was installed on the host, yet a rails app was created and deployed in docker containers, talking to a mysql server.</p>

<p>And therein lies the elegance of dockerizing application deployment: simple, clean, repeatable units of software. Orchestrated.</p>

<p>Have fun!</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Self-standing Ceph/deis-store Docker Containers]]></title>
    <link href="http://ianblenke.github.io/blog/2014/11/05/self-standing-ceph-slash-deis-store-docker-containers/"/>
    <updated>2014-11-05T14:40:59-05:00</updated>
    <id>http://ianblenke.github.io/blog/2014/11/05/self-standing-ceph-slash-deis-store-docker-containers</id>
    <content type="html"><![CDATA[<p>A common challenge for cloud orchestration is simulating or providing an S3 service layer, particularly for development environments.</p>

<p>As Docker is meant for immutable infrastructure, this poses somewhat of a challenge for production deployments. Rather than tackle that subject here, we’ll revisit persistence on immutable infrastructure in a production capacity in a future blog post.</p>

<p>The first challenge is identifying an S3 implementation to throw into a container.</p>

<p>There are a few feature sparse/dummy solutions that might suit development needs:</p>

<ul>
  <li><a href="http://s3ninja.net/">s3-ninja</a> (github <a href="https://github.com/scireum/s3ninja">scireum/s3ninja</a>)</li>
  <li><a href="https://github.com/jubos/fake-s3">fake-s3</a></li>
  <li><a href="http://sourceforge.net/projects/s3mockup/">S3Mockup</a>
(and a number of others which I’d rather not even consider)</li>
</ul>

<p>There are a number of good functional options for actual S3 implementations:</p>

<ul>
  <li><a href="http://ceph.com">ceph</a> (github <a href="https://github.com/ceph/ceph">ceph/ceph</a>), specifically the <a href="http://ceph.com/docs/master/radosgw/">radosgw</a></li>
  <li><a href="https://github.com/eucalyptus/eucalyptus/wiki/Walrus-S3-API">walrus</a> from Eucalyptus</li>
  <li><a href="http://basho.com/riak-cloud-storage/">riak cs</a></li>
  <li><a href="http://www.skylable.com/download/#LibreS3">libres3</a>, backended by the opensource <a href="http://www.skylable.com/download/#SX">Skylable Sx</a></li>
  <li><a href="https://github.com/nimbusproject/nimbus/tree/master/cumulus">cumulus</a> is an S3 implementation for <a href="http://www.nimbusproject.org/docs/current/faq.html#cumulus">Nimbus</a></li>
  <li><a href="http://www.cloudian.com/community-edition.php">cloudian</a> which is a non-opensource commercial product</li>
  <li><a href="https://github.com/stackforge/swift3">swift3</a> as an S3 compatibility layer with swift on the backend</li>
  <li><a href="https://github.com/cloudfoundry-attic/vblob">vblob</a> a node.js based attic’ed project at CloudFoundry</li>
  <li><a href="https://github.com/mattjamieson/parkplace">parkplace</a> backended by bittorrent</li>
  <li><a href="https://github.com/razerbeans/boardwalk">boardwalk</a> backended by ruby, sinatra, and mongodb</li>
</ul>

<p>Of the above, one stands out as the underlying persistence engine used by a larger docker backended project: <a href="http://deis.io">Deis</a></p>

<p>Rather than re-invent the wheel, it is possible to use deis-store directly.</p>

<p>As Deis deploys on CoreOS, there is an understandable inherent dependency on <a href="http://github.com/coreos/etcd/">etcd</a> for service discovery.</p>

<p>If you happen to be targeting CoreOS, you can simply point your etcd –peers option or <code>ETCD_HOST</code> environment variable at <code>$COREOS_PRIVATE_IPV4</code> and skip this next step.</p>

<p>First, make sure your environment includes the <code>DOCKER_HOST</code> and related variables for the boot2docker environment:</p>

<div><script src="https://gist.github.com/cab2661e67f5d79ae9bd.js"></script>
<noscript><pre><code>eval $(boot2docker shellinit)</code></pre></noscript></div>

<p>Now, discover the IP of the boot2docker guest VM, as that is what we will bind the etcd to:</p>

<div><script src="https://gist.github.com/00d61147bbf81ca26d2d.js"></script>
<noscript><pre><code>IP=&quot;$(boot2docker ip 2&gt;/dev/null)&quot;</code></pre></noscript></div>

<p>Next, we can spawn etcd and publish the ports for the other containers to use:</p>

<div><script src="https://gist.github.com/3a47603ef0561e54ecb6.js"></script>
<noscript><pre><code>docker run --name etcd \
           --publish 4001:4001 \
           --publish 7001:7001 \
           --detach \
           coreos/etcd:latest \
           /go/bin/app -listen-client-urls http://0.0.0.0:4001 \
                       -advertise-client-urls http://$IP:4001 \
                       -listen-peer-urls http://0.0.0.0:7001 \
                       -initial-advertise-peer-urls http://$IP:7001 \
                       -data-dir=/tmp/etcd</code></pre></noscript></div>

<p>Normally, we wouldn’t put the etcd persistence in a tmpfs for consistency reasons after a reboot, but for a development container: we love speed!</p>

<p>Now that we have an etcd container running, we can spawn the deis-store daemon container that runs the ceph object-store daemon (OSD) module.</p>

<div><script src="https://gist.github.com/c05525539a9f38e51e4b.js"></script>
<noscript><pre><code>docker run --name deis-store-daemon \
           --volumes-from=deis-store-daemon-data \
           --env HOST=$IP \
           --publish 6800 \
           --net host \
           --detach \
           deis/store-daemon:latest</code></pre></noscript></div>

<p>It is probably a good idea to mount the /var/lib/deis/store volume for persistence, but this is a developer container, so we’ll forego that step.</p>

<p>The ceph-osd will wait in a loop when starting until it can talk to ceph-mon, which is the next component provided by the deis-store monitor container.</p>

<p>In order to prepare the etcd config tree for deis-store monitor, we must first set a key for this new deis-store-daemon component.</p>

<p>While we could do that with a wget/curl PUT to the etcd client port (4001), using etcdctl makes things a bit easier.</p>

<p>It is generally a good idea to match the version of the etcdctl client with the version of etcd you are using.</p>

<p>As the CoreOS team doesn’t put out an etcdctl container as of yet, one way to do this is to build/install etcdctl inside a coreos/etcd container:</p>

<div><script src="https://gist.github.com/4fcf5bcca7077a85e7ce.js"></script>
<noscript><pre><code>docker run --rm \
           coreos/etcd \
           /bin/sh -c &quot;cd /go/src/github.com/coreos/etcd/etcdctl; \
                       go install ; \
                       /go/bin/etcdctl --peers $IP:4001 \
                                       set /deis/store/hosts/$IP $IP&quot;</code></pre></noscript></div>

<p>This isn’t ideal, of course, as there is a slight delay as etcdctl is built and installed before we use it, but it serves the purpose.</p>

<p>There are also <a href="http://docs.deis.io/en/latest/managing_deis/store_daemon_settings/">deis/store-daemon settings</a> of etcd keys that customize the behavior of ceph-osd a bit.</p>

<p>Now we can start deis-store-monitor, which will use that key to spin up a ceph-mon that monitors this (and any other) ceph-osd instances likewise registered in the etcd configuration tree.</p>

<div><script src="https://gist.github.com/543a13ba9410f6cf2f8e.js"></script>
<noscript><pre><code>docker run --name deis-store-monitor \
           --env HOST=$IP \
           --publish 6789 \
           --net host \
           --detach \
           deis/store-monitor:latest</code></pre></noscript></div>

<p>As before, there are volumes that probably should be mounted for /etc/ceph and /var/lib/ceph/mon, but this is a development image, so we’ll skip that.</p>

<p>There are also <a href="http://docs.deis.io/en/latest/managing_deis/store_monitor_settings/">deis/store-monitor settings</a> of etcd keys that customize the behavior of ceph-mon a bit.</p>

<p>Now that ceph-mon is running, ceph-osd will continue starting up. We now have a single-node self-standing ceph storage platform, but no S3.</p>

<p>The S3 functionality is provided by the ceph-radosgw component, which is provided by the deis-store-gateway container.</p>

<div><script src="https://gist.github.com/5634583a93347c415b3d.js"></script>
<noscript><pre><code>docker run --name deis-store-gateway \
           --hostname deis-store-gateway \
           --env HOST=$IP \
           --env EXTERNAL_PORT=8888 \
           --publish 8888:8888 \
           --detach \
           deis/store-gateway:latest</code></pre></noscript></div>

<p>There is no persistence in ceph-radosgw that warrant a volume mapping, so we can ignore that entirely regardless of environment.</p>

<p>There are also <a href="http://docs.deis.io/en/latest/managing_deis/store_gateway_settings/">deis/store-gateway settings</a> of etcd keys that customize the behavior of ceph-radosgw a bit.</p>

<p>We now have a functional self-standing S3 gateway, but we don’t know the credentials to use it. For that, we can run etcdctl again:</p>

<div><script src="https://gist.github.com/9c43ccd03c6c082073b7.js"></script>
<noscript><pre><code>AWS_ACCESS_KEY_ID=$(docker run --rm coreos/etcd /bin/sh -c &quot;cd /go/src/github.com/coreos/etcd/etcdctl; go install ; /go/bin/etcdctl --peers $IP:4001 get /deis/store/gateway/accessKey&quot;)
AWS_SECRET_ACCESS_KEY=$(docker run --rm coreos/etcd /bin/sh -c &quot;cd /go/src/github.com/coreos/etcd/etcdctl; go install ; /go/bin/etcdctl --peers $IP:4001 get /deis/store/gateway/secretKey&quot;)
AWS_S3_HOST=$(docker run --rm coreos/etcd /bin/sh -c &quot;cd /go/src/github.com/coreos/etcd/etcdctl; go install ; /go/bin/etcdctl --peers $IP:4001 get /deis/store/gateway/host&quot;)
AWS_S3_PORT=$(docker run --rm coreos/etcd /bin/sh -c &quot;cd /go/src/github.com/coreos/etcd/etcdctl; go install ; /go/bin/etcdctl --peers $IP:4001 get /deis/store/gateway/port&quot;)</code></pre></noscript></div>

<p>Note that the host here isn’t the normal AWS gateway address, so you will need to specify things for your S3 client to access it correctly.</p>

<p>Likewise, you may need to specify an URL scheme of “http”, as the above does not expose an HTTPS encrypted port.</p>

<p>There are also S3 client changes that <a href="https://github.com/deis/deis/issues/2326">may be necessary</a> depending on the “calling format” of the client libraries. You may need to <a href="http://stackoverflow.com/questions/24312350/using-paperclip-fog-and-ceph">changes things like paperclip</a> to <a href="https://github.com/thoughtbot/paperclip/issues/1577">work with fog</a>. There are numerous tools that work happily with ceph, like <a href="https://github.com/stiller/s3_to_ceph/blob/master/s3_to_ceph.rb">s3_to_ceph</a> and even gems like <a href="https://github.com/fog/fog-radosgw">fog-radosgw</a> that try and help make this painless for your apps.</p>

<p>I will update this blog post shortly with an example of a containerized s3 client to show how to prove your ceph radosgw is working.</p>

<p>Have fun!</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Using Boot2Docker]]></title>
    <link href="http://ianblenke.github.io/blog/2014/11/04/using-boot2docker/"/>
    <updated>2014-11-04T23:13:25-05:00</updated>
    <id>http://ianblenke.github.io/blog/2014/11/04/using-boot2docker</id>
    <content type="html"><![CDATA[<h2 id="boot2docker-command-line">Boot2Docker command-line</h2>

<p>Preface: the <a href="https://github.com/boot2docker/boot2docker">boot2docker README</a> is a great place to discover the below commands in more detail.</p>

<p>Now that we have Boot2Docker installed, we need to initialize a VM instance</p>

<pre><code>boot2docker init
</code></pre>

<p>This merely defines the default boot2docker VM, it does not start it. To do that, we need to bring it “up”</p>

<pre><code>boot2docker up
</code></pre>

<p>When run, it looks something like this:</p>

<pre><code>icbcfmbp:~ icblenke$ boot2docker up
Waiting for VM and Docker daemon to start...
..........ooo
Started.
Writing /Users/icblenke/.boot2docker/certs/boot2docker-vm/ca.pem
Writing /Users/icblenke/.boot2docker/certs/boot2docker-vm/cert.pem
Writing /Users/icblenke/.boot2docker/certs/boot2docker-vm/key.pem

To connect the Docker client to the Docker daemon, please set:
    export DOCKER_CERT_PATH=/Users/icblenke/.boot2docker/certs/boot2docker-vm
    export DOCKER_TLS_VERIFY=1
    export DOCKER_HOST=tcp://192.168.59.103:2376

icbcfmbp:~ icblenke$
</code></pre>

<p>This is all fine and dandy, but that shell didn’t actually source those variables. To do that we use boot2docker shellinit:</p>

<pre><code>eval $(boot2docker shellinit)
</code></pre>

<p>Now the shell has those variables exported for the running boot2docker VM.</p>

<p>The persistence of the boot2docker VM lasts only until we run a boot2docker destroy</p>

<pre><code>boot2docker destroy
</code></pre>

<p>After doing this, there is no longer a VM defined. We would need to go back to the boot2docker init step above and repeat.</p>

<h2 id="docker-command-line">Docker command-line</h2>

<p>From this point forward, we use the docker command to interact with the boot2docker VM as if we are on a linux docker host.</p>

<p>The docker command is just a compiled go application that makes RESTful calls to the docker daemon inside the linux VM.</p>

<pre><code>bash-3.2$ docker info
Containers: 0
Images: 0
Storage Driver: aufs
 Root Dir: /mnt/sda1/var/lib/docker/aufs
  Dirs: 0
  Execution Driver: native-0.2
  Kernel Version: 3.16.4-tinycore64
  Operating System: Boot2Docker 1.3.1 (TCL 5.4); master : 9a31a68 - Fri Oct 31 03:14:34 UTC 2014
  Debug mode (server): true
  Debug mode (client): false
  Fds: 10
  Goroutines: 11
  EventsListeners: 0
  Init Path: /usr/local/bin/docker
</code></pre>

<p>This holds true for both OS/X and Windows. </p>

<p>The boot2docker facade is just a handy wrapper to prepare the guest linux host VM for the docker daemin and local docker command-line client for your development host OS environment.</p>

<p>And now you have a starting point for exploring <a href="http://docker.io">Docker</a>!</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Installing Boot2Docker]]></title>
    <link href="http://ianblenke.github.io/blog/2014/11/04/installing-boot2docker/"/>
    <updated>2014-11-04T22:41:53-05:00</updated>
    <id>http://ianblenke.github.io/blog/2014/11/04/installing-boot2docker</id>
    <content type="html"><![CDATA[<p>Starting with a new team of developers, it helps to document the bootstrapping steps to a development environment.</p>

<p>Rather than try and use a convergence tool like Chef, Puppet, Ansible, or SALT, this time the environment will embrace Docker.</p>

<p>We could use a tool like Vagrant, but we need to support both Windows and Mac development workstations, and Vagrant under Windows can be vexing.</p>

<p>For this, we will begin anew using <a href="http://boot2docker.io">Boot2Docker</a></p>

<p>Before we begin, be sure to install <a href="https://www.virtualbox.org/">VirtualBox</a> from Oracle’s <a href="https://www.virtualbox.org/">VirtualBox.org website</a></p>

<p>The easiest way to install VirtualBox is to use <a href="http://caskroom.io/">HomeBrew Cask</a> under <a href="http://brew.sh">HomeBrew</a></p>

<pre><code>brew install caskroom/cask/brew-cask
brew cask install virtualbox
</code></pre>

<p>The easiest way to install boot2docker is to use <a href="http://brew.sh">HomeBrew</a></p>

<pre><code>brew install boot2docker
</code></pre>

<p>Afterward, be sure to upgrade the homebrew bottle to the latest version of boot2docker:</p>

<pre><code>boot2docker upgrade
</code></pre>

<p>Alternatively, a sample commandline install of boot2docker might look like this:</p>

<pre><code>wget https://github.com/boot2docker/osx-installer/releases/download/v1.3.1/Boot2Docker-1.3.1.pkg
sudo installer -pkg ~/Downloads/Boot2Docker-1.3.1.pkg -target /
</code></pre>

<p>I’ll leave the commandline install of VirtualBox up to your imagination. With <a href="http://caskroom.io">HomeBrew Cask</a>, there’s really not much of a point.</p>

<p>If you’re still not comfortable, below is a pictoral screenshot guide to installing boot2docker the point-and-click way.</p>

<h2 id="step-0">Step 0</h2>

<p>Download <a href="https://github.com/boot2docker/osx-installer/releases">boot2docker for OS/X</a> or <a href="https://github.com/boot2docker/windows-installer/releases">boot2docker for Windows</a></p>

<h2 id="step-1">Step 1</h2>

<p>Run the downloaded Boot2Docker.pkg or docker-install.exe to kick off the installer.</p>

<p><img src="http://ianblenke.github.io/images/screenshots/boot2docker/step1-downloads.png" width="1636" height="1022" title="Downloads" alt="Boot2docker.pkg in Downloads folder" /></p>

<h2 id="step-2">Step 2</h2>

<p>Click the Continue button to allow the installer to run a program to detect if boot2docker can be installed.&lt;/p&gt;</p>

<p><img src="http://ianblenke.github.io/images/screenshots/boot2docker/step2-run-a-program.png" width="1318" height="968" title="Click Continue button" alt="Allow installer to run a program to detect if boot2docker can be installed" /></p>

<h2 id="step-3">Step 3</h2>

<p>Click the Continue button to proceed beyond the initial installation instructions dialog.</p>

<p><img src="http://ianblenke.github.io/images/screenshots/boot2docker/step3-install-splash.png" width="1316" height="944" title="Click Continue button" alt="Instructions to install boot2docker" /></p>

<h2 id="step-4">Step 4</h2>

<p>The installer will now ask for an admin username/password to obtain admin rights to install boot2docker.</p>

<p><img src="http://ianblenke.github.io/images/screenshots/boot2docker/step4-enter-password.png" width="1390" height="1104" title="Enter your password" alt="Installer asks for admin rights to install boot2docker" /></p>

<h2 id="step-5">Step 5</h2>

<p>Before installing, the installer will advise how much space the install will take. Click the Install button to start the actual install.</p>

<p><img src="http://ianblenke.github.io/images/screenshots/boot2docker/step5-standard-install.png" width="1310" height="968" title="Click Install button" alt="Advice on how much space boot2docker will take when installed" /></p>

<h2 id="step-6">Step 6</h2>

<p>When the installation is successfully, click the Close button to exit the installer.</p>

<p><img src="http://ianblenke.github.io/images/screenshots/boot2docker/step6-install-completed-successfully.png" width="1334" height="984" title="Click Close button" alt="Install completed successfully" /></p>

<h2 id="step-7">Step 7</h2>

<p>You now have a shiny icon for boot2docker in /Applications you can click on to start a boot2docker terminal window session.</p>

<p><img src="http://ianblenke.github.io/images/screenshots/boot2docker/step7-installed-boot2docker-app.png" width="1352" height="1006" title="Run boot2docker app" alt="Boot2docker app is in Applications" /></p>

<p>Congrats. You now have boot2docker installed.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Immutable Infrastructure Persistence]]></title>
    <link href="http://ianblenke.github.io/blog/2014/10/18/immutable-infrastructure-persistence/"/>
    <updated>2014-10-18T12:55:14-04:00</updated>
    <id>http://ianblenke.github.io/blog/2014/10/18/immutable-infrastructure-persistence</id>
    <content type="html"><![CDATA[<p>Here is a link to my <a href="http://barcamptampabay.org/">Tampa Bay Barcamp 2014 presentation slides</a> for <a href="http://ian.blenke.com/immutable-infrastructure-persistence/">Immutable Infrastructure Persistence</a></p>
]]></content>
  </entry>
  
</feed>
