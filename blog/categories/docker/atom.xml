<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Docker | Ian Blenke - DevOps]]></title>
  <link href="http://ian.blenke.com/blog/categories/docker/atom.xml" rel="self"/>
  <link href="http://ian.blenke.com/"/>
  <updated>2015-07-07T11:55:48-04:00</updated>
  <id>http://ian.blenke.com/</id>
  <author>
    <name><![CDATA[Ian Blenke]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[AWS Docker Walkthrough With ElasticBeanstalk: Part 2]]></title>
    <link href="http://ian.blenke.com/blog/2015/06/28/aws-docker-walkthrough-with-elasticbeanstalk-part-2/"/>
    <updated>2015-06-28T00:57:08-04:00</updated>
    <id>http://ian.blenke.com/blog/2015/06/28/aws-docker-walkthrough-with-elasticbeanstalk-part-2</id>
    <content type="html"><![CDATA[<p>While deploying docker containers for immutable infrastructure on AWS ElasticBeanstalk,
I’ve learned a number of useful tricks that go beyond the official Amazon documentation.</p>

<p>This series of posts are an attempt to summarize some of the useful bits that may benefit
others facing the same challenges.</p>

<hr />

<p>Previously: <a href="/blog/2015/06/27/aws-docker-walkthrough-with-elasticbeanstalk-part-1/">Part 1 : Preparing a VPC for your ElasticBeanstalk environments</a></p>

<hr />

<h1 id="part-2--creating-your-elasticbeanstalk-environment">Part 2 : Creating your ElasticBeanstalk environment</h1>

<hr />

<h3 id="step-1-create-your-application-in-aws">Step 1: Create your Application in AWS</h3>

<p>Each AWS account needs to have your ElasticBeanstalk application defined initially.</p>

<p>Operationally, there are few reasons to remove an application from an AWS account, so there’s a good bet it’s already there.</p>

<p><code>bash
aws elasticbeanstalk create-application \
  --profile aws-dev \
  --region us-east-1 \
  --application-name myapp \
  --description 'My Application'
</code></p>

<p>You should really only ever have to do this once per AWS account.</p>

<p>There is an example of this in the Makefile as the <code>make application</code> target.</p>

<h3 id="step-2--update-your-aws-development-environment">Step 2 : Update your AWS development environment.</h3>

<p>During our initial VPC creation, we used the <code>aws</code> command from the <code>awscli</code> python package.</p>

<p>When deploying ElasticBeanstalk applications, we use the <code>eb</code> command from the <code>awsebcli</code> python package.</p>

<p>On OS/X, we run:</p>

<p><code>bash
brew install awsebcli
</code></p>

<p>On Windows, chocolatey doesn’t have awsebcli, but we can install python pip:</p>

<p><code>bash
choco install pip
</code></p>

<p>Again, because <code>awsebcli</code> is a python tool, we can install with:</p>

<p><code>bash
pip install awscli
</code></p>

<p>You may (or may not) need to prefix that pip install with <code>sudo</code> on linux/unix flavors, depending. ie:</p>

<p><code>bash
sudo pip install awsebcli
</code></p>

<p>These tools will detect if they are out of date when you run them. You may eventually get a message like:</p>

<p><code>
Alert: An update to this CLI is available.
</code></p>

<p>When this happens, you will likely want to either upgrade via homebrew:</p>

<p><code>bash
brew update &amp; brew upgrade awsebcli
</code></p>

<p>or, more likely, upgrade using pip directly:</p>

<p><code>bash
pip install --upgrade awsebcli
</code></p>

<p>Again, you may (or may not) need to prefix that pip install with <code>sudo</code>, depending. ie:</p>

<p><code>bash
sudo pip install --upgrade awsebcli
</code></p>

<p>There really should be an awsebcli Docker image, but there presently is not. Add that to the list of images to build.</p>

<h3 id="step-3-create-a-ssh-key-pair-to-use">Step 3: Create a ssh key pair to use</h3>

<p>Typically you will want to generate an ssh key locally and upload the public part:</p>

<p><code>bash
ssh-keygen -t rsa -b 2048 -f ~/.ssh/myapp-dev -P ''
aws ec2 import-key-pair --key-name myapp-dev --public-key-material "$(cat ~/.ssh/myapp-dev.pub)"
</code></p>

<p>Alternatively, if you are on a development platform without ssh-keygen for some reason, you can have AWS generate it for you:</p>

<p><code>bash
aws ec2 create-key-pair --key-name myapp-dev &gt; ~/.ssh/myapp-dev
</code></p>

<p>The downside to the second method is that AWS has the private key (as they generated it, and you shipped it via https over the network to your local machine), whereas in the first example they do not.</p>

<p>This ssh key can be used to access AWS instances directly.</p>

<p>After creating this ssh key, it is probably a good idea that you add it to your team’s password management tool (Keepass, Hashicorp Vault, Trousseau, Ansible Vault, Chef Encrypted Databags, LastPass, 1Password, Dashlane, etc) so that the private key isn’t only on your development workstation in your local user account.</p>

<p>Note the naming convention of <code>~/.ssh/$(PROJECT)-$(ENVIRONMENT)</code> - this is the default key filename that <code>eb ssh</code> will use.</p>

<p>If you do not use the above naming convention, you will have to add the generated ssh private key to your ssh-agent’s keychain in order to use it:</p>

<p><code>bash
[ -n $SSH_AUTH_SOCK ] || eval $(ssh-agent)
ssh-add ~/.ssh/myapp-dev
</code></p>

<p>To list the ssh keys in your keychain, use:</p>

<p><code>bash
ssh-add -l
</code></p>

<p>So long as you see 4 or fewer keys, including they key you created above, you should be ok.</p>

<p>If you have more than 4 keys listed in your ssh-agent keychain, depending on the order they are tried by your ssh client, that may exceed the default number of ssh key retries allowed on the remote sshd server side, which will prevent you from connecting.</p>

<p>Now we should have an ssh key pair defined in AWS that we can use when spinning up instances.</p>

<h3 id="step-4-initialize-your-local-development-directory-for-the-eb-cli">Step 4: Initialize your local development directory for the eb cli</h3>

<p>Before using the <code>eb</code> command, you must <code>eb init</code> your project to create a <code>.elasticbeanstalk/config.yml</code> file:</p>

<p><code>bash
eb init --profile aws-dev
</code></p>

<p>The <code>--profile aws-dev</code> is optional, if you created profiles in your <code>~/.aws/config</code> file. If you are using AWS environment variables your your ACCESS/SECRET keys, or only one default AWS account, you may omit that.</p>

<p>The application must exist in AWS first, which is why this is run <em>after</em> the previous step of creating the Application in AWS.</p>

<p>You may be prompted for some critical bits:</p>

<pre><code>$ eb init --profile aws-dev
eb init --profile aws-dev

Select a default region
1) us-east-1 : US East (N. Virginia)
2) us-west-1 : US West (N. California)
3) us-west-2 : US West (Oregon)
4) eu-west-1 : EU (Ireland)
5) eu-central-1 : EU (Frankfurt)
6) ap-southeast-1 : Asia Pacific (Singapore)
7) ap-southeast-2 : Asia Pacific (Sydney)
8) ap-northeast-1 : Asia Pacific (Tokyo)
9) sa-east-1 : South America (Sao Paulo)
(default is 3): 1

Select an application to use
1) myapp
2) [ Create new Application ]
(default is 2): 1

Select a platform.
1) PHP
2) Node.js
3) IIS
4) Tomcat
5) Python
6) Ruby
7) Docker
8) Multi-container Docker
9) GlassFish
10) Go
(default is 1): 7

Select a platform version.
1) Docker 1.6.2
2) Docker 1.6.0
3) Docker 1.5.0
(default is 1): 1
Do you want to set up SSH for your instances?
(y/n): y

Select a keypair.
1) myapp-dev
2) [ Create new KeyPair ]
(default is 2): 1
</code></pre>

<p>Alternatively, to avoid the questions, you can specify the full arguments:</p>

<p><code>bash
eb init myapp --profile aws-dev --region us-east-1 -p 'Docker 1.6.2' -k myapp-dev
</code></p>

<p>The end result is a <code>.elasticbeanstalk/config.yml</code> that will look something like this:</p>

<p><code>yaml
branch-defaults:
  master:
    environment: null
global:
  application_name: myapp
  default_ec2_keyname: myapp-dev
  default_platform: Docker 1.6.2
  default_region: us-east-1
  profile: aws-dev
  sc: git
</code></p>

<p>Any field appearing as <code>null</code> will likely need some manual attention from you after the next step.</p>

<h3 id="step-5-create-the-elasticbeanstalk-environment">Step 5: Create the ElasticBeanstalk Environment</h3>

<p>Previously, in <a href="/blog/2015/06/27/aws-docker-walkthrough-with-elasticbeanstalk-part-1/">Part 1 : Preparing a VPC for your ElasticBeanstalk environments</a>, we generated a VPC using a CloudFormation with an output of the Subnets and Security Group. We will need those things below.</p>

<p>Here is a repeat of that gist:</p>

<p><div><script src='https://gist.github.com/59715079304a6db7182c.js'></script>
<noscript><pre><code>aws cloudformation describe-stacks --stack-name myapp-dev --profile aws-dev --region us-east-1 | jq -r &#39;.Stacks[].Outputs&#39;
[
  {
    &quot;Description&quot;: &quot;VPC Id&quot;,
    &quot;OutputKey&quot;: &quot;VpcId&quot;,
    &quot;OutputValue&quot;: &quot;vpc-b7d1d8d2&quot;
  },
  {
    &quot;Description&quot;: &quot;VPC&quot;,
    &quot;OutputKey&quot;: &quot;VPCDefaultNetworkAcl&quot;,
    &quot;OutputValue&quot;: &quot;acl-b3cfc7d6&quot;
  },
  {
    &quot;Description&quot;: &quot;VPC Default Security Group that we blissfully ignore thanks to self-referencing bugs&quot;,
    &quot;OutputKey&quot;: &quot;VPCDefaultSecurityGroup&quot;,
    &quot;OutputValue&quot;: &quot;sg-3e50a559&quot;
  },
  {
    &quot;Description&quot;: &quot;VPC Security Group created by this stack&quot;,
    &quot;OutputKey&quot;: &quot;VPCSecurityGroup&quot;,
    &quot;OutputValue&quot;: &quot;sg-0c50a56b&quot;
  },
  {
    &quot;Description&quot;: &quot;The subnet id for VPCSubnet0&quot;,
    &quot;OutputKey&quot;: &quot;VPCSubnet0&quot;,
    &quot;OutputValue&quot;: &quot;subnet-995236b2&quot;
  },
  {
    &quot;Description&quot;: &quot;The subnet id for VPCSubnet1&quot;,
    &quot;OutputKey&quot;: &quot;VPCSubnet1&quot;,
    &quot;OutputValue&quot;: &quot;subnet-6aa4fd1d&quot;
  },
  {
    &quot;Description&quot;: &quot;The subnet id for VPCSubnet2&quot;,
    &quot;OutputKey&quot;: &quot;VPCSubnet2&quot;,
    &quot;OutputValue&quot;: &quot;subnet-ad3644f4&quot;
  }
]</code></pre></noscript></div>
</p>

<p>There are two ways to create a new ElasticBeanstalk environment:</p>

<ul>
  <li>Using <code>eb create</code> with full arguments for the various details of the environment.</li>
  <li>Using <code>eb create</code> with a <code>--cfg</code> argument of a previous <code>eb config save</code> to a YAML file in <code>.elasticbeanstalk/saved_configs</code>.</li>
</ul>

<p>The first way looks something like this:</p>

<pre><code>eb create myapp-dev --verbose \
  --profile aws-dev \
  --tier WebServer \
  --cname myapp-dev \
  -p '64bit Amazon Linux 2015.03 v1.4.3 running Docker 1.6.2' \
  -k myapp-dev \
  -ip myapp-dev-InstanceProfile-1KCQJP9M5TSVZ \
  --tags Project=myapp,Environment=dev \
  --envvars DEBUG=info \
  --vpc.ec2subnets=subnet-995236b2,subnet-6aa4fd1d,subnet-ad3644f4 \
  --vpc.elbsubnets=subnet-995236b2,subnet-6aa4fd1d,subnet-ad3644f4 \
  --vpc.publicip --vpc.elbpublic --vpc.securitygroups=sg-0c50a56b
</code></pre>

<p>The <code>Makefile</code> has an <code>environment</code> target that removes the need to fill in the fields manually:</p>

<pre><code>outputs:
    @which jq &gt; /dev/null 2&gt;&amp;1 || ( which brew &amp;&amp; brew install jq || which apt-get &amp;&amp; apt-get install jq || which yum &amp;&amp; yum install jq || which choco &amp;&amp; choco install jq)
    @aws cloudformation describe-stacks --stack-name myapp-dev --profile aws-dev --region us-east-1 | jq -r '.Stacks[].Outputs | map({key: .OutputKey, value: .OutputValue}) | from_entries'

environment:
    eb create $(STACK) --verbose \
      --profile aws-dev \
      --tier WebServer \
      --cname $(shell whoami)-$(STACK) \
      -p '64bit Amazon Linux 2015.03 v1.4.3 running Docker 1.6.2' \
      -k $(STACK) \
      -ip $(shell make outputs | jq -r .InstanceProfile) \
      --tags Project=$(PROJECT),Environment=$(ENVIRONMENT) \
      --envvars DEBUG=info \
      --vpc.ec2subnets=$(shell make outputs | jq -r '[ .VPCSubnet0, .VPCSubnet1, .VPCSubnet2 ] | @csv') \
      --vpc.elbsubnets=$(shell make outputs | jq -r '[ .VPCSubnet0, .VPCSubnet1, .VPCSubnet2 ] | @csv') \
      --vpc.publicip --vpc.elbpublic \
      --vpc.securitygroups=$(shell make outputs | jq -r .VPCSecurityGroup)
</code></pre>

<p>On the other hand, after a quick config save:</p>

<p><code>bash
eb config save myapp-dev --profile aws-dev --region us-east-1 --cfg myapp-dev-sc
</code></p>

<p>We now have the above settings in a YAML file <code>.elasticbeanstalk/saved_configs/myapp-dev-sc.cfg.yml</code> which can be committed to our git project.</p>

<p>This leads to the second way to create an ElasticBeanstalk environment:</p>

<p><code>bash
eb create myapp-dev --cname myapp-dev --cfg myapp-dev-sc --profile aws-dev
</code></p>

<p>The flip side of that is the YAML save config has static values embedded in it for a specific deployed VPC.</p>

<p>More docker goodness to come in Part 3…</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[AWS Docker Walkthrough With ElasticBeanstalk: Part 1]]></title>
    <link href="http://ian.blenke.com/blog/2015/06/27/aws-docker-walkthrough-with-elasticbeanstalk-part-1/"/>
    <updated>2015-06-27T13:14:08-04:00</updated>
    <id>http://ian.blenke.com/blog/2015/06/27/aws-docker-walkthrough-with-elasticbeanstalk-part-1</id>
    <content type="html"><![CDATA[<p>While deploying docker containers for immutable infrastructure on AWS ElasticBeanstalk,
I’ve learned a number of useful tricks that go beyond the official Amazon documentation.</p>

<p>This series of posts are an attempt to summarize some of the useful bits that may benefit
others facing the same challenges.</p>

<hr />

<h1 id="part-1--preparing-a-vpc-for-your-elasticbeanstalk-environments">Part 1 : Preparing a VPC for your ElasticBeanstalk environments</h1>

<hr />

<h3 id="step-1--prepare-your-aws-development-environment">Step 1 : Prepare your AWS development environment.</h3>

<p>On OS/X, I install <a href="http://brew.sh">homebrew</a>, and then:</p>

<p><code>bash
brew install awscli
</code></p>

<p>On Windows, I install <a href="https://chocolatey.org/">chocolatey</a> and then:</p>

<p><code>bash
choco install awscli
</code></p>

<p>Because <code>awscli</code> is a python tool, on either of these, or on the various Linux distribution flavors, we can also avoid native package management and alternatively use python <code>easyinstall</code> or <code>pip</code> directly:</p>

<p><code>bash
pip install awscli
</code></p>

<p>You may (or may not) need to prefix that pip install with <code>sudo</code>, depending. ie:</p>

<p><code>bash
sudo pip install awscli
</code></p>

<p>These tools will detect if they are out of date when you run them. You may eventually get a message like:</p>

<p><code>
Alert: An update to this CLI is available.
</code></p>

<p>When this happens, you will likely want to either upgrade via homebrew:</p>

<p><code>bash
brew update &amp; brew upgrade awscli
</code></p>

<p>or, more likely, upgrade using pip directly:</p>

<p><code>bash
pip install --upgrade awscli
</code></p>

<p>Again, you may (or may not) need to prefix that pip install with <code>sudo</code>, depending. ie:</p>

<p><code>bash
sudo pip install --upgrade awscli
</code></p>

<p>For the hardcore Docker fans out there, this is pretty trivial to run as a container as well. See <a href="https://github.com/CenturyLinkLabs/docker-aws-cli">CenturyLinkLabs/docker-aws-cli</a> for a good example of that. Managing an aws config file requires volume mapping, or passing <code>-e AWS_ACCESS_KEY_ID={redacted} -e AWS_SECRET_ACCESS_KEY={redacted}</code>. There are various guides to doing this out there. This will not be one of them ;)</p>

<h3 id="step-2-prepare-your-aws-environment-variables">Step 2: Prepare your AWS environment variables</h3>

<p>If you haven’t already, <a href="http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html#config-settings-and-precedence">prepare for AWS cli access</a>.</p>

<p>You can now configure your <code>~/.aws/config</code> by running:</p>

<pre><code>aws configure
</code></pre>

<p>This will create a default configuration.</p>

<p>I’ve yet to work with any company with only one AWS account though. You will likely find that you need to support managing multiple AWS configuration profiles.</p>

<p>Here’s an example <code>~/.aws/config</code> file with multiple profiles:</p>

<p>&#8220;`
[default]
output = json
region = us-east-1</p>

<p>[profile aws-dev]
AWS_ACCESS_KEY_ID={REDACTED}
AWS_SECRET_ACCESS_KEY={REDACTED}</p>

<p>[profile aws-prod]
AWS_ACCESS_KEY_ID={REDACTED}
AWS_SECRET_ACCESS_KEY={REDACTED}
&#8220;`</p>

<p>You can create this by running:</p>

<p><code>bash
$ aws configure --profile aws-dev
AWS Access Key ID [REDACTED]: YOURACCESSKEY
AWS Secret Access Key [REDACTED]: YOURSECRETKEY
Default region name [None]: us-east-1
Default output format [None]: json
</code></p>

<p>Getting in the habit of specifying <code>--profile aws-dev</code> is a bit of a reassurance that you’re provisioning resources into the correct AWS account, and not sullying AWS cloud resources between VPC environments.</p>

<h3 id="step-3-preparing-a-vpc">Step 3: Preparing a VPC</h3>

<p>Deploying anything to AWS EC2 Classic instances these days is to continue down the path of legacy maintenance.</p>

<p>For new ElasticBeanstalk deployments, a VPC should be used.</p>

<p>The easiest/best way to deploy a VPC is to use a <a href="http://aws.amazon.com/cloudformation/aws-cloudformation-templates/">CloudFormation template</a>. </p>

<p>Below is a public gist of a VPC CloudFormation that I use for deployment:</p>

<p><div><script src='https://gist.github.com/0a6a6f26d1ecaa0d81eb.js'></script>
<noscript><pre><code>{
  &quot;AWSTemplateFormatVersion&quot;: &quot;2010-09-09&quot;,
  &quot;Description&quot;: &quot;MyApp VPC&quot;,
  &quot;Parameters&quot; : {
    &quot;Project&quot; : {
      &quot;Description&quot; : &quot;Project name to tag resources with&quot;,
      &quot;Type&quot; : &quot;String&quot;,
      &quot;MinLength&quot;: &quot;1&quot;,
      &quot;MaxLength&quot;: &quot;16&quot;,
      &quot;AllowedPattern&quot; : &quot;[a-z]*&quot;,
      &quot;ConstraintDescription&quot; : &quot;any alphabetic string (1-16) characters in length&quot;
    },
    &quot;Environment&quot; : {
      &quot;Description&quot; : &quot;Environment name to tag resources with&quot;,
      &quot;Type&quot; : &quot;String&quot;,
      &quot;AllowedValues&quot; : [ &quot;dev&quot;, &quot;qa&quot;, &quot;prod&quot; ],
      &quot;ConstraintDescription&quot; : &quot;must be one of dev, qa, or prod&quot;
    },
    &quot;SSHFrom&quot;: {
      &quot;Description&quot; : &quot;Lockdown SSH access (default: can be accessed from anywhere)&quot;,
      &quot;Type&quot; : &quot;String&quot;,
      &quot;MinLength&quot;: &quot;9&quot;,
      &quot;MaxLength&quot;: &quot;18&quot;,
      &quot;Default&quot; : &quot;0.0.0.0/0&quot;,
      &quot;AllowedPattern&quot; : &quot;(\\d{1,3})\\.(\\d{1,3})\\.(\\d{1,3})\\.(\\d{1,3})/(\\d{1,2})&quot;,
      &quot;ConstraintDescription&quot; : &quot;must be a valid CIDR range of the form x.x.x.x/x.&quot;
    },
    &quot;VPCNetworkCIDR&quot; : {
      &quot;Description&quot;: &quot;The CIDR block for the entire VPC network&quot;,
      &quot;Type&quot;: &quot;String&quot;,
      &quot;AllowedPattern&quot; : &quot;(\\d{1,3})\\.(\\d{1,3})\\.(\\d{1,3})\\.(\\d{1,3})/(\\d{1,2})&quot;,
      &quot;Default&quot;: &quot;10.114.0.0/16&quot;,
      &quot;ConstraintDescription&quot; : &quot;must be an IPv4 dotted quad plus slash plus network bit length in CIDR format&quot;
    },
    &quot;VPCSubnet0CIDR&quot; : {
      &quot;Description&quot;: &quot;The CIDR block for VPC subnet0 segment&quot;,
      &quot;Type&quot;: &quot;String&quot;,
      &quot;AllowedPattern&quot; : &quot;(\\d{1,3})\\.(\\d{1,3})\\.(\\d{1,3})\\.(\\d{1,3})/(\\d{1,2})&quot;,
      &quot;Default&quot;: &quot;10.114.0.0/24&quot;,
      &quot;ConstraintDescription&quot; : &quot;must be an IPv4 dotted quad plus slash plus network bit length in CIDR format&quot;
    },
    &quot;VPCSubnet1CIDR&quot; : {
      &quot;Description&quot;: &quot;The CIDR block for VPC subnet1 segment&quot;,
      &quot;Type&quot;: &quot;String&quot;,
      &quot;AllowedPattern&quot; : &quot;(\\d{1,3})\\.(\\d{1,3})\\.(\\d{1,3})\\.(\\d{1,3})/(\\d{1,2})&quot;,
      &quot;Default&quot;: &quot;10.114.1.0/24&quot;,
      &quot;ConstraintDescription&quot; : &quot;must be an IPv4 dotted quad plus slash plus network bit length in CIDR format&quot;
    },
    &quot;VPCSubnet2CIDR&quot; : {
      &quot;Description&quot;: &quot;The CIDR block for VPC subnet2 segment&quot;,
      &quot;Type&quot;: &quot;String&quot;,
      &quot;AllowedPattern&quot; : &quot;(\\d{1,3})\\.(\\d{1,3})\\.(\\d{1,3})\\.(\\d{1,3})/(\\d{1,2})&quot;,
      &quot;Default&quot;: &quot;10.114.2.0/24&quot;,
      &quot;ConstraintDescription&quot; : &quot;must be an IPv4 dotted quad plus slash plus network bit length in CIDR format&quot;
    }
  },
  &quot;Resources&quot; : {
    &quot;VPC&quot; : {
      &quot;Type&quot; : &quot;AWS::EC2::VPC&quot;,
      &quot;Properties&quot; : {
        &quot;EnableDnsSupport&quot; : &quot;true&quot;,
        &quot;EnableDnsHostnames&quot; : &quot;true&quot;,
        &quot;CidrBlock&quot; : { &quot;Ref&quot;: &quot;VPCNetworkCIDR&quot; },
        &quot;Tags&quot; : [
          { &quot;Key&quot; : &quot;Name&quot;, &quot;Value&quot; : { &quot;Fn::Join&quot;: [ &quot;-&quot;, [ &quot;vpc&quot;, { &quot;Ref&quot;: &quot;Project&quot; }, { &quot;Ref&quot; : &quot;Environment&quot; } ] ] } },
          { &quot;Key&quot; : &quot;Project&quot;, &quot;Value&quot; : { &quot;Ref&quot;: &quot;Project&quot; } },
          { &quot;Key&quot; : &quot;Environment&quot;, &quot;Value&quot; : { &quot;Ref&quot;: &quot;Environment&quot; } }
        ]
      }
    },
    &quot;VPCSubnet0&quot; : {
      &quot;Type&quot; : &quot;AWS::EC2::Subnet&quot;,
      &quot;Properties&quot; : {
        &quot;VpcId&quot; : { &quot;Ref&quot; : &quot;VPC&quot; },
        &quot;AvailabilityZone&quot;: { &quot;Fn::Select&quot; : [ 0, { &quot;Fn::GetAZs&quot; : &quot;&quot; } ] },
        &quot;CidrBlock&quot; : { &quot;Ref&quot;: &quot;VPCSubnet0CIDR&quot; },
        &quot;Tags&quot; : [
          { &quot;Key&quot; : &quot;Name&quot;, &quot;Value&quot; : { &quot;Fn::Join&quot;: [ &quot;-&quot;, [ &quot;subnet&quot;, { &quot;Ref&quot;: &quot;Project&quot; }, { &quot;Ref&quot;: &quot;Environment&quot; } ] ] } },
          { &quot;Key&quot; : &quot;AZ&quot;, &quot;Value&quot; : { &quot;Fn::Select&quot; : [ 0, { &quot;Fn::GetAZs&quot; : &quot;&quot; } ] } },
          { &quot;Key&quot; : &quot;Project&quot;, &quot;Value&quot; : { &quot;Ref&quot;: &quot;Project&quot; } },
          { &quot;Key&quot; : &quot;Environment&quot;, &quot;Value&quot; : { &quot;Ref&quot;: &quot;Environment&quot; } }
        ]
      }
    },
    &quot;VPCSubnet1&quot; : {
      &quot;Type&quot; : &quot;AWS::EC2::Subnet&quot;,
      &quot;Properties&quot; : {
        &quot;VpcId&quot; : { &quot;Ref&quot; : &quot;VPC&quot; },
        &quot;AvailabilityZone&quot;: { &quot;Fn::Select&quot; : [ 1, { &quot;Fn::GetAZs&quot; : &quot;&quot; } ] },
        &quot;CidrBlock&quot; : { &quot;Ref&quot;: &quot;VPCSubnet1CIDR&quot; },
        &quot;Tags&quot; : [
          { &quot;Key&quot; : &quot;Name&quot;, &quot;Value&quot; : { &quot;Fn::Join&quot;: [ &quot;-&quot;, [ &quot;subnet&quot;, { &quot;Ref&quot;: &quot;Project&quot; }, { &quot;Ref&quot;: &quot;Environment&quot; } ] ] } },
          { &quot;Key&quot; : &quot;AZ&quot;, &quot;Value&quot; : { &quot;Fn::Select&quot; : [ 1, { &quot;Fn::GetAZs&quot; : &quot;&quot; } ] } },
          { &quot;Key&quot; : &quot;Project&quot;, &quot;Value&quot; : { &quot;Ref&quot;: &quot;Project&quot; } },
          { &quot;Key&quot; : &quot;Environment&quot;, &quot;Value&quot; : { &quot;Ref&quot;: &quot;Environment&quot; } }
        ]
      }
    },
    &quot;VPCSubnet2&quot; : {
      &quot;Type&quot; : &quot;AWS::EC2::Subnet&quot;,
      &quot;Properties&quot; : {
        &quot;VpcId&quot; : { &quot;Ref&quot; : &quot;VPC&quot; },
        &quot;AvailabilityZone&quot;: { &quot;Fn::Select&quot; : [ 2, { &quot;Fn::GetAZs&quot; : &quot;&quot; } ] },
        &quot;CidrBlock&quot; : { &quot;Ref&quot;: &quot;VPCSubnet2CIDR&quot; },
        &quot;Tags&quot; : [
          { &quot;Key&quot; : &quot;Name&quot;, &quot;Value&quot; : { &quot;Fn::Join&quot;: [ &quot;-&quot;, [ &quot;subnet&quot;, { &quot;Ref&quot;: &quot;Project&quot; }, { &quot;Ref&quot;: &quot;Environment&quot; } ] ] } },
          { &quot;Key&quot; : &quot;AZ&quot;, &quot;Value&quot; : { &quot;Fn::Select&quot; : [ 2, { &quot;Fn::GetAZs&quot; : &quot;&quot; } ] } },
          { &quot;Key&quot; : &quot;Project&quot;, &quot;Value&quot; : { &quot;Ref&quot;: &quot;Project&quot; } },
          { &quot;Key&quot; : &quot;Environment&quot;, &quot;Value&quot; : { &quot;Ref&quot;: &quot;Environment&quot; } }
        ]
      }
    },
    &quot;InternetGateway&quot; : {
      &quot;Type&quot; : &quot;AWS::EC2::InternetGateway&quot;,
      &quot;Properties&quot; : {
        &quot;Tags&quot; : [
          { &quot;Key&quot; : &quot;Name&quot;, &quot;Value&quot; : { &quot;Fn::Join&quot;: [ &quot;-&quot;, [ &quot;igw&quot;, { &quot;Ref&quot;: &quot;Project&quot; }, { &quot;Ref&quot;: &quot;Environment&quot; } ] ] } },
          { &quot;Key&quot; : &quot;Project&quot;, &quot;Value&quot; : { &quot;Ref&quot;: &quot;Project&quot; } },
          { &quot;Key&quot; : &quot;Environment&quot;, &quot;Value&quot; : { &quot;Ref&quot;: &quot;Environment&quot; } }
        ]
      }
    },
    &quot;GatewayToInternet&quot; : {
       &quot;Type&quot; : &quot;AWS::EC2::VPCGatewayAttachment&quot;,
       &quot;Properties&quot; : {
         &quot;VpcId&quot; : { &quot;Ref&quot; : &quot;VPC&quot; },
         &quot;InternetGatewayId&quot; : { &quot;Ref&quot; : &quot;InternetGateway&quot; }
       }
    },
    &quot;PublicRouteTable&quot; : {
      &quot;Type&quot; : &quot;AWS::EC2::RouteTable&quot;,
      &quot;DependsOn&quot; : &quot;GatewayToInternet&quot;,
      &quot;Properties&quot; : {
        &quot;VpcId&quot; : { &quot;Ref&quot; : &quot;VPC&quot; },
        &quot;Tags&quot; : [
          { &quot;Key&quot; : &quot;Name&quot;, &quot;Value&quot; : { &quot;Fn::Join&quot;: [ &quot;-&quot;, [ &quot;route&quot;, { &quot;Ref&quot;: &quot;Project&quot; }, { &quot;Ref&quot; : &quot;Environment&quot; } ] ] } },
          { &quot;Key&quot; : &quot;Project&quot;, &quot;Value&quot; : { &quot;Ref&quot;: &quot;Project&quot; } },
          { &quot;Key&quot; : &quot;Environment&quot;, &quot;Value&quot; : { &quot;Ref&quot;: &quot;Environment&quot; } }
        ]
      }
    },
    &quot;PublicRoute&quot; : {
      &quot;Type&quot; : &quot;AWS::EC2::Route&quot;,
      &quot;DependsOn&quot; : &quot;GatewayToInternet&quot;,
      &quot;Properties&quot; : {
        &quot;RouteTableId&quot; : { &quot;Ref&quot; : &quot;PublicRouteTable&quot; },
        &quot;DestinationCidrBlock&quot; : &quot;0.0.0.0/0&quot;,
        &quot;GatewayId&quot; : { &quot;Ref&quot; : &quot;InternetGateway&quot; }
      }
    },
    &quot;VPCSubnet0RouteTableAssociation&quot; : {
      &quot;Type&quot; : &quot;AWS::EC2::SubnetRouteTableAssociation&quot;,
      &quot;Properties&quot; : {
        &quot;SubnetId&quot; : { &quot;Ref&quot; : &quot;VPCSubnet0&quot; },
        &quot;RouteTableId&quot; : { &quot;Ref&quot; : &quot;PublicRouteTable&quot; }
      }
    },
    &quot;VPCSubnet1RouteTableAssociation&quot; : {
      &quot;Type&quot; : &quot;AWS::EC2::SubnetRouteTableAssociation&quot;,
      &quot;Properties&quot; : {
        &quot;SubnetId&quot; : { &quot;Ref&quot; : &quot;VPCSubnet1&quot; },
        &quot;RouteTableId&quot; : { &quot;Ref&quot; : &quot;PublicRouteTable&quot; }
      }
    },
    &quot;VPCSubnet2RouteTableAssociation&quot; : {
      &quot;Type&quot; : &quot;AWS::EC2::SubnetRouteTableAssociation&quot;,
      &quot;Properties&quot; : {
        &quot;SubnetId&quot; : { &quot;Ref&quot; : &quot;VPCSubnet2&quot; },
        &quot;RouteTableId&quot; : { &quot;Ref&quot; : &quot;PublicRouteTable&quot; }
      }
    },
    &quot;InstanceRole&quot;: {
      &quot;Type&quot;: &quot;AWS::IAM::Role&quot;,
      &quot;Properties&quot;: {
        &quot;AssumeRolePolicyDocument&quot;: {
          &quot;Version&quot;: &quot;2012-10-17&quot;,
          &quot;Statement&quot;: [
            {
              &quot;Effect&quot;: &quot;Allow&quot;,
              &quot;Principal&quot;: {
                &quot;Service&quot;: [ &quot;ec2.amazonaws.com&quot; ]
              },
              &quot;Action&quot;: [ &quot;sts:AssumeRole&quot; ]
            }
          ]
        },
        &quot;Path&quot;: &quot;/&quot;,
        &quot;Policies&quot;: [
          {
            &quot;PolicyName&quot;: &quot;ApplicationPolicy&quot;,
            &quot;PolicyDocument&quot;: {
              &quot;Version&quot;: &quot;2012-10-17&quot;,
              &quot;Statement&quot;: [
                {
                  &quot;Effect&quot;: &quot;Allow&quot;,
                  &quot;Action&quot;: [
                    &quot;elasticbeanstalk:*&quot;,
                    &quot;elastiCache:*&quot;,
                    &quot;ec2:*&quot;,
                    &quot;elasticloadbalancing:*&quot;,
                    &quot;autoscaling:*&quot;,
                    &quot;cloudwatch:*&quot;,
                    &quot;dynamodb:*&quot;,
                    &quot;s3:*&quot;,
                    &quot;sns:*&quot;,
                    &quot;sqs:*&quot;,
                    &quot;cloudformation:*&quot;,
                    &quot;rds:*&quot;,
                    &quot;iam:AddRoleToInstanceProfile&quot;,
                    &quot;iam:CreateInstanceProfile&quot;,
                    &quot;iam:CreateRole&quot;,
                    &quot;iam:PassRole&quot;,
                    &quot;iam:ListInstanceProfiles&quot;
                  ],
                  &quot;Resource&quot;: &quot;*&quot;
                }
              ]
            }
          }
        ]
      }
    },
    &quot;InstanceProfile&quot;: {
       &quot;Type&quot;: &quot;AWS::IAM::InstanceProfile&quot;,
       &quot;Properties&quot;: {
          &quot;Path&quot;: &quot;/&quot;,
          &quot;Roles&quot;: [ { &quot;Ref&quot;: &quot;InstanceRole&quot; } ]
       }
    },
    &quot;VPCSecurityGroup&quot; : {
      &quot;Type&quot; : &quot;AWS::EC2::SecurityGroup&quot;,
      &quot;Properties&quot; : {
        &quot;GroupDescription&quot; : { &quot;Fn::Join&quot;: [ &quot;&quot;, [ &quot;VPC Security Group for &quot;, { &quot;Fn::Join&quot;: [ &quot;-&quot;, [ { &quot;Ref&quot;: &quot;Project&quot; }, { &quot;Ref&quot;: &quot;Environment&quot; } ] ] } ] ] },
        &quot;SecurityGroupIngress&quot; : [
          {&quot;IpProtocol&quot;: &quot;tcp&quot;, &quot;FromPort&quot; : &quot;22&quot;,  &quot;ToPort&quot; : &quot;22&quot;,  &quot;CidrIp&quot; : { &quot;Ref&quot; : &quot;SSHFrom&quot; }},
          {&quot;IpProtocol&quot;: &quot;tcp&quot;, &quot;FromPort&quot;: &quot;80&quot;, &quot;ToPort&quot;: &quot;80&quot;, &quot;CidrIp&quot;: &quot;0.0.0.0/0&quot; },
          {&quot;IpProtocol&quot;: &quot;tcp&quot;, &quot;FromPort&quot;: &quot;443&quot;, &quot;ToPort&quot;: &quot;443&quot;, &quot;CidrIp&quot;: &quot;0.0.0.0/0&quot; }
        ],
        &quot;VpcId&quot; : { &quot;Ref&quot; : &quot;VPC&quot; },
        &quot;Tags&quot; : [
          { &quot;Key&quot; : &quot;Name&quot;, &quot;Value&quot; : { &quot;Fn::Join&quot;: [ &quot;-&quot;, [ &quot;sg&quot;, { &quot;Ref&quot;: &quot;Project&quot; }, { &quot;Ref&quot; : &quot;Environment&quot; } ] ] } },
          { &quot;Key&quot; : &quot;Project&quot;, &quot;Value&quot; : { &quot;Ref&quot;: &quot;Project&quot; } },
          { &quot;Key&quot; : &quot;Environment&quot;, &quot;Value&quot; : { &quot;Ref&quot;: &quot;Environment&quot; } }
        ]
      }
    },
    &quot;VPCSGIngress&quot;: {
      &quot;Type&quot;: &quot;AWS::EC2::SecurityGroupIngress&quot;,
      &quot;Properties&quot;: {
        &quot;GroupId&quot;: { &quot;Ref&quot; : &quot;VPCSecurityGroup&quot; },
        &quot;IpProtocol&quot;: &quot;-1&quot;,
        &quot;FromPort&quot;: &quot;0&quot;,
        &quot;ToPort&quot;: &quot;65535&quot;,
        &quot;SourceSecurityGroupId&quot;: { &quot;Ref&quot;: &quot;VPCSecurityGroup&quot; }
      }
    }
  },
  &quot;Outputs&quot; : {
    &quot;VpcId&quot; : {
      &quot;Description&quot; : &quot;VPC Id&quot;,
      &quot;Value&quot; :  { &quot;Ref&quot; : &quot;VPC&quot; }
    },
    &quot;VPCDefaultNetworkAcl&quot; : {
      &quot;Description&quot; : &quot;VPC&quot;,
      &quot;Value&quot; :  { &quot;Fn::GetAtt&quot; : [&quot;VPC&quot;, &quot;DefaultNetworkAcl&quot;] }
    },
    &quot;VPCDefaultSecurityGroup&quot; : {
      &quot;Description&quot; : &quot;VPC Default Security Group that we blissfully ignore thanks to self-referencing bugs&quot;,
      &quot;Value&quot; :  { &quot;Fn::GetAtt&quot; : [&quot;VPC&quot;, &quot;DefaultSecurityGroup&quot;] }
    },
    &quot;VPCSecurityGroup&quot; : {
      &quot;Description&quot; : &quot;VPC Security Group created by this stack&quot;,
      &quot;Value&quot; :  { &quot;Ref&quot;: &quot;VPCSecurityGroup&quot; }
    },
    &quot;VPCSubnet0&quot;: {
      &quot;Description&quot;: &quot;The subnet id for VPCSubnet0&quot;,
      &quot;Value&quot;: {
        &quot;Ref&quot;: &quot;VPCSubnet0&quot;
      }
    },
    &quot;VPCSubnet1&quot;: {
      &quot;Description&quot;: &quot;The subnet id for VPCSubnet1&quot;,
      &quot;Value&quot;: {
        &quot;Ref&quot;: &quot;VPCSubnet1&quot;
      }
    },
    &quot;VPCSubnet2&quot;: {
      &quot;Description&quot;: &quot;The subnet id for VPCSubnet2&quot;,
      &quot;Value&quot;: {
        &quot;Ref&quot;: &quot;VPCSubnet2&quot;
      }
    }
  }
}</code></pre></noscript></div>
</p>

<p>Here is an example CloudFormation parameters file for this template:</p>

<p><div><script src='https://gist.github.com/9f4b8dd2b39c7d1c31ef.js'></script>
<noscript><pre><code>[
  { &quot;ParameterKey&quot;: &quot;Project&quot;, &quot;ParameterValue&quot;: &quot;myapp&quot; },
  { &quot;ParameterKey&quot;: &quot;Environment&quot;, &quot;ParameterValue&quot;: &quot;dev&quot; },
  { &quot;ParameterKey&quot;: &quot;VPCNetworkCIDR&quot;, &quot;ParameterValue&quot;: &quot;10.0.0.0/16&quot; },
  { &quot;ParameterKey&quot;: &quot;VPCSubnet0CIDR&quot;, &quot;ParameterValue&quot;: &quot;10.0.0.0/24&quot; },
  { &quot;ParameterKey&quot;: &quot;VPCSubnet1CIDR&quot;, &quot;ParameterValue&quot;: &quot;10.0.1.0/24&quot; },
  { &quot;ParameterKey&quot;: &quot;VPCSubnet2CIDR&quot;, &quot;ParameterValue&quot;: &quot;10.0.2.0/24&quot; }
]</code></pre></noscript></div>
</p>

<p>To script the creation, updating, watching, and deleting of the CloudFormation VPC, I have this Makefile as well:</p>

<p><div><script src='https://gist.github.com/55b740ff19825d621ef4.js'></script>
<noscript><pre><code>STACK:=myapp-dev
TEMPLATE:=cloudformation-template_vpc-iam.json
PARAMETERS:=cloudformation-parameters_myapp-dev.json
AWS_REGION:=us-east-1
AWS_PROFILE:=aws-dev

all:
	@which aws || pip install awscli
	aws cloudformation create-stack --stack-name $(STACK) --template-body file://`pwd`/$(TEMPLATE) --parameters file://`pwd`/$(PARAMETERS) --capabilities CAPABILITY_IAM --profile $(AWS_PROFILE) --region $(AWS_REGION)

update:
	aws cloudformation update-stack --stack-name $(STACK) --template-body file://`pwd`/$(TEMPLATE) --parameters file://`pwd`/$(PARAMETERS) --capabilities CAPABILITY_IAM --profile $(AWS_PROFILE) --region $(AWS_REGION)

events:
	aws cloudformation describe-stack-events --stack-name $(STACK) --profile $(AWS_PROFILE) --region $(AWS_REGION)

watch:
	watch --interval 10 &quot;bash -c &#39;make events | head -25&#39;&quot;
	
outputs:
	@which jq || ( which brew &amp;&amp; brew install jq || which apt-get &amp;&amp; apt-get install jq || which yum &amp;&amp; yum install jq || which choco &amp;&amp; choco install jq)
	aws cloudformation describe-stacks --stack-name $(STACK) --profile $(AWS_PROFILE) --region $(AWS_REGION) | jq -r &#39;.Stacks[].Outputs&#39;

delete:
	aws cloudformation delete-stack --stack-name $(STACK) --profile $(AWS_PROFILE) --region $(AWS_REGION)
</code></pre></noscript></div>
</p>

<p>You can get these same files by cloning my github project, and ssuming you have a profile named <code>aws-dev</code> as mentioned above, you can even run <code>make</code> and have it create the <code>myapp-dev</code> VPC via CloudFormation:</p>

<pre><code>git clone https://github.com/ianblenke/aws-docker-walkthrough
cd aws-docker-walkthrough
make
</code></pre>

<p>You can run <code>make watch</code> to watch the CloudFormation events and wait for a <code>CREATE_COMPLETE</code> state.</p>

<p>When this is complete, you can see the CloudFormation outputs by running:</p>

<pre><code>make output
</code></pre>

<p>The output will look something like this:</p>

<p><div><script src='https://gist.github.com/59715079304a6db7182c.js'></script>
<noscript><pre><code>aws cloudformation describe-stacks --stack-name myapp-dev --profile aws-dev --region us-east-1 | jq -r &#39;.Stacks[].Outputs&#39;
[
  {
    &quot;Description&quot;: &quot;VPC Id&quot;,
    &quot;OutputKey&quot;: &quot;VpcId&quot;,
    &quot;OutputValue&quot;: &quot;vpc-b7d1d8d2&quot;
  },
  {
    &quot;Description&quot;: &quot;VPC&quot;,
    &quot;OutputKey&quot;: &quot;VPCDefaultNetworkAcl&quot;,
    &quot;OutputValue&quot;: &quot;acl-b3cfc7d6&quot;
  },
  {
    &quot;Description&quot;: &quot;VPC Default Security Group that we blissfully ignore thanks to self-referencing bugs&quot;,
    &quot;OutputKey&quot;: &quot;VPCDefaultSecurityGroup&quot;,
    &quot;OutputValue&quot;: &quot;sg-3e50a559&quot;
  },
  {
    &quot;Description&quot;: &quot;VPC Security Group created by this stack&quot;,
    &quot;OutputKey&quot;: &quot;VPCSecurityGroup&quot;,
    &quot;OutputValue&quot;: &quot;sg-0c50a56b&quot;
  },
  {
    &quot;Description&quot;: &quot;The subnet id for VPCSubnet0&quot;,
    &quot;OutputKey&quot;: &quot;VPCSubnet0&quot;,
    &quot;OutputValue&quot;: &quot;subnet-995236b2&quot;
  },
  {
    &quot;Description&quot;: &quot;The subnet id for VPCSubnet1&quot;,
    &quot;OutputKey&quot;: &quot;VPCSubnet1&quot;,
    &quot;OutputValue&quot;: &quot;subnet-6aa4fd1d&quot;
  },
  {
    &quot;Description&quot;: &quot;The subnet id for VPCSubnet2&quot;,
    &quot;OutputKey&quot;: &quot;VPCSubnet2&quot;,
    &quot;OutputValue&quot;: &quot;subnet-ad3644f4&quot;
  }
]</code></pre></noscript></div>
</p>

<p>These CloudFormation Outputs list parameters that we will need to pass to the ElasticBeanstalk Environment creation during the next part of this walkthrough. </p>

<h1 id="one-final-vpc-note-iam-permissions-for-ec2-instance-profiles">One final VPC note: IAM permissions for EC2 instance profiles</h1>

<p>As a general rule of thumb, each AWS ElasticBanstalk Application Environment should be given its own IAM Instance Profile to use.</p>

<p>Each AWS EC2 instance should be allowed to assume an IAM role for an IAM instance profile that gives it access to the AWS cloud resources it must interface with.</p>

<p>This is accomplished by <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html">introspecting on AWS instance metadata</a>. If you haven’t been exposed to this yet, I strongly recommend poking around at <code>http://169.254.169.254</code> from your EC2 instances:</p>

<p><code>bash
curl http://169.254.169.254/latest/meta-data/iam/security-credentials/role-myapp-dev
</code></p>

<p>The JSON returned from that command allows an AWS library call with no credentials automatically obtain time-limited IAM STS credentials when run on AWS EC2 instances.</p>

<p>This avoids having to embed “permanent” IAM access/secret keys as environment variables that may “leak” over time to parties that shouldn’t have access.</p>

<p>Early on, we tried to do this as an ebextension in <code>.ebextensions/00_iam.config</code>, but this only works if the admin running the <code>eb create</code> has IAM permissions for the AWS account, and it appears impossible to change the launch InstanceProfile by defining option settings or overriding cloud resources in an ebextensions config file.</p>

<p>Instead, the VPC above generates an <code>InstanceProfile</code> that can be referenced later. More on that later in Part 2.</p>

<p>Stay tuned…</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Deploying Amazon ECS on CoreOS]]></title>
    <link href="http://ian.blenke.com/blog/2015/03/10/deploying-amazon-ecs-on-coreos/"/>
    <updated>2015-03-10T16:38:33-04:00</updated>
    <id>http://ian.blenke.com/blog/2015/03/10/deploying-amazon-ecs-on-coreos</id>
    <content type="html"><![CDATA[<p>Today, I stumbled on the official <a href="https://coreos.com/docs/running-coreos/cloud-providers/ecs/">CoreOS page on ECS</a>.</p>

<p>I’ve been putting off ECS for a while, it was time to give it a try.</p>

<p>To create the ECS cluster, we will need the aws commandline tool:</p>

<pre><code>which aws || pip install awscli
</code></pre>

<p>Make sure you have your <code>AWS_ACCESS_KEY_ID</code> and <code>AWS_SECRET_ACCESS_KEY</code> defined in your shell environment.</p>

<p>Create the ECS cluster:</p>

<pre><code>aws ecs create-cluster --cluster-name Cosmos-Dev
{
    "cluster": {
        "clusterName": "Cosmos-Dev",
        "status": "ACTIVE",
        "clusterArn": "arn:aws:ecs:us-east-1:123456789012:cluster/My-ECS-Cluster"
    }
}
</code></pre>

<p>Install the global fleet unit for amazon-ecs-agent.service:</p>

<pre><code>cat &lt;&lt;EOF &gt; amazon-ecs-agent.service
[Unit]
Description=Amazon ECS Agent
After=docker.service
Requires=docker.service
[Service]
Environment=ECS_CLUSTER=My-ECS-Cluster
Environment=ECS_LOGLEVEL=warn
Environment=AWS_REGION=us-east-1
ExecStartPre=-/usr/bin/docker kill ecs-agent
ExecStartPre=-/usr/bin/docker rm ecs-agent
ExecStartPre=/usr/bin/docker pull amazon/amazon-ecs-agent
ExecStart=/usr/bin/docker run --name ecs-agent \
    --env=ECS_CLUSTER=${ECS_CLUSTER}\
    --env=ECS_LOGLEVEL=${ECS_LOGLEVEL} \
    --publish=127.0.0.1:51678:51678 \
    --volume=/var/run/docker.sock:/var/run/docker.sock \
    amazon/amazon-ecs-agent
ExecStop=/usr/bin/docker stop ecs-agent
[X-Fleet]
Global=true
EOF
fleetctl start amazon-ecs-agent.service
</code></pre>

<p>This registers a ContainerInstance to the <code>My-ECS-Cluster</code> in region <code>us-east-1</code>.</p>

<p>Note: this is using the EC2 instance’s instance profile IAM credentials. You will want to make sure you’ve assigned an instance profile with a Role that has “ecs:*” access.
For this, you may want to take a look at the <a href="https://s3.amazonaws.com/amazon-ecs-cloudformation/Amazon_ECS_Quickstart.template">Amazon ECS Quickstart CloudFormation template</a>.</p>

<p>Now from a CoreOS host, we can query locally to enumerate the running ContainerInstances in our fleet:</p>

<pre><code>fleetctl list-machines -fields=ip -no-legend | while read ip ; do \
    echo $ip $(ssh -n $ip curl -s http://169.254.169.254/latest/meta-data/instance-id) \
    $(ssh -n $ip curl -s http://localhost:51678/v1/metadata | \
      docker run -i realguess/jq jq .ContainerInstanceArn) ; \
  done
</code></pre>

<p>Which returns something like:</p>

<pre><code>10.113.0.23 i-12345678 "arn:aws:ecs:us-east-1:123456789012:container-instance/674140ae-1234-4321-1234-4abf7878caba"
10.113.1.42 i-23456789 "arn:aws:ecs:us-east-1:123456789012:container-instance/c3506771-1234-4321-1234-1f1b1783c924"
10.113.2.66 i-34567891 "arn:aws:ecs:us-east-1:123456789012:container-instance/75d30c64-1234-4321-1234-8be8edeec9c6"
</code></pre>

<p>And we can query ECS and get the same:</p>

<pre><code>$ aws ecs list-container-instances --cluster My-ECS-Cluster | grep arn | cut -d'"' -f2 | \
  xargs -L1 -I% aws ecs describe-container-instances --cluster My-ECS-Cluster --container-instance % | \
  jq '.containerInstances[] | .ec2InstanceId + " " + .containerInstanceArn'
"i-12345678 arn:aws:ecs:us-east-1:123456789012:container-instance/674140ae-1234-4321-1234-4abf7878caba"
"i-23456789 arn:aws:ecs:us-east-1:123456789012:container-instance/c3506771-1234-4321-1234-1f1b1783c924"
"i-34567891 arn:aws:ecs:us-east-1:123456789012:container-instance/75d30c64-1234-4321-1234-8be8edeec9c6"
</code></pre>

<p>This ECS cluster is ready to use.</p>

<p>Unfortunately, there is no scheduler here. ECS is a harness for orchestrating docker containers in a cluster as <em>tasks</em>. </p>

<p>Where these tasks are allocated is left up to the AWS customer.</p>

<p>What we really need is a <em>scheduler</em>.</p>

<p>CoreOS has a form of a scheduler in fleet, but that is for fleet units of systemd services, and is not limited to docker containers as ECS is.
Fleet’s scheduler is also currently a bit weak in that it schedules new units to the fleet machine with the fewest number of units.</p>

<p>Kubernetes has a random scheduler, which is better in a couple ways, but does not fairly allocate the system resources.</p>

<p>The <em>best</em> scheduler at present is Mesos, which takes into account resource sizing estimates and current utilization.</p>

<p>Normally, Mesos uses Mesos Slaves to run work. Mesos can also use ECS as a backend instead.</p>

<p>My next steps: Deploy Mesos using the <a href="https://github.com/awslabs/ecs-mesos-scheduler-driver">ecs-mesos-scheduler-driver</a>, as <a href="http://jpetazzo.github.io/2015/01/14/amazon-docker-ecs-ec2-container-service/">summarized by jpetazzo</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Docker Rspec TDD]]></title>
    <link href="http://ian.blenke.com/blog/2014/11/10/docker-rspec-tdd/"/>
    <updated>2014-11-10T14:38:37-05:00</updated>
    <id>http://ian.blenke.com/blog/2014/11/10/docker-rspec-tdd</id>
    <content type="html"><![CDATA[<p>A Dockerfile both describes a Docker image as well as layers for the working directory, environment variables, ports, entrypoint commands, and other important interfaces.</p>

<p>Test-Driven Design should drive a developer toward implementation details, not the other way around.</p>

<p>A devops without tests is a sad devops indeed.</p>

<p>Working toward a docker based development environment, my first thoughts were toward <a href="http://serverspec.org/">Serverspec</a> by <a href="https://github.com/mizzy">Gosuke Miayshita</a>, as it is entirely framework agnostic. Gosuke gave an excellent presentation at ChefConf this year re-inforcing that Serverspec is <em>not</em> a chef centric tool, and works quite well in conjunction with other configuration management tools.</p>

<p>Researching Serverspec and docker a bit more, <a href="https://github.com/tcnksm">Taichi Nakashima</a> based his <a href="https://github.com/tcnksm-sample/docker-rspec">TDD of Dockerfile by RSpec on OS/X</a> using ssh.</p>

<p>With Docker 1.3 and later, there is a “docker exec” interactive docker API for allowing live sessions on processes spawned in the same process namespace as a running container, effectively allowing external access into a running docker container using only the docker API.</p>

<p><a href="http://blog.wercker.com/2013/12/23/Test-driven-development-for-docker.html">PIETER JOOST VAN DE SANDE</a> posted about using the docker-api to accomplish the goal of testing a Dockerfile. His work is based on the <a href="https://rubygems.org/gems/docker-api">docker-api</a> gem (github <a href="https://github.com/swipely/docker-api">swipely/docker-api</a>).</p>

<p>Looking into the docker-api source, there is no support yet for docker 1.3’s exec API interface to run Serverspec tests against the contents of a running docker container.</p>

<p>Attempting even the most basic docker API calls with docker-api, <a href="https://github.com/swipely/docker-api/issues/202">issue 202</a> made it apparent that TLS support for boot2docker would need to be addressed first.</p>

<p>Here is my functional <code>spec_helper.rb</code> with the fixes necessary to use docker-api without modifications:</p>

<p><div><script src='https://gist.github.com/5335483e4021954d815f.js'></script>
<noscript><pre><code>require &quot;docker&quot;

docker_host = ENV[&#39;DOCKER_HOST&#39;].dup

if(ENV[&#39;DOCKER_TLS_VERIFY&#39;])
  cert_path = File.expand_path ENV[&#39;DOCKER_CERT_PATH&#39;]
  Docker.options = {
    client_cert: File.join(cert_path, &#39;cert.pem&#39;),
    client_key: File.join(cert_path, &#39;key.pem&#39;)
  }
  Excon.defaults[:ssl_ca_file] = File.join(cert_path, &#39;ca.pem&#39;)
  docker_host.gsub!(/^tcp/,&#39;https&#39;)
end

Docker.url = docker_host</code></pre></noscript></div>
</p>

<p>Following this, I can drive the generation of a Dockerfile with a spec:</p>

<p><div><script src='https://gist.github.com/261e6fe930c922202151.js'></script>
<noscript><pre><code>require &quot;spec_helper&quot;

describe &quot;dockerfile built my_app image&quot; do
  before(:all) do
    @image = Docker::Image.all(:all =&gt; true).find { |image|
      Docker::Util.parse_repo_tag( image.info[&#39;RepoTags&#39;].first ).first == &#39;my_app&#39;
    }
    p @image.json[&quot;Env&quot;]
  end

  it &quot;should exist&quot; do
    expect(@image).not_to be_nil
  end

  it &quot;should have CMD&quot; do
    expect(@image.json[&quot;Config&quot;][&quot;Cmd&quot;]).to include(&quot;/run.sh&quot;)
  end

  it &quot;should expose the default port&quot; do
    expect(@image.json[&quot;Config&quot;][&quot;ExposedPorts&quot;].has_key?(&quot;3000/tcp&quot;)).to be_truthy
  end

  it &quot;should have environmental variable&quot; do
    expect(@image.json[&quot;Config&quot;][&quot;Env&quot;]).to include(&quot;HOME=/usr/src/app&quot;)
  end
end</code></pre></noscript></div>
</p>

<p>This drives me iteratively to write a Dockerfile that looks like:</p>

<p><div><script src='https://gist.github.com/ef7150420cbd328d5e41.js'></script>
<noscript><pre><code>FROM rails:onbuild
ENV HOME /usr/src/app
ADD docker/run.sh /run.sh
RUN chmod 755 /run.sh
EXPOSE 3000
CMD /run.sh</code></pre></noscript></div>
</p>

<p>Next step: extend docker-api to support exec for serverspec based testing of actual docker image contents.</p>

<p>Sláinte!</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Fig-docker]]></title>
    <link href="http://ian.blenke.com/blog/2014/11/07/fig-docker/"/>
    <updated>2014-11-07T19:20:06-05:00</updated>
    <id>http://ian.blenke.com/blog/2014/11/07/fig-docker</id>
    <content type="html"><![CDATA[<p>A common devops problem when developing <a href="http://docker.io">Docker</a> containers is managing the orchestration of multiple containers in a development environment.</p>

<p>There are a number of orchestration harnesses for Docker available:</p>

<ul>
  <li>Docker’s <a href="http://fig.sh">Fig</a></li>
  <li><a href="https://github.com/dcm-oss/blockade">blockade</a></li>
  <li><a href="https://docs.vagrantup.com/v2/provisioning/docker.html">Vagrant</a></li>
  <li><a href="https://github.com/GoogleCloudPlatform/kubernetes">kubernetes</a></li>
  <li><a href="https://github.com/signalfuse/maestro-ng">maestro-ng</a></li>
  <li><a href="https://github.com/michaelsauter/crane">crane</a></li>
  <li>Centurylink’s <a href="http://panamax.io/">Panamax</a></li>
  <li><a href="http://shipyard-project.com/">Shipyard</a></li>
  <li><a href="http://decking.io/">Decking</a></li>
  <li>NewRelic’s <a href="https://github.com/newrelic/centurion">Centurion</a></li>
  <li>Spotify’s <a href="https://github.com/spotify/helios">Helios</a></li>
  <li><a href="https://github.com/cattleio/stampede">Stampede</a></li>
  <li><a href="https://www.getchef.com/solutions/docker/">Chef</a></li>
  <li><a href="http://www.ansible.com/docker">Ansible</a></li>
  <li><a href="https://flynn.io/">Flynn</a></li>
  <li><a href="https://github.com/mailgun/shipper">Shipper</a></li>
  <li><a href="http://octohost.io">Octohost</a></li>
  <li><a href="http://tsuru.io/">Tsuru</a> with <a href="https://github.com/tsuru/docker-cluster">docker-cluster</a></li>
  <li><a href="https://clusterhq.com/">Flocker</a></li>
  <li><a href="https://github.com/CloudCredo/cloudfocker">CloudFocker</a></li>
  <li><a href="http://www.cloudsoftcorp.com/blog/2014/06/clocker-creating-a-docker-cloud-with-apache-brooklyn/">Clocker</a> and <a href="http://brooklyn.incubator.apache.org">Apache Brooklyn</a></li>
  <li><a href="http://cloudfoundry.org">CloudFoundry</a>’s <a href="https://github.com/cf-platform-eng/docker-boshrelease">docker-boshrelease</a>/<a href="https://github.com/cloudfoundry-incubator/diego-release">diego</a></li>
  <li>Mesosphere <a href="https://github.com/mesosphere/deimos">Deimos</a></li>
  <li><a href="http://deis.io">Deis</a> (a PaaS that can git push deploy containers using <a href="http://heroku.com">Heroku</a> buildpacks <em>or</em> a Dockerfile)</li>
</ul>

<p>There are a number of hosted service offerings now as well:</p>

<ul>
  <li><a href="http://aws.amazon.com/ecs">Amazon ECS</a></li>
  <li><a href="http://www.qualisystems.com/cloudshell-6-0-sneak-peek/">CloudShell</a></li>
  <li><a href="https://elasticbox.com/how-it-works/">ElasticBox</a></li>
  <li>Aw, heck, just check the <a href="http://www.mindmeister.com/389671722/docker-ecosystem">docker ecosystem mindmap</a></li>
</ul>

<p>There are also RAFT/GOSSIP clustering solutions like:</p>

<ul>
  <li><a href="https://coreos.com/">CoreOS</a>/<a href="https://github.com/coreos/fleet">Fleet</a></li>
  <li><a href="https://www.openshift.com/products/origin">OpenShift Origin</a> uses <a href="http://www.projectatomic.io/">ProjectAtomic</a>/<a href="https://openshift.github.io/geard/">Geard</a></li>
</ul>

<p>My <a href="https://github.com/ianblenke/coreos-vagrant-kitchen-sink">coreos-vagrant-kitchen-sink</a> github project submits <a href="https://github.com/ianblenke/coreos-vagrant-kitchen-sink/tree/master/cloud-init">cloud-init units</a> via a YAML file when booting member nodes. It’s a good model for production, but it’s a bit heavy for development.</p>

<p>Docker is currently working on <a href="https://www.youtube.com/watch?v=vtnSL79rZ6o">Docker Clustering</a>, but it is presently just a proof-of-concept and is now under a total re-write.</p>

<p>They are also <a href="https://www.youtube.com/watch?v=YuSq6bXHnOI">implementing docker composition</a> which provides Fig like functionality using upcoming docker “groups”.</p>

<p>That influence of Fig makes sense, as <a href="http://venturebeat.com/2014/07/22/docker-buys-orchard-a-2-man-startup-with-a-cloud-service-for-running-docker-friendly-apps/">Docker bought Orchard</a>.</p>

<p>Internally, Docker developers use <a href="http://fig.sh">Fig</a>.</p>

<p>Docker’s website also directs everyone to <a href="http://boot2docker.io">Boot2Docker</a>, as that is the tool Docker developers use as their docker baseline environment. </p>

<p>Boot2Docker spawns a <a href="https://www.virtualbox.org/">VirtualBox</a> based VM as well as a native docker client runtime on the developer’s host machine, and provides the <code>DOCKER_HOST</code> and related enviroments necessary for the client to talk to the VM.</p>

<p>This allows a developer’s Windows or OS/X machine to have a docker command that behaves as if the docker containers are running natively on their host machine.</p>

<p>While Fig is easy to install under OS/X as it has native Python support (“pip install fig”), installing Fig on a Windows developer workstation would normally require Python support be installed separately.</p>

<p>Rather than do that, I’ve built a new <a href="https://registry.hub.docker.com/u/ianblenke/fig-docker/">ianblenke/fig-docker</a> docker Hub image, which is auto-built from <a href="https://github.com/ianblenke/docker-fig-docker">ianblenke/docker-fig-docker</a> on github.</p>

<p>This allows running fig inside a docker container using:</p>

<p><div><script src='https://gist.github.com/6cd8f8a4065bcac99443.js'></script>
<noscript><pre><code>docker run -v $(pwd):/app -v $DOCKER_CERT_PATH:/certs -e DOCKER_CERT_PATH=/certs -e DOCKER_HOST=$DOCKER_HOST -e DOCKER_TLS_VERIFY=$DOCKER_TLS_VERIFY -ti --rm ianblenke/fig-docker fig --help</code></pre></noscript></div>
</p>

<p>Alternatively, a developer can alias it:</p>

<p><div><script src='https://gist.github.com/f48bc5cf8b2567bd8006.js'></script>
<noscript><pre><code>alias fig=&quot;docker run -v $(pwd):/app -v $DOCKER_CERT_PATH:/certs -e DOCKER_CERT_PATH=/certs -e DOCKER_HOST=$DOCKER_HOST -e DOCKER_TLS_VERIFY=$DOCKER_TLS_VERIFY -ti --rm ianblenke/fig-docker fig&quot;</code></pre></noscript></div>
</p>

<p>Now the developer can run <code>fig</code> as if it is running on their development host, continuing the boot2docker illusion.</p>

<p>In the above examples, the current directory <code>$(pwd)</code> is being mounted as /app inside the docker container.</p>

<p>On a boot2docker install, the boot2docker VM is the actual source of that volume path.</p>

<p>That means you would actually have to have the current path inside the boot2docker VM as well.</p>

<p>To do that, on a Mac, do this:</p>

<p><div><script src='https://gist.github.com/444c3f6552744ef25f59.js'></script>
<noscript><pre><code>boot2docker down
VBoxManage sharedfolder add boot2docker-vm -name home -hostpath /Users
boot2docker up</code></pre></noscript></div>
</p>

<p>From this point forward, until the next <code>boot2docker init</code>, your boot2docker VM should have your home directory mounted as /Users and the path should be the same.</p>

<p>A similar trick happens for Windows hosts, providing the same path inside the boot2docker VM as a developer would use.</p>

<p>This allows a normalized docker/fig interface for developers to begin their foray into docker orchestration.</p>

<p>Let’s setup a very quick <a href="http://rubyonrails.org/">Ruby on Rails</a> application from scratch, and then add a Dockerfile and fig.yml that spins up a mysql service for it to talk to.</p>

<p>Here’s a quick script that does just that. The only requirement is a functional docker command able to spin up containers.</p>

<p><div><script src='https://gist.github.com/92648de57cb7d38fd1e8.js'></script>
<noscript><pre><code>#!/bin/bash
set -ex

# Source the boot2docker environment variables
eval $(boot2docker shellinit 2&gt;/dev/null)

# Use a rails container to create a new rails project in the current directory called figgypudding
docker run -it --rm -v $(pwd):/app rails:latest bash -c &#39;rails new figgypudding; cp -a /figgypudding /app&#39;

cd figgypudding

# Create the Dockerfile used to build the figgypudding_web:latest image used by the figgypudding_web_1 container
cat &lt;&lt;EOD &gt; Dockerfile
FROM rails:onbuild
ENV HOME /usr/src/app
EOD

# This is the Fig orchestration configuration
cat &lt;&lt;EOF &gt; fig.yml
mysql:
  environment:
    MYSQL_ROOT_PASSWORD: supersecret
    MYSQL_DATABASE: figgydata
    MYSQL_USER: figgyuser
    MYSQL_PASSWORD: password
  ports:
    - &quot;3306:3306&quot;
  image: mysql:latest
figgypudding:
  environment:
    RAILS_ENV: development
    DATABASE_URL: mysql2://figgyuser:password@172.17.42.1:3306/figgydata
  links:
    - mysql
  ports:
    - &quot;3000:3000&quot;
  build: .
  command: bash -xc &#39;bundle exec rake db:migrate &amp;&amp; bundle exec rails server&#39;
EOF

# Rails defaults to sqlite, convert it to use mysql
sed -i -e &#39;s/sqlite3/mysql2/&#39; Gemfile

# Update the Gemfile.lock using the rails container we referenced earlier
docker run --rm -v $(pwd):/usr/src/app -w /usr/src/app rails:latest bundle update

# Use the fig command from my fig-docker container to fire up the Fig formation
docker run -v $(pwd):/app -v $DOCKER_CERT_PATH:/certs -e DOCKER_CERT_PATH=/certs -e DOCKER_HOST=$DOCKER_HOST -e DOCKER_TLS_VERIFY=$DOCKER_TLS_VERIFY -ti --rm ianblenke/fig-docker fig up</code></pre></noscript></div>
</p>

<p>After running that, there should now be a web server running on the boot2docker VM, which should generally be <a href="http://192.168.59.103:3000/">http://192.168.59.103:3000/</a> as that seems to be the common boot2docker default IP.</p>

<p>This is fig, distilled to its essence.</p>

<p>Beyond this point, a developer can “fig build ; fig up” and see the latest result of their work. This is something ideally added as a git post-commit hook or a iteration harness like <a href="https://github.com/guard/guard">Guard</a>.</p>

<p>While it may not appear <em>pretty</em> at first glance, realize that only <code>cat</code>, and <code>sed</code> were used on the host here (and very well could also themselves have also been avoided). No additional software was installed on the host, yet a rails app was created and deployed in docker containers, talking to a mysql server.</p>

<p>And therein lies the elegance of dockerizing application deployment: simple, clean, repeatable units of software. Orchestrated.</p>

<p>Have fun!</p>

]]></content>
  </entry>
  
</feed>
